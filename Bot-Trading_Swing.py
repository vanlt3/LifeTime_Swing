# Standard library imports
print("🚀 [Bot] Starting imports...")

# ==================================================
# ENCODING FIX FOR GOOGLE COLAB
# ==================================================
import sys
import os

# Ensure UTF-8 encoding for stdout and stderr
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8')

# Set environment variables for UTF-8
os.environ['PYTHONIOENCODING'] = 'utf-8'

print("🔧 [Encoding] UTF-8 encoding configured successfully")

import asyncio
import copy
import json
import logging
import re
import sqlite3
import time
import threading
import warnings
print("✅ [Bot] Basic imports completed")
import glob
import shutil
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, Union
from collections import deque
from datetime import datetime, timedelta

# ==============================================================================
# REFACTORED CONSTANTS AND CONFIGURATION
# ==============================================================================

from dataclasses import dataclass
from enum import Enum
from typing import Dict, Any, List, Optional, Union, Tuple

# Feature flags
ENABLE_HISTORICAL_SL_CHECK = False  # Set to False to disable historical SL checking
SEND_HISTORICAL_SL_ALERTS = False   # Set to True to send Discord alerts for historical SL (only if ENABLE_HISTORICAL_SL_CHECK is True)

class SymbolType(Enum):
    """Enum for symbol types"""
    CRYPTO = "crypto"
    FOREX = "forex"
    EQUITY = "equity"
    COMMODITY = "commodity"
    UNKNOWN = "unknown"

class TimeFrame(Enum):
    """Enum for timeframes"""
    M1 = "M1"
    M5 = "M5"
    M15 = "M15"
    M30 = "M30"
    H1 = "H1"
    H4 = "H4"
    D1 = "D1"
    W1 = "W1"

@dataclass
class APIConfig:
    """API configuration dataclass"""
    api_key: str
    base_url: str
    rate_limit: int
    timeout: int = 30
    retry_attempts: int = 3

@dataclass
class SymbolConfig:
    """Symbol configuration dataclass"""
    symbol: str
    symbol_type: SymbolType
    primary_timeframe: TimeFrame
    weight: float
    max_exposure: float
    risk_multiplier: float
    is_active: bool = True

# Refactored API configuration
API_CONFIGS: Dict[str, APIConfig] = {
    'FINHUB': APIConfig(
        api_key='d1b3ichr01qjhvtsbj8g',
        base_url='https://finhub.io/api/v1',
        rate_limit=60
    ),
    'MARKETAUX': APIConfig(
        api_key='CkuQmx9sPsjw0FRDeSkoO8U3O9Jj3HWnUYMJNEql',
        base_url='https://api.marketaux.com/v1',
        rate_limit=100
    ),
    'NEWSAPI': APIConfig(
        api_key='abd8f43b808f42fdb8d28fb1c429af72',
        base_url='https://newsapi.org/v2',
        rate_limit=1000
    ),
    'EODHD': APIConfig(
        api_key='68bafd7d44a7f0.25202650',
        base_url='https://eodhistoricaldata.com/api',
        rate_limit=20
    )
}

# Trading constants
class TradingConstants:
    """Trading-related constants"""
    MIN_CONFIDENCE_THRESHOLD = 0.5
    MAX_CONFIDENCE_THRESHOLD = 0.95
    CONFIDENCE_SMOOTHING_FACTOR = 0.1
    TRAILING_STOP_MULTIPLIER = 0.5
    POSITION_SIZE_MULTIPLIER = 1.0
    SPREAD_COST_PIPS = 2.0
    SLIPPAGE_PIPS = 1.0
    MAX_RISK_PER_TRADE = 0.02
    MAX_DAILY_RISK = 0.05
    MAX_PORTFOLIO_RISK = 0.10

# Feature engineering constants
class FeatureConstants:
    """Feature engineering constants"""
    RSI_PERIOD = 14
    MACD_FAST = 12
    MACD_SLOW = 26
    MACD_SIGNAL = 9
    BB_PERIOD = 20
    BB_STD = 2
    EMA_PERIODS = [20, 50, 200]
    VOLUME_PERIOD = 20
    MIN_CANDLES_FOR_ANALYSIS = 100
    MAX_STALE_MINUTES = 30

# Symbol classifications
CRYPTO_SYMBOLS = {'BTCUSD', 'ETHUSD', 'XRPUSD', 'LTCUSD', 'ADAUSD'}
EQUITY_INDICES = {'SPX500', 'NAS100', 'US30', 'DE40', 'UK100', 'FR40', 'JP225', 'AU200'}
FOREX_PAIRS = {'EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD', 'USDCAD', 'NZDUSD', 'AUDNZD'}
COMMODITIES = {'XAUUSD', 'XAGUSD', 'USOIL', 'UKOIL', 'NATGAS'}

# Legacy API configuration (for backward compatibility)
API_KEYS = {name: config.api_key for name, config in API_CONFIGS.items()}
API_ENDPOINTS = {
    'FINHUB': {'base_url': API_CONFIGS['FINHUB'].base_url, 'quote': '/quote', 'news': '/company-news', 'sentiment': '/news-sentiment'},
    'MARKETAUX': {'base_url': API_CONFIGS['MARKETAUX'].base_url, 'news': '/news/all', 'news_intraday': '/news/intraday'},
    'NEWSAPI': {'base_url': API_CONFIGS['NEWSAPI'].base_url, 'everything': '/everything', 'top_headlines': '/top-headlines'},
    'EODHD': {'base_url': API_CONFIGS['EODHD'].base_url, 'eod': '/eod', 'real_time': '/real-time', 'fundamentals': '/fundamentals'}
}
RATE_LIMITS = {name: config.rate_limit for name, config in API_CONFIGS.items()}

# ==============================================================================
# REFACTORED DECORATORS AND MIXINS
# ==============================================================================

import functools
from typing import Callable, Any, Optional

def retry_on_failure(max_retries: int = 3, delay: float = 1.0, backoff: float = 2.0):
    """
    Decorator to retry function execution on failure
    
    Args:
        max_retries: Maximum number of retry attempts
        delay: Initial delay between retries in seconds
        backoff: Multiplier for delay after each retry
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            current_delay = delay
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries:
                        logging.warning(f"Attempt {attempt + 1} failed for {func.__name__}: {e}")
                        time.sleep(current_delay)
                        current_delay *= backoff
                    else:
                        logging.error(f"All {max_retries + 1} attempts failed for {func.__name__}")
            
            raise last_exception
        return wrapper
    return decorator

def log_execution_time(func: Callable) -> Callable:
    """Decorator to log function execution time"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            execution_time = time.time() - start_time
            logging.info(f"{func.__name__} executed in {execution_time:.3f} seconds")
            return result
        except Exception as e:
            execution_time = time.time() - start_time
            logging.error(f"{func.__name__} failed after {execution_time:.3f} seconds: {e}")
            raise
    return wrapper

class LoggingMixin:
    """Mixin to add consistent logging capabilities"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def log_info(self, message: str, **kwargs):
        """Log info message with optional context"""
        if kwargs:
            message = f"{message} | {kwargs}"
        self.logger.info(message)
    
    def log_warning(self, message: str, **kwargs):
        """Log warning message with optional context"""
        if kwargs:
            message = f"{message} | {kwargs}"
        self.logger.warning(message)
    
    def log_error(self, message: str, **kwargs):
        """Log error message with optional context"""
        if kwargs:
            message = f"{message} | {kwargs}"
        self.logger.error(message)

class ValidationMixin:
    """Mixin to add input validation capabilities"""
    
    def validate_not_none(self, value: Any, name: str) -> None:
        """Validate that value is not None"""
        if value is None:
            raise ValueError(f"{name} cannot be None")
    
    def validate_positive(self, value: float, name: str) -> None:
        """Validate that value is positive"""
        if value <= 0:
            raise ValueError(f"{name} must be positive, got {value}")
    
    def validate_range(self, value: float, min_val: float, max_val: float, name: str) -> None:
        """Validate that value is within range"""
        if not (min_val <= value <= max_val):
            raise ValueError(f"{name} must be between {min_val} and {max_val}, got {value}")

# ==============================================================================
# DETAILED LOGGING CONFIGURATION FOR ANALYSIS BOT
# ==============================================================================

def setup_detailed_logging() -> Dict[str, logging.Logger]:
    """
    Setup enhanced logging for Analysis Bot with improved error handling and better formatting
    
    Returns:
        Dictionary of configured loggers
        
    Raises:
        OSError: If unable to create logs directory
        Exception: If logging configuration fails
    """
    try:
        # Create logs directory if it doesn't exist
        from pathlib import Path
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        # Enhanced logging format with colors, emojis, and better structure
        class EnhancedColoredFormatter(logging.Formatter):
            """Custom formatter with colors, emojis, and better structure for console output"""
            COLORS = {
                'DEBUG': '\033[36m',    # Cyan
                'INFO': '\033[32m',     # Green
                'WARNING': '\033[33m',   # Yellow
                'ERROR': '\033[31m',    # Red
                'CRITICAL': '\033[35m', # Magenta
                'RESET': '\033[0m'      # Reset
            }
            
            # Emoji mapping for different log levels and categories
            EMOJIS = {
                'DEBUG': '🔍',
                'INFO': 'ℹ️',
                'WARNING': '⚠️',
                'ERROR': '❌',
                'CRITICAL': '🚨'
            }
            
            # Category emojis for better visual organization
            CATEGORY_EMOJIS = {
                'BotAnalysis': '🤖',
                'RLStrategy': '🧠',
                'EnsembleStrategy': '🎯',
                'DataManager': '📊',
                'FeatureEngineer': '⚙️',
                'PositionManager': '💼',
                'RiskManager': '🛡️',
                'NewsAnalyzer': '📰',
                'DiscordAlerts': '💬',
                'APIMonitor': '🔗',
                'PerformanceMonitor': '📈',
                'AutoRetrain': '🔄'
            }
            
            def format(self, record):
                # Add emoji and color to levelname
                level_emoji = self.EMOJIS.get(record.levelname, '📝')
                if record.levelname in self.COLORS:
                    record.levelname = f"{self.COLORS[record.levelname]}{level_emoji} {record.levelname}{self.COLORS['RESET']}"
                else:
                    record.levelname = f"{level_emoji} {record.levelname}"
                
                # Add category emoji
                category_emoji = self.CATEGORY_EMOJIS.get(record.name, '📋')
                record.name = f"{category_emoji} {record.name}"
                
                # Format timestamp with better readability
                record.asctime = datetime.fromtimestamp(record.created).strftime('%H:%M:%S')
                
                # Add visual separators for better readability
                if record.levelname in ['ERROR', 'CRITICAL']:
                    record.msg = f"🔥 {record.msg}"
                elif record.levelname == 'WARNING':
                    record.msg = f"⚠️ {record.msg}"
                elif 'success' in str(record.msg).lower():
                    record.msg = f"✅ {record.msg}"
                elif 'error' in str(record.msg).lower() or 'failed' in str(record.msg).lower():
                    record.msg = f"❌ {record.msg}"
                elif 'start' in str(record.msg).lower() or 'begin' in str(record.msg).lower():
                    record.msg = f"🚀 {record.msg}"
                elif 'complete' in str(record.msg).lower() or 'finish' in str(record.msg).lower():
                    record.msg = f"🏁 {record.msg}"
                
                return super().format(record)
        
        # Console handler with enhanced colors and emojis
        console_handler = logging.StreamHandler()
        console_formatter = EnhancedColoredFormatter(
            '%(asctime)s | %(levelname)-12s | %(name)-25s | %(message)s'
        )
        console_handler.setFormatter(console_formatter)
        console_handler.setLevel(logging.INFO)
        
        # File handler with detailed format
        file_handler = logging.FileHandler(
            log_dir / f'bot_trading_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', 
            mode='w', 
            encoding='utf-8'
        )
        file_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(name)-20s | %(funcName)-15s:%(lineno)-4d | %(message)s'
        )
        file_handler.setFormatter(file_formatter)
        file_handler.setLevel(logging.DEBUG)
        
        # Configure root logger
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.DEBUG)
        root_logger.handlers.clear()
        root_logger.addHandler(console_handler)
        root_logger.addHandler(file_handler)
        
        # Create separate loggers for each module with enhanced configuration
        logger_configs = {
            'BotAnalysis': 'Main bot analysis logger',
            'RLStrategy': 'Reinforcement learning strategy logger', 
            'EnsembleStrategy': 'Ensemble model strategy logger',
            'DataManager': 'Data management logger',
            'FeatureEngineer': 'Feature engineering logger',
            'PositionManager': 'Position management logger',
            'RiskManager': 'Risk management logger',
            'NewsAnalyzer': 'News analysis logger',
            'DiscordAlerts': 'Discord alerts logger',
            'APIMonitor': 'API monitoring logger',
            'PerformanceMonitor': 'Performance monitoring logger',
            'AutoRetrain': 'Auto retrain manager logger'
        }
        
        loggers = {}
        for logger_name, description in logger_configs.items():
            logger = logging.getLogger(logger_name)
            logger.setLevel(logging.DEBUG)
            logger.propagate = True
            loggers[logger_name] = logger
        
        # Disable logging from external libraries to reduce noise
        external_loggers = ['urllib3', 'requests', 'tensorflow', 'torch', 'sklearn', 'matplotlib', 'numpy']
        for logger_name in external_loggers:
            logging.getLogger(logger_name).setLevel(logging.WARNING)
        
        print("🔧 [Logging] Enhanced logging setup completed for Analysis Bot")
        print(f"📁 [Logging] Log files saved to: {log_dir.absolute()}")
        return loggers
        
    except OSError as e:
        print(f"❌ [Logging] Failed to create logs directory: {e}")
        raise
    except Exception as e:
        print(f"❌ [Logging] Failed to setup logging: {e}")
        raise

# Initialize logging immediately when importing
BOT_LOGGERS = setup_detailed_logging()

# ==============================================================================
# ENHANCED LOG MANAGEMENT AND FILTERING UTILITIES
# ==============================================================================

class LogManager:
    """Enhanced log manager with filtering, categorization, and analytics"""
    
    def __init__(self):
        self.log_categories = {
            'TRADING': ['RLStrategy', 'EnsembleStrategy', 'PositionManager'],
            'DATA': ['DataManager', 'FeatureEngineer', 'APIMonitor'],
            'ANALYSIS': ['NewsAnalyzer', 'BotAnalysis', 'PerformanceMonitor'],
            'SYSTEM': ['AutoRetrain', 'DiscordAlerts', 'RiskManager']
        }
        
        self.priority_levels = {
            'HIGH': ['ERROR', 'CRITICAL'],
            'MEDIUM': ['WARNING'],
            'LOW': ['INFO', 'DEBUG']
        }
    
    @staticmethod
    def format_log_entry(timestamp: str, level: str, module: str, message: str) -> str:
        """Format a single log entry with enhanced visual formatting"""
        
        # Level emojis and colors
        level_format = {
            'DEBUG': '🔍 DEBUG',
            'INFO': 'ℹ️ INFO',
            'WARNING': '⚠️ WARNING',
            'ERROR': '❌ ERROR',
            'CRITICAL': '🚨 CRITICAL'
        }
        
        # Module emojis
        module_emojis = {
            'BotAnalysis': '🤖',
            'RLStrategy': '🧠',
            'EnsembleStrategy': '🎯',
            'DataManager': '📊',
            'FeatureEngineer': '⚙️',
            'PositionManager': '💼',
            'RiskManager': '🛡️',
            'NewsAnalyzer': '📰',
            'DiscordAlerts': '💬',
            'APIMonitor': '🔗',
            'PerformanceMonitor': '📈',
            'AutoRetrain': '🔄'
        }
        
        formatted_level = level_format.get(level, f'📝 {level}')
        module_emoji = module_emojis.get(module, '📋')
        
        # Add context-aware message formatting
        if 'success' in message.lower() or 'completed' in message.lower():
            message = f"✅ {message}"
        elif 'error' in message.lower() or 'failed' in message.lower():
            message = f"❌ {message}"
        elif 'starting' in message.lower() or 'begin' in message.lower():
            message = f"🚀 {message}"
        elif 'warning' in message.lower():
            message = f"⚠️ {message}"
        
        return f"{timestamp} | {formatted_level:<12} | {module_emoji} {module:<20} | {message}"
    
    def filter_logs_by_time(self, log_content: str, hours_back: int = 1) -> str:
        """Filter logs by time range (last N hours)"""
        from datetime import datetime, timedelta
        
        current_time = datetime.now()
        cutoff_time = current_time - timedelta(hours=hours_back)
        
        filtered_lines = []
        for line in log_content.split('\n'):
            if line.strip():
                try:
                    # Extract timestamp from log line
                    timestamp_str = line.split('|')[0].strip()
                    if len(timestamp_str) >= 19:  # Basic timestamp format check
                        # Try different timestamp formats
                        for fmt in ['%Y-%m-%d %H:%M:%S,%f', '%Y-%m-%d %H:%M:%S', '%H:%M:%S']:
                            try:
                                if fmt == '%H:%M:%S':
                                    # For time-only format, assume today's date
                                    timestamp = datetime.strptime(timestamp_str, fmt).replace(
                                        year=current_time.year,
                                        month=current_time.month,
                                        day=current_time.day
                                    )
                                else:
                                    timestamp = datetime.strptime(timestamp_str, fmt)
                                
                                if timestamp >= cutoff_time:
                                    filtered_lines.append(line)
                                break
                            except ValueError:
                                continue
                except (IndexError, ValueError):
                    # If timestamp parsing fails, include the line
                    filtered_lines.append(line)
        
        return '\n'.join(filtered_lines)
    
    def filter_logs_by_level(self, log_content: str, min_level: str = 'INFO') -> str:
        """Filter logs by minimum level (DEBUG < INFO < WARNING < ERROR < CRITICAL)"""
        level_hierarchy = {'DEBUG': 0, 'INFO': 1, 'WARNING': 2, 'ERROR': 3, 'CRITICAL': 4}
        min_level_value = level_hierarchy.get(min_level.upper(), 1)
        
        filtered_lines = []
        for line in log_content.split('\n'):
            if line.strip():
                try:
                    parts = line.split('|')
                    if len(parts) >= 2:
                        level_part = parts[1].strip()
                        # Extract level name from formatted level (remove emojis)
                        for level_name in level_hierarchy.keys():
                            if level_name in level_part:
                                if level_hierarchy[level_name] >= min_level_value:
                                    filtered_lines.append(line)
                                break
                except (IndexError, ValueError):
                    filtered_lines.append(line)
        
        return '\n'.join(filtered_lines)
    
    def filter_logs_by_category(self, log_content: str, category: str) -> str:
        """Filter logs by category (TRADING, DATA, ANALYSIS, SYSTEM)"""
        if category.upper() not in self.log_categories:
            return log_content
        
        target_modules = self.log_categories[category.upper()]
        filtered_lines = []
        
        for line in log_content.split('\n'):
            if line.strip():
                for module in target_modules:
                    if module in line:
                        filtered_lines.append(line)
                        break
        
        return '\n'.join(filtered_lines)
    
    def generate_log_summary(self, log_content: str) -> dict:
        """Generate summary statistics from log content"""
        summary = {
            'total_entries': 0,
            'by_level': {'DEBUG': 0, 'INFO': 0, 'WARNING': 0, 'ERROR': 0, 'CRITICAL': 0},
            'by_category': {'TRADING': 0, 'DATA': 0, 'ANALYSIS': 0, 'SYSTEM': 0, 'OTHER': 0},
            'recent_errors': [],
            'performance_metrics': {}
        }
        
        for line in log_content.split('\n'):
            if line.strip():
                summary['total_entries'] += 1
                
                # Count by level
                for level in summary['by_level'].keys():
                    if level in line:
                        summary['by_level'][level] += 1
                        
                        # Collect recent errors
                        if level in ['ERROR', 'CRITICAL'] and len(summary['recent_errors']) < 10:
                            summary['recent_errors'].append(line)
                        break
                
                # Count by category
                categorized = False
                for category, modules in self.log_categories.items():
                    for module in modules:
                        if module in line:
                            summary['by_category'][category] += 1
                            categorized = True
                            break
                    if categorized:
                        break
                
                if not categorized:
                    summary['by_category']['OTHER'] += 1
        
        return summary
    
    def print_log_summary(self, summary: dict):
        """Print formatted log summary"""
        print("\n" + "="*60)
        print("📊 LOG SUMMARY REPORT")
        print("="*60)
        
        print(f"\n📈 Total Entries: {summary['total_entries']}")
        
        print("\n🏷️ By Level:")
        for level, count in summary['by_level'].items():
            if count > 0:
                emoji = {'DEBUG': '🔍', 'INFO': 'ℹ️', 'WARNING': '⚠️', 'ERROR': '❌', 'CRITICAL': '🚨'}
                print(f"   {emoji.get(level, '📝')} {level}: {count}")
        
        print("\n📋 By Category:")
        for category, count in summary['by_category'].items():
            if count > 0:
                emoji = {'TRADING': '💹', 'DATA': '📊', 'ANALYSIS': '🔬', 'SYSTEM': '⚙️', 'OTHER': '📄'}
                print(f"   {emoji.get(category, '📄')} {category}: {count}")
        
        if summary['recent_errors']:
            print(f"\n🚨 Recent Errors ({len(summary['recent_errors'])}):")
            for error in summary['recent_errors'][:5]:  # Show only first 5
                print(f"   • {error[:80]}...")
        
        print("="*60 + "\n")

# Create global log manager instance
LOG_MANAGER = LogManager()

# ==============================================================================
# LOG UTILITY FUNCTIONS
# ==============================================================================

def analyze_current_logs(log_file_path: str = "Log.txt"):
    """Analyze current log file and display enhanced summary"""
    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            log_content = f.read()
        
        print("🔍 ANALYZING LOG FILE...")
        print(f"📁 File: {log_file_path}")
        print("-" * 60)
        
        # Generate and display summary
        summary = LOG_MANAGER.generate_log_summary(log_content)
        LOG_MANAGER.print_log_summary(summary)
        
        # Show recent activity (last 1 hour)
        print("⏰ RECENT ACTIVITY (Last 1 hour):")
        recent_logs = LOG_MANAGER.filter_logs_by_time(log_content, hours_back=1)
        recent_lines = recent_logs.split('\n')
        for line in recent_lines[-10:]:  # Show last 10 lines
            if line.strip():
                print(f"   {line}")
        
        # Show only errors and warnings
        print("\n🚨 ERRORS & WARNINGS:")
        error_logs = LOG_MANAGER.filter_logs_by_level(log_content, min_level='WARNING')
        error_lines = error_logs.split('\n')
        for line in error_lines[-5:]:  # Show last 5 errors/warnings
            if line.strip():
                print(f"   {line}")
        
        return summary
        
    except FileNotFoundError:
        print(f"❌ Log file not found: {log_file_path}")
        return None
    except Exception as e:
        print(f"❌ Error analyzing logs: {e}")
        return None

def filter_and_display_logs(log_file_path: str = "Log.txt", 
                          category: str = None, 
                          level: str = "INFO", 
                          hours_back: int = 24):
    """Filter and display logs with enhanced formatting"""
    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            log_content = f.read()
        
        # Apply filters
        filtered_content = log_content
        
        if hours_back:
            filtered_content = LOG_MANAGER.filter_logs_by_time(filtered_content, hours_back)
        
        if level:
            filtered_content = LOG_MANAGER.filter_logs_by_level(filtered_content, level)
        
        if category:
            filtered_content = LOG_MANAGER.filter_logs_by_category(filtered_content, category)
        
        # Display filtered logs
        print(f"🔍 FILTERED LOGS - Category: {category or 'ALL'}, Level: {level}, Time: Last {hours_back}h")
        print("-" * 80)
        
        lines = filtered_content.split('\n')
        for line in lines:
            if line.strip():
                print(line)
        
        print(f"\n📊 Total filtered entries: {len([l for l in lines if l.strip()])}")
        
    except FileNotFoundError:
        print(f"❌ Log file not found: {log_file_path}")
    except Exception as e:
        print(f"❌ Error filtering logs: {e}")

def save_filtered_logs(source_file: str = "Log.txt", 
                      output_file: str = "filtered_logs.txt",
                      category: str = None,
                      level: str = "INFO",
                      hours_back: int = 24):
    """Save filtered logs to a new file"""
    try:
        with open(source_file, 'r', encoding='utf-8') as f:
            log_content = f.read()
        
        # Apply filters
        filtered_content = log_content
        
        if hours_back:
            filtered_content = LOG_MANAGER.filter_logs_by_time(filtered_content, hours_back)
        
        if level:
            filtered_content = LOG_MANAGER.filter_logs_by_level(filtered_content, level)
        
        if category:
            filtered_content = LOG_MANAGER.filter_logs_by_category(filtered_content, category)
        
        # Save to new file
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# FILTERED LOGS - Category: {category or 'ALL'}, Level: {level}, Time: Last {hours_back}h\n")
            f.write(f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("# " + "="*70 + "\n\n")
            f.write(filtered_content)
        
        lines = filtered_content.split('\n')
        filtered_count = len([l for l in lines if l.strip()])
        
        print(f"✅ Filtered logs saved to: {output_file}")
        print(f"📊 Total entries saved: {filtered_count}")
        
        return output_file
        
    except FileNotFoundError:
        print(f"❌ Source log file not found: {source_file}")
        return None
    except Exception as e:
        print(f"❌ Error saving filtered logs: {e}")
        return None

# ==============================================================================
# ENHANCED CONSOLE OUTPUT UTILITIES
# ==============================================================================

class ConsoleFormatter:
    """Enhanced console output formatting with colors and better structure"""
    
    @staticmethod
    def print_header(title: str, char: str = "=", width: int = 60):
        """Print a formatted header"""
        print(f"\n{char * width}")
        print(f" {title.center(width - 2)} ")
        print(f"{char * width}\n")
    
    @staticmethod
    def print_section(title: str, char: str = "-", width: int = 40):
        """Print a formatted section"""
        print(f"\n{char * width}")
        print(f" {title}")
        print(f"{char * width}")
    
    @staticmethod
    def print_success(message: str):
        """Print success message with green color"""
        print(f"ℹ️ {message}")
    
    @staticmethod
    def print_warning(message: str):
        """Print warning message with yellow color"""
        print(f"ℹ️ {message}")
    
    @staticmethod
    def print_error(message: str):
        """Print error message with red color"""
        print(f"ℹ️ {message}")
    
    @staticmethod
    def print_info(message: str):
        """Print info message with blue color"""
        print(f"ℹ️ {message}")
    
    @staticmethod
    def print_progress(current: int, total: int, message: str = ""):
        """Print progress bar"""
        percentage = (current / total) * 100
        bar_length = 30
        filled_length = int(bar_length * current // total)
        bar = "█" * filled_length + "░" * (bar_length - filled_length)
        print(f"\r{message} [{bar}] {percentage:.1f}% ({current}/{total})", end="", flush=True)
        if current == total:
            print()  # New line when complete

# ==============================================================================
# API CALLS MANAGEMENT CLASS
# ==============================================================================

class APIManager:
    """Refactored API Manager with better error handling and rate limiting"""
    
    def __init__(self):
        self.logger = BOT_LOGGERS['DataManager']
        self.rate_limiters = {}
        self.last_request_time = {}
        
        # Initialize rate limiters using refactored config
        for api_name, config in API_CONFIGS.items():
            self.rate_limiters[api_name] = deque()
            self.last_request_time[api_name] = 0
    
    def _check_rate_limit(self, api_name: str) -> bool:
        """
        Check rate limit for API with improved error handling
        
        Args:
            api_name: Name of the API service
            
        Returns:
            True if request is allowed, False if rate limited
        """
        try:
            if api_name not in self.rate_limiters:
                self.logger.error(f"❌ [API] Unknown API name: {api_name}")
                return False
            
            if api_name not in API_CONFIGS:
                self.logger.error(f"⚠️ [API] No configuration found for {api_name}")
                return False
                
            current_time = time.time()
            rate_limit = API_CONFIGS[api_name].rate_limit
            
            # Remove requests older than 1 minute
            while (self.rate_limiters[api_name] and 
                   current_time - self.rate_limiters[api_name][0] > 60):
                self.rate_limiters[api_name].popleft()
            
            # Check if exceeding rate limit
            if len(self.rate_limiters[api_name]) >= rate_limit:
                self.logger.warning(f"⚠️ [{api_name}] Rate limit exceeded ({len(self.rate_limiters[api_name])}/{rate_limit})")
                return False
            
            # Add current request
            self.rate_limiters[api_name].append(current_time)
            return True
            
        except Exception as e:
            self.logger.error(f"❌ [API] Error checking rate limit for {api_name}: {e}")
            return False
    
    def _make_request(self, url: str, params: dict = None, timeout: int = 10) -> Optional[dict]:
        """
        Execute HTTP request with comprehensive error handling
        
        Args:
            url: Request URL
            params: Request parameters
            timeout: Request timeout in seconds
            
        Returns:
            Response data as dictionary or None if failed
        """
        try:
            self.logger.debug(f"🔗 [API] Making request to: {url}")
            
            # Import requests here to avoid circular imports
            import requests
            
            response = requests.get(url, params=params, timeout=timeout)
            response.raise_for_status()
            
            data = response.json()
            self.logger.debug(f"✅ [API] Request successful, data size: {len(str(data))}")
            return data
            
        except requests.exceptions.Timeout:
            self.logger.error(f"⏰ [API] Request timeout: {url}")
            return None
        except requests.exceptions.HTTPError as e:
            self.logger.error(f"❌ [API] HTTP error {e.response.status_code}: {url}")
            return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"❌ [API] Request failed: {url}, error: {e}")
            return None
        except json.JSONDecodeError as e:
            self.logger.error(f"❌ [API] JSON decode error: {url}, error: {e}")
            return None
        except Exception as e:
            self.logger.error(f"❌ [API] Unexpected error: {url}, error: {e}")
            return None
    
    # ==============================================================================
    # FINHUB API METHODS
    # ==============================================================================
    
    def get_finhub_quote(self, symbol: str) -> Optional[dict]:
        """
        Get real-time price from Finhub with improved error handling
        
        Args:
            symbol: Trading symbol to get quote for
            
        Returns:
            Quote data as dictionary or None if failed
        """
        try:
            if not symbol or not isinstance(symbol, str):
                self.logger.error(f"❌ [Finhub] Invalid symbol: {symbol}")
                return None
                
            if not self._check_rate_limit('FINHUB'):
                self.logger.warning("⚠️ [Finhub] Rate limit exceeded, skipping request")
                return None
            
            url = f"{API_ENDPOINTS['FINHUB']['base_url']}{API_ENDPOINTS['FINHUB']['quote']}"
            params = {
                'symbol': symbol.upper(),
                'token': API_KEYS['FINHUB']
            }
            
            self.logger.debug(f"📊 [Finhub] Getting quote for {symbol}")
            result = self._make_request(url, params)
            
            if result:
                self.logger.debug(f"✅ [Finhub] Quote retrieved for {symbol}")
            else:
                self.logger.warning(f"❌ [Finhub] Failed to get quote for {symbol}")
                
            return result
            
        except Exception as e:
            self.logger.error(f"❌ [Finhub] Error getting quote for {symbol}: {e}")
            return None
    
    def get_finhub_news(self, symbol: str, from_date: Optional[str] = None, to_date: Optional[str] = None) -> Optional[dict]:
        """
        Get news from Finhub with improved error handling
        
        Args:
            symbol: Trading symbol to get news for
            from_date: Start date for news (optional)
            to_date: End date for news (optional)
            
        Returns:
            News data as dictionary or None if failed
        """
        try:
            if not symbol or not isinstance(symbol, str):
                self.logger.error(f"❌ [Finhub] Invalid symbol: {symbol}")
                return None
                
            if not self._check_rate_limit('FINHUB'):
                self.logger.warning("⚠️ [Finhub] Rate limit exceeded, skipping request")
                return None
            
            url = f"{API_ENDPOINTS['FINHUB']['base_url']}{API_ENDPOINTS['FINHUB']['news']}"
            params = {
                'symbol': symbol.upper(),
                'token': API_KEYS['FINHUB']
            }
            
            if from_date:
                params['from'] = from_date
            if to_date:
                params['to'] = to_date
            
            self.logger.debug(f"📰 [Finhub] Getting news for {symbol}")
            result = self._make_request(url, params)
            
            if result:
                self.logger.debug(f"✅ [Finhub] News retrieved for {symbol}")
            else:
                self.logger.warning(f"❌ [Finhub] Failed to get news for {symbol}")
                
            return result
            
        except Exception as e:
            self.logger.error(f"❌ [Finhub] Error getting news for {symbol}: {e}")
            return None
    
    def get_finhub_sentiment(self, symbol: str) -> Optional[dict]:
        """
        Get sentiment from Finhub with improved error handling
        
        Args:
            symbol: Trading symbol to get sentiment for
            
        Returns:
            Sentiment data as dictionary or None if failed
        """
        try:
            if not symbol or not isinstance(symbol, str):
                self.logger.error(f"❌ [Finhub] Invalid symbol: {symbol}")
                return None
                
            if not self._check_rate_limit('FINHUB'):
                self.logger.warning("⚠️ [Finhub] Rate limit exceeded, skipping request")
                return None
            
            url = f"{API_ENDPOINTS['FINHUB']['base_url']}{API_ENDPOINTS['FINHUB']['sentiment']}"
            params = {
                'symbol': symbol.upper(),
                'token': API_KEYS['FINHUB']
            }
            
            self.logger.debug(f"💭 [Finhub] Getting sentiment for {symbol}")
            result = self._make_request(url, params)
            
            if result:
                self.logger.debug(f"✅ [Finhub] Sentiment retrieved for {symbol}")
            else:
                self.logger.warning(f"❌ [Finhub] Failed to get sentiment for {symbol}")
                
            return result
            
        except Exception as e:
            self.logger.error(f"❌ [Finhub] Error getting sentiment for {symbol}: {e}")
            return None
    
    # ==============================================================================
    # MARKETAUX API METHODS
    # ==============================================================================
    
    def get_marketaux_news(self, symbols: list = None, limit: int = 10) -> dict:
        """Get news from Marketaux"""
        if not self._check_rate_limit('MARKETAUX'):
            self.logger.warning("⚠️ [Marketaux] Rate limit exceeded, skipping request")
            return None
        
        url = f"{API_ENDPOINTS['MARKETAUX']['base_url']}{API_ENDPOINTS['MARKETAUX']['news']}"
        params = {
            'api_token': API_KEYS['MARKETAUX'],
            'filter_entities': 'true',
            'language': 'en',
            'limit': limit
        }
        
        if symbols:
            params['symbols'] = ','.join(symbols)
        
        self.logger.debug(f"📰 [Marketaux] Getting news for symbols: {symbols}")
        return self._make_request(url, params)
    
    def get_marketaux_intraday_news(self, symbols: list = None) -> dict:
        """Get intraday news from Marketaux"""
        if not self._check_rate_limit('MARKETAUX'):
            self.logger.warning("⚠️ [Marketaux] Rate limit exceeded, skipping request")
            return None
        
        url = f"{API_ENDPOINTS['MARKETAUX']['base_url']}{API_ENDPOINTS['MARKETAUX']['news_intraday']}"
        params = {
            'api_token': API_KEYS['MARKETAUX'],
            'filter_entities': 'true',
            'language': 'en'
        }
        
        if symbols:
            params['symbols'] = ','.join(symbols)
        
        self.logger.debug(f"📰 [Marketaux] Getting intraday news for symbols: {symbols}")
        return self._make_request(url, params)
    
    # ==============================================================================
    # NEWSAPI METHODS
    # ==============================================================================
    
    def get_newsapi_everything(self, query: str, language: str = 'en', sort_by: str = 'publishedAt') -> dict:
        """Get news from NewsAPI"""
        if not self._check_rate_limit('NEWSAPI'):
            self.logger.warning("⚠️ [NewsAPI] Rate limit exceeded, skipping request")
            return None
        
        url = f"{API_ENDPOINTS['NEWSAPI']['base_url']}{API_ENDPOINTS['NEWSAPI']['everything']}"
        params = {
            'q': query,
            'apiKey': API_KEYS['NEWSAPI'],
            'language': language,
            'sortBy': sort_by,
            'pageSize': 20
        }
        
        self.logger.debug(f"📰 [NewsAPI] Getting news for query: {query}")
        return self._make_request(url, params)
    
    def get_newsapi_top_headlines(self, category: str = 'business', country: str = 'us') -> dict:
        """Get top headlines from NewsAPI"""
        if not self._check_rate_limit('NEWSAPI'):
            self.logger.warning("⚠️ [NewsAPI] Rate limit exceeded, skipping request")
            return None
        
        url = f"{API_ENDPOINTS['NEWSAPI']['base_url']}{API_ENDPOINTS['NEWSAPI']['top_headlines']}"
        params = {
            'category': category,
            'country': country,
            'apiKey': API_KEYS['NEWSAPI'],
            'pageSize': 20
        }
        
        self.logger.debug(f"📰 [NewsAPI] Getting top headlines for category: {category}")
        return self._make_request(url, params)
    
    # ==============================================================================
    # EODHD API METHODS
    # ==============================================================================
    
    def get_eodhd_data(self, symbol: str, from_date: str = None, to_date: str = None) -> dict:
        """Get EOD data from EODHD"""
        if not self._check_rate_limit('EODHD'):
            self.logger.warning("⚠️ [EODHD] Rate limit exceeded, skipping request")
            return None
        
        url = f"{API_ENDPOINTS['EODHD']['base_url']}{API_ENDPOINTS['EODHD']['eod']}/{symbol}"
        params = {
            'api_token': API_KEYS['EODHD'],
            'fmt': 'json'
        }
        
        if from_date:
            params['from'] = from_date
        if to_date:
            params['to'] = to_date
        
        self.logger.debug(f"📊 [EODHD] Getting EOD data for {symbol}")
        return self._make_request(url, params)
    
    def get_eodhd_real_time(self, symbol: str) -> dict:
        """Get real-time price from EODHD"""
        if not self._check_rate_limit('EODHD'):
            self.logger.warning("⚠️ [EODHD] Rate limit exceeded, skipping request")
            return None
        
        url = f"{API_ENDPOINTS['EODHD']['base_url']}{API_ENDPOINTS['EODHD']['real_time']}/{symbol}"
        params = {
            'api_token': API_KEYS['EODHD'],
            'fmt': 'json'
        }
        
        self.logger.debug(f"📊 [EODHD] Getting real-time data for {symbol}")
        return self._make_request(url, params)
    
    def get_eodhd_fundamentals(self, symbol: str) -> dict:
        """Get fundamentals data from EODHD"""
        if not self._check_rate_limit('EODHD'):
            self.logger.warning("⚠️ [EODHD] Rate limit exceeded, skipping request")
            return None
        
        url = f"{API_ENDPOINTS['EODHD']['base_url']}{API_ENDPOINTS['EODHD']['fundamentals']}/{symbol}"
        params = {
            'api_token': API_KEYS['EODHD'],
            'fmt': 'json'
        }
        
        self.logger.debug(f"📊 [EODHD] Getting fundamentals for {symbol}")
        return self._make_request(url, params)

# Initialize API Manager
api_manager = APIManager()

# ==============================================================================

# Third-party imports
import aiohttp
# eod imporfrom modeloved to optional section
import google.generativeai as genai
import gymnasium as gym
import lightgbm as lgb
import numpy as np
import pandas as pd
import pytz
import requests
import tensorflow as tf
import torch
import xgboost as xgb

# Scientific computing
from scipy.signal import argrelextrema
from scipy.special import expit

# Machine Learning
from sklearn.base import clone
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.exceptions import NotFittedError
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, brier_score_loss, precision_score, recall_score
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, f_classif
# optuna imports moved to optional section

# Deep Learning
try:
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Attention, Input
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.regularizers import l2
    from tensorflow.keras.layers import BatchNormalization
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("⚠️ TensorFlow is not installed. Some deep learning features will be disabled.")

# Reinforcement Learning
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.env_checker import check_env

# Technical Analysis
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.trend import ADXIndicator, EMAIndicator, MACD
from ta.volatility import AverageTrueRange, BollingerBands

# Sentiment Analysis
# NewsApiClient imporfrom modeloved to optional section
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Configure warnings
warnings.filterwarnings("ignore")

# Import optimization configuration from experiment system
try:
    from strategy_config import STRATEGY_CONFIG_BY_SYMBOL, CONFIG_METADATA
    print(f"✅ Loaded optimization config for {len(STRATEGY_CONFIG_BY_SYMBOL)} symbols")
    print(f"ℹ️ Config version: {CONFIG_METADATA.get('version', 'unknown')}")
    print(f"ℹ️ Created at: {CONFIG_METADATA.get('created_at', 'unknown')}")
except ImportError:
    print("⚠️ No optimization config found, using default configuration")
    STRATEGY_CONFIG_BY_SYMBOL = {}
    CONFIG_METADATA = {}


class UltraOverfittingPrevention:
    """Ultra-sin overfitting prevention system"""

    def __init__(self):
        self.validation_threshold = 0.02  # Ultra-strict: from test results
        self.early_stopping_patience = 3   # Very early stopping
        self.regularization_strength = 0.15  # Very sin regularization from test results
        self.dropout_rate = 0.7  # Very high dropout from test results
        self.batch_normalization = True
        self.data_augmentation = True
        self.cross_validation_folds = 10  # More folds from test results
        self.out_of_sample_testing = True
        self.max_depth_limit = 4  # Very shallow trees from test results
        self.min_samples_split_limit = 50  # Very high minimum from test results
        self.min_samples_leaf_limit = 25   # Very high minimum from test results
        self.max_features_limit = 0.3  # Very few features from test results
        self.cpu_threshold = 80  # CPU optimization from test results
        self.memory_threshold = 85  # Memory optimization from test results
        self.alert_limit = 5  # Reduced alert limit from test results
        self.overfitting_detection = True  # Enable overfitting detection
        self.validation_requirements = "STRICT"  # Strict validation
    def apply_ultra_regularization(self, model, model_type: str = "tree"):
        """Apply ultra-strong regularization to prevent overfitting"""

        if model_type == "tree":
            # For tree-based models - ultra-sin regularization
            if hasattr(model, 'max_depth'):
                model.max_depth = min(model.max_depth, self.max_depth_limit)
            if hasattr(model, 'min_samples_split'):
                model.min_samples_split = max(model.min_samples_split, self.min_samples_split_limit)
            if hasattr(model, 'min_samples_leaf'):
                model.min_samples_leaf = max(model.min_samples_leaf, self.min_samples_leaf_limit)
            if hasattr(model, 'max_features'):
                model.max_features = self.max_features_limit  # Very few features

        return model

    def validate_model_generalization(self, model, X_train, y_train,
                                    X_val, y_val) -> Dict[str, float]:
        """Validate model generalization capability with ultra-strict criteria"""

        # Train performance
        train_pred = model.predict(X_train)
        train_accuracy = accuracy_score(y_train, train_pred)
        train_f1 = f1_score(y_train, train_pred, average='weighted')

        # Validation performance
        val_pred = model.predict(X_val)
        val_accuracy = accuracy_score(y_val, val_pred)
        val_f1 = f1_score(y_val, val_pred, average='weighted')

        # Calculate generalization gap
        accuracy_gap = train_accuracy - val_accuracy
        f1_gap = train_f1 - val_f1

        # Ultra-strict validation
        is_overfitting = (accuracy_gap > self.validation_threshold or
                         f1_gap > self.validation_threshold)

        return {
            'train_accuracy': train_accuracy,
            'val_accuracy': val_accuracy,
            'train_f1': train_f1,
            'val_f1': val_f1,
            'accuracy_gap': accuracy_gap,
            'f1_gap': f1_gap,
            'is_overfitting': is_overfitting,
            'generalization_score': 1.0 - max(accuracy_gap, f1_gap)
        }

    def check_system_resources(self):
        """Check system resourcefixnd optimize if needed"""
        import psutil

        cpu_percent = psutil.cpu_percent(interval=1)
        memory_percent = psutil.virtual_memory().percent

        if cpu_percent > self.cpu_threshold:
            print(f"⚠️ CPU usage high: {cpu_percent}% > {self.cpu_threshold}%")
            return False

        if memory_percent > self.memory_threshold:
            print(f"⚠️ Memory usage high: {memory_percent}% > {self.memory_threshold}%")
            return False

        return True

    def should_send_alert(self, alert_type: str) -> bool:
        """Check if alert should be sent based on limits"""
        # Simple implementation - in production, use proper rate limiting
        return True  # For now, allow all alerts

    def detect_overfitting(self, model_results: dict) -> bool:
        """Detect overfitting in model results"""
        if not self.overfitting_detection:
            return False

        # Check for signs of overfitting
        cv_scores = model_results.get('cv_scores', [])
        if len(cv_scores) > 1:
            cv_std = np.std(cv_scores)
            cv_mean = np.mean(cv_scores)

            # High variance indicates overfitting
            if cv_std > 0.2 or cv_mean < 0.4:
                return True

        return False

    def validate_requirements(self, model_data: dict) -> bool:
        """Validate model meets strict requirements"""
        if self.validation_requirements != "STRICT":
            return True

        # Check all required fields
        required_fields = ['cv_scores', 'feature_importance', 'model_weights']
        for field in required_fields:
            if field not in model_data:
                return False

# Compatibility functions for scikit-learn version differences
def create_calibrated_classifier(estimator, method='isotonic', cv=3):
    """Create CalibratedClassifierCV with version compatibility"""
    try:
        # Try new parameter name (scikit-learn >= 0.24)
        return CalibratedClassifierCV(estimator=estimator, method=method, cv=cv)
    except TypeError:
        try:
            # Fallback to old parameter name (scikit-learn < 0.24)
            return CalibratedClassifierCV(base_estimator=estimator, method=method, cv=cv)
        except Exception as e:
            logging.error(f"CalibratedClassifierCV compatibility error: {e}")
            # Returifncalibrated estimator as last resort
            return estimator

class EnhancedPurgedGroupTimeSeriesSplit:
    """
    Enhanced Purged Group Time Series Split with proper label-based purging
    Based on advances in financial machine learning practices
    """

    def __init__(self, n_splits=5, embargo_period=5, label_horizon=1, gap=0, max_train_size=None):
        self.n_splits = n_splits
        self.embargo_period = embargo_period
        self.label_horizon = label_horizon
        self.gap = gap
        self.max_train_size = max_train_size

    def split(self, X, y=None, groups=None):
        """Generate indices to split data into training and test set with proper purging"""
        n_samples = len(X)
        indices = np.arange(n_samples)

        # Calculate label-based purging
        purged_indices = self._calculate_purged_indices(y, groups) if y is not None else set()

        # Calculate test size for each fold
        test_size = n_samples // (self.n_splits + 1)

        for i in range(self.n_splits):
            # Test set boundaries
            test_start = (i + 1) * test_size
            test_end = min((i + 2) * test_size, n_samples)

            # Apply embargo period
            embargo_start = max(0, test_start - self.embargo_period)
            embargo_end = min(n_samples, test_end + self.embargo_period)

            # Training set (before embargo)
            train_indices = indices[:embargo_start]

            # Remove purged indices from training
            train_indices = [idx for idx in train_indices if idx not in purged_indices]

            # Test set
            test_indices = indices[test_start:test_end]

            # Apply max_train_size if specified
            if self.max_train_size and len(train_indices) > self.max_train_size:
                train_indices = train_indices[-self.max_train_size:]

            yield train_indices, test_indices

    def _calculate_purged_indices(self, y, groups):
        """Calculate indices to purge based on label horizon and overlap"""
        purged_indices = set()

        if y is None or len(y) < self.label_horizon + self.embargo_period:
            return purged_indices

        # Calculate purging based on label horizon
        for i in range(len(y) - self.label_horizon):
            # Check if current label affects future labels within horizon
            if self._has_label_overlap(y, i, self.label_horizon):
                # Purge overlapping periods
                purge_start = max(0, i - self.embargo_period)
                purge_end = min(len(y), i + self.label_horizon + self.embargo_period)

                for j in range(purge_start, purge_end):
                    purged_indices.add(j)

        # Additional purging based on groups if provided
        if groups is not None:
            purged_indices.update(self._calculate_group_purged_indices(groups))

        return purged_indices

    def _has_label_overlap(self, y, start_idx, horizon):
        """Check if labels overlap within horizon"""
        if start_idx + horizon >= len(y):
            return False

        # Check for overlapping information in labels
        # This ifix simplified implementation - can be enhanced based on specific use case
        current_label = y[start_idx]

        for i in range(start_idx + 1, min(start_idx + horizon + 1, len(y))):
            if y[i] == current_label:
                return True

        return False

    def _calculate_group_purged_indices(self, groups):
        """Calculate purged indices based on groups"""
        purged_indices = set()

        if groups is None:
            return purged_indices

        # Group consecutive samples with same group ID
        current_group = groups[0]
        group_start = 0

        for i in range(1, len(groups)):
            if groups[i] != current_group:
                # End of current group, purge overlap
                group_end = i - 1

                # Purge overlap between groups
                overlap_start = max(0, group_start - self.embargo_period)
                overlap_end = min(len(groups), group_end + self.embargo_period)

                for j in range(overlap_start, overlap_end):
                    purged_indices.add(j)

                # Start new group
                current_group = groups[i]
                group_start = i

        # Handle last group
        group_end = len(groups) - 1
        overlap_start = max(0, group_start - self.embargo_period)
        overlap_end = min(len(groups), group_end + self.embargo_period)

        for j in range(overlap_start, overlap_end):
            purged_indices.add(j)

        return purged_indices

class PurgedGroupTimeSeriesSplit:
    """
    Custom implementation of Purged Group Time Series Split for financial data
    Based on advances in financial machine learning practices
    """

    def __init__(self, n_splits=5, gap=0, max_train_size=None):
        self.n_splits = n_splits
        self.gap = gap
        self.max_train_size = max_train_size

    def split(self, X, y=None, groups=None):
        """Generate indices to split data into training and test set"""
        n_samples = len(X)
        indices = np.arange(n_samples)

        # Calculate test size for each fold
        test_size = n_samples // (self.n_splits + 1)

        for i in range(self.n_splits):
            # Test set boundaries
            test_start = (i + 1) * test_size
            test_end = min(test_start + test_size, n_samples)

            if test_start >= n_samples:
                break

            # Training set boundaries (before test set with gap)
            train_end = max(0, test_start - self.gap)
            train_start = 0

            # Apply max_train_size if specified
            if self.max_train_size and (train_end - train_start) > self.max_train_size:
                train_start = train_end - self.max_train_size

            # Ensure we have valid training data
            if train_start < train_end and test_start < test_end:
                train_indices = indices[train_start:train_end]
                test_indices = indices[test_start:test_end]

                yield train_indices, test_indices

    def get_n_splits(self, X=None, y=None, groups=None):
        """Returns the number of splitting iterations"""
        return self.n_splits

def safe_cross_val_score(estimator, X, y, cv=5, scoring='f1', n_jobs=None):
    """Safe cross-validation with error handling"""
    try:
        return cross_val_score(estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs)
    except Exception as e:
        logging.warning(f"Cross-validation error: {e}")
        # Fallback to simple train-test split
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        estimator.fit(X_train, y_train)
        y_pred = estimator.predict(X_test)
        from sklearn.metrics import f1_score
        return [f1_score(y_test, y_pred)]

# Optional GPU configuration for TensorFlow
# physical_devices = tf.config.list_physical_devices('GPU')
# if len(physical_devices) > 0:
#     tf.config.set_visible_devices([], 'GPU')

GOOGLE_AI_API_KEY = "AIzaSyBCexoODvgrN2QRG8_iKv3p5VTJ5jaJ_B0"
TRADING_ECONOMICS_API_KEY = "a284ad0cdba547c:p5oyv77j6kovqhv"  # Trading Economics API credentials

# Risk Management Configuration
RISK_MANAGEMENT = {
    "MAX_RISK_PER_TRADE": 0.02,  # 2% max risk per trade
    "MAX_PORTFOLIO_RISK": 0.10,  # 10% max total portfolio risk
    "MAX_OPEN_POSITIONS": 5,     # Maximum number of open positions
    "VOLATILITY_LOOKBACK": 20,   # ATR lookback period
    "SL_ATR_MULTIPLIER": 1.5,    # Stop losfixTR multiplier
    "BASE_RR_RATIO": 1.5,        # Base risk-reward ratio
    "TRAILING_STOP_MULTIPLIER": 1.0,  # Trailing stop multiplier
}

# === CONSTANTS ===
# Market timing constants
FOREX_MARKET_CLOSE_HOUR_UTC = 21  # Friday close time
WEEKEND_FRIDAY = 4
WEEKEND_SATURDAY = 5
WEEKEND_SUNDAY = 6

# Cache timeout constants (seconds)
NEWS_CACHE_TIMEOUT = 3600  # 1 hour
ECONOMIC_CALENDAR_CACHE_TIMEOUT = 86400  # 24 hours

# Trading constants
DEFAULT_CANDLE_WAIT_MINUTES = 60  # H1 default
CRYPTO_PREFIXES = ["BTC", "ETH"]

# Symbol alias mapping for different broker naming conventions
SYMBOL_ALIAS = {
    "BTCUSD": ["BTC-USD", "BTCUSDT", "XBTUSD"],
    "ETHUSD": ["ETH-USD", "ETHUSDT"]
}

# Feature engineering constants
DEFAULT_ATR_MULTIPLIER = 2.0
DEFAULT_RANGE_WINDOW = 50
DEFAULT_EMA_PERIOD = 200
DEFAULT_ADX_THRESHOLD = 25

# Advanced ML constants
STACKING_CV_FOLDS = 5
CALIBRATION_CV_FOLDS = 3
MIN_SAMPLES_LEAF = 50
OPTUNA_N_TRIALS = 100
OPTUNA_TIMEOUT = 3600  # 1 hour

# Quality gates thresholds
MIN_SHARPE_RATIO = 1.2
MAX_DRAWDOWN_THRESHOLD = 0.15
MIN_CALMAR_RATIO = 0.8
MIN_INFORMATION_RATIO = 0.5

# Enhanced risk management configuration by asset class
RISK_CONFIG_BY_ASSET_CLASS = {
    "equity_index": {
        "max_position_size": 0.25,  # 25% max per equity index
        "correlation_threshold": 0.7,
        "var_multiplier": 1.0,
        "stop_loss_atr": 2.0,
        "take_profit_atr": 4.0,
        "max_daily_loss": 0.02,
        "session_risk_adjustment": True,
        "gap_risk_factor": 1.5
    },
    "commodity": {
        "max_position_size": 0.20,  # 20% max per commodity
        "correlation_threshold": 0.6,
        "var_multiplier": 1.2,
        "stop_loss_atr": 2.5,
        "take_profit_atr": 5.0,
        "max_daily_loss": 0.025,
        "session_risk_adjustment": False,
        "gap_risk_factor": 1.0
    },
    "forex": {
        "max_position_size": 0.15,  # 15% max per forex pair
        "correlation_threshold": 0.8,
        "var_multiplier": 0.8,
        "stop_loss_atr": 1.5,
        "take_profit_atr": 3.0,
        "max_daily_loss": 0.015,
        "session_risk_adjustment": True,
        "gap_risk_factor": 1.2
    },
    "cryptocurrency": {
        "max_position_size": 0.10,  # 10% max per crypto
        "correlation_threshold": 0.5,
        "var_multiplier": 1.5,
        "stop_loss_atr": 3.0,
        "take_profit_atr": 6.0,
        "max_daily_loss": 0.03,
        "session_risk_adjustment": False,
        "gap_risk_factor": 1.0
    }
}

# Portfolio-level risk limits
PORTFOLIO_RISK_LIMITS = {
    "max_total_exposure": 0.80,  # 80% max total portfolio exposure
    "max_asset_class_exposure": 0.50,  # 50% max per asset class
    "max_correlation_exposure": 0.40,  # 40% max for highly correlated positions
    "daily_var_limit": 0.02,  # 2% daily VaR limit
    "weekly_var_limit": 0.05,  # 5% weekly VaR limit
    "max_drawdown_limit": 0.15,  # 15% max drawdown
    "concentration_limit": 0.30  # 30% max concentration in single position
}

# Observability constants
DISCORD_RATE_LIMIT_SECONDS = 60
ALERT_COOLDOWN_MINUTES = 15
PERFORMANCE_LOG_INTERVAL = 24  # hours

# === DATA FRESHNESS LIMITS (minutes) ===
DATA_FRESHNESS_LIMIT_MIN = {
    "H1": 10080,  # 7 days (more lenient for H1 to handle weekends and market closures)
    "H4": 20160,  # 14 days (more lenient for H4)
    "D1": 43200,  # 30 days (more lenient for D1)
    "W1": 86400   # 60 days (more lenient for W1)
}

# === ENHANCED SYMBOL EXCHANGE MAPPING ===
ENHANCED_SYMBOL_EXCHANGE_MAP = {
    # Forex (24/5)
    "EURUSD": {"exchange": "FOREX", "session": "24/5", "breaks": []},
    "GBPUSD": {"exchange": "FOREX", "session": "24/5", "breaks": []},
    "USDJPY": {"exchange": "FOREX", "session": "24/5", "breaks": []},
    "USDCHF": {"exchange": "FOREX", "session": "24/5", "breaks": []},
    "AUDUSD": {"exchange": "FOREX", "session": "24/5", "breaks": []},
    "USDCAD": {"exchange": "FOREX", "session": "24/5", "breaks": []},
    "NZDUSD": {"exchange": "FOREX", "session": "24/5", "breaks": []},

    # Commodities
    "XAUUSD": {"exchange": "COMEX", "session": "23/5", "breaks": [{"start": "21:00", "end": "22:00"}]},
    "XAGUSD": {"exchange": "COMEX", "session": "23/5", "breaks": [{"start": "21:00", "end": "22:00"}]},
    "USOIL": {"exchange": "thisMEX", "session": "23/5", "breaks": [{"start": "21:00", "end": "22:00"}]},

    # Equity Indices
    "SPX500": {"exchange": "CME", "session": "23/5", "breaks": [{"start": "21:00", "end": "22:00"}]},
    "NAS100": {"exchange": "CME", "session": "23/5", "breaks": [{"start": "21:00", "end": "22:00"}]},
    "US30": {"exchange": "CME", "session": "23/5", "breaks": [{"start": "21:00", "end": "22:00"}]},
    "DE40": {"exchange": "EUREX", "session": "23/5", "breaks": [{"start": "21:00", "end": "22:00"}]},
    "UK100": {"exchange": "ICE", "session": "23/5", "breaks": [{"start": "21:00", "end": "22:00"}]},
    "FR40": {"exchange": "EURONEXT", "session": "23/5", "breaks": [{"start": "21:00", "end": "22:00"}]},
    "JP225": {"exchange": "OSE", "session": "23/5", "breaks": [{"start": "21:00", "end": "22:00"}]},
    "AU200": {"exchange": "ASX", "session": "23/5", "breaks": [{"start": "21:00", "end": "22:00"}]},

    # Crypto (24/7)
    "BTCUSD": {"exchange": "CRYPTO", "session": "24/7", "breaks": []},
    "ETHUSD": {"exchange": "CRYPTO", "session": "24/7", "breaks": []},
}

def pre_trade_weekend_guard(symbol, current_time=None):
    """
    Check if this symbol should be traded on weekends.
    Crypto symbols can trade 24/7, other symbolfixre blocked on weekends.
    
    Args:
        symbol: Symbol to check
        current_time: current time (optional, defaults to datetime.now())
    
    Returns:
        bool: True if can trade, False if blocked
    """
    if current_time is None:
        current_time = datetime.now()
    
    # Crypto symbols can trade 24/7
    if is_crypto_symbol(symbol):
        return True
    
    # Other symbolfixre blocked on weekends
    return not is_weekend()

def is_weekend() -> bool:
    """
    Check if current time is weekend
    
    Returns:
        True if it's weekend (Saturday or Sunday)
    """
    try:
        now = datetime.now(pytz.UTC)
        weekday = now.weekday()
        return weekday >= 5  # Saturday = 5, Sunday = 6
    except Exception as e:
        logging.warning(f"Weekend check error: {e}")
        # Fallback: simple weekend check
        now = datetime.now()
        return now.weekday() >= 5

def enhanced_is_market_open(symbol: str, current_time: Optional[datetime] = None) -> bool:
    """
    Enhanced version of is_market_open with better timezone handling
    
    Args:
        symbol: Symbol to check
        current_time: Current time (optional)
        
    Returns:
        True if market is open
    """
    return is_market_open(symbol, current_time)

# === ENHANCED DATA FRESHNESS MONITOR ===
class DataFreshnessMonitor:
    """Enhanced data freshness monitoring with root cause analysifixnd auto-recovery"""

    def __init__(self):
        self.freshness_thresholds = {
            "H1": 1440,  # 24 hours (more lenient for H1)
            "H4": 2880,  # 48 hours (more lenient for H4)
            "D1": 7200,  # 5 days (more lenient for D1)
            "W1": 20160  # 2 weeks (more lenient for W1)
        }
        self.freshness_history = {}
        self.recovery_attempts = {}
        self.root_cause_stats = {
            "API_TIMEOUT": 0,
            "NETWORK_ISSUE": 0,
            "MARKET_CLOSED": 0,
            "DATA_STALE": 0
        }

    def check_data_freshness_detailed(self, symbol, timeframe, data):
        """Detailed freshness check with root cause analysis"""
        current_time = datetime.now()
        last_update = data.index[-1] if not data.empty else None

        if last_update:
            delay_minutes = (current_time - last_update).total_seconds() / 60
            threshold = self.freshness_thresholds.get(timeframe, 3600)

            # Root cause analysis
            root_causes = []
            if delay_minutes > threshold * 2:
                root_causes.append("API_TIMEOUT")
                self.root_cause_stats["API_TIMEOUT"] += 1
            elif delay_minutes > threshold * 1.5:
                root_causes.append("NETWORK_ISSUE")
                self.root_cause_stats["NETWORK_ISSUE"] += 1
            elif delay_minutes > threshold:
                root_causes.append("MARKET_CLOSED")
                self.root_cause_stats["MARKET_CLOSED"] += 1
            else:
                root_causes.append("DATA_FRESH")

            # Auto-recovery mechanism
            recovery_attempted = False
            if delay_minutes > threshold:
                recovery_attempted = self.attempt_data_recovery(symbol, timeframe)

            # Log detailed information
            logging.info(f"📊 [Data Freshness] {symbol} {timeframe}: "
                        f"Delay={delay_minutes:.1f}min, Threshold={threshold}min, "
                        f"Root Cause={root_causes[0]}, Recovery={'Attempted' if recovery_attempted else 'Not Needed'}")

            return {
                "is_fresh": delay_minutes <= threshold,
                "delay_minutes": delay_minutes,
                "threshold": threshold,
                "root_causes": root_causes,
                "last_update": last_update,
                "recovery_attempted": recovery_attempted,
                "freshness_score": max(0, 1 - (delay_minutes / threshold))
            }

        return {
            "is_fresh": False,
            "delay_minutes": float('inf'),
            "threshold": self.freshness_thresholds.get(timeframe, 3600),
            "root_causes": ["NO_DATA"],
            "last_update": None,
            "recovery_attempted": False,
            "freshness_score": 0
        }

    def attempt_data_recovery(self, symbol, timeframe):
        """Auto-recovery mechanism with exponential backoff"""
        if symbol not in self.recovery_attempts:
            self.recovery_attempts[symbol] = {}

        if timeframe not in self.recovery_attempts[symbol]:
            self.recovery_attempts[symbol][timeframe] = 0

        max_attempts = 3
        if self.recovery_attempts[symbol][timeframe] < max_attempts:
            self.recovery_attempts[symbol][timeframe] += 1

            # Implement re-fetch logic with exponential backoff
            wait_time = 2 ** self.recovery_attempts[symbol][timeframe]  # 2, 4, 8 seconds
            logging.info(f"🔄 [Data Recovery] Attempting recovery for {symbol} {timeframe} "
                        f"(attempt {self.recovery_attempts[symbol][timeframe]}/{max_attempts}, "
                        f"wait {wait_time}s)")

            # Here you would implement actual re-fetch logic
            # For now, just log the attempt
            return True

        logging.warning(f"⚠️ [Data Recovery] Max recovery attempts reached for {symbol} {timeframe}")
        return False

    def get_freshness_report(self):
        """Generate comprehensive freshness report"""
        return {
            "timestamp": datetime.now(),
            "root_cause_stats": self.root_cause_stats,
            "recovery_attempts": self.recovery_attempts,
            "freshness_history": self.freshness_history
        }

def is_market_open(symbol, current_time=None):
    """
    Check if the market for a specific symbol is currently open (UTC-aware version).
    Special rule: Allow crypto symbols trading 24/7.

    Args:
        symbol (str): Trading symbol to check
        current_time (datetime, optional): Current time to check (defaults to None)

    Returns:
        bool: True if market is open, False otherwise
    """
    try:
        # Crypto symbols trade 24/7
        if is_crypto_symbol(symbol):
            return True
        
        # For other symbols, check if it's weekend
        return not is_weekend()
    except Exception as e:
        logging.warning(f"Market open check error for {symbol}: {e}")
        # Fallback: assume market is open for crypto, closed for others on weekend
        if is_crypto_symbol(symbol):
            return True
        return not is_weekend()

def is_crypto_symbol(symbol: str) -> bool:
    """
    Check if a symbol represents cryptocurrency
    
    Args:
        symbol: Trading symbol to check
        
    Returns:
        True if symbol is cryptocurrency
    """
    return get_symbol_type(symbol) == SymbolType.CRYPTO


async def wait_for_next_primary_candle_async(primary_tf, symbol="DUMMY"):
    """
    Wait until the next primary timeframe candle starts (async version).
    Skip if it'fix crypto symbol.

    Args:
        primary_tf (str): Primary timeframe (e.g., "H1", "H4", "D1")
        symbol (str): Trading symbol, defaults to "DUMMY"
    """
    # Input validation: ensure symbol ifix valid string
    if symbol is None or not isinstance(symbol, str):
        symbol = "DUMMY"

    if is_crypto_symbol(symbol):
        logging.info(f"[Async] Crypto {symbol} doesn't need handle synchronization.")
        return

    try:
        # Simple wait logic for non-crypto symbols
        now = datetime.now(pytz.UTC)
        
        # Calculate minutes until next candle
        if primary_tf == "H1":
            minutes_to_next = 60 - now.minute
        elif primary_tf == "H4":
            minutes_to_next = 240 - (now.hour % 4) * 60 - now.minute
        elif primary_tf == "D1":
            minutes_to_next = 1440 - now.hour * 60 - now.minute
        else:
            minutes_to_next = 60  # Default to H1
        
        if minutes_to_next > 0:
            wait_seconds = minutes_to_next * 60
            logging.info(f"[Async] Waiting {wait_seconds} seconds for next {primary_tf} candle...")
        await asyncio.sleep(wait_seconds)
    except Exception as e:
        logging.warning(f"[Async] Candle wait error: {e}")
        # Fallback: wait 1 minute
        await asyncio.sleep(60)


def wait_for_next_primary_candle(primary_tf, symbol="DUMMY"):
    """
    Wait until the next primary timeframe candle starts (sync version - deprecated).
    Skip if it'fix crypto symbol.

    Args:
        primary_tf (str): Primary timeframe (e.g., "H1", "H4", "D1")
        symbol (str): Trading symbol, defaults to "DUMMY"
    """
    # Input validation: ensure symbol ifix valid string
    if symbol is None or not isinstance(symbol, str):
        symbol = "DUMMY"

    if is_crypto_symbol(symbol):
        logging.info(f"[Sync] Crypto {symbol} doesn't need handle synchronization.")
        return

    try:
        # Simple wait logic for non-crypto symbols
        now = datetime.now(pytz.UTC)
        
        # Calculate minutes until next candle
        if primary_tf == "H1":
            minutes_to_next = 60 - now.minute
        elif primary_tf == "H4":
            minutes_to_next = 240 - (now.hour % 4) * 60 - now.minute
        elif primary_tf == "D1":
            minutes_to_next = 1440 - now.hour * 60 - now.minute
        else:
            minutes_to_next = 60  # Default to H1
        
        if minutes_to_next > 0:
            wait_seconds = minutes_to_next * 60
            logging.info(f"[Sync] Waiting {wait_seconds} seconds for next {primary_tf} candle...")
        time.sleep(wait_seconds)
    except Exception as e:
        logging.warning(f"[Sync] Candle wait error: {e}")
        # Fallback: wait 1 minute
        time.sleep(60)

# Note: Old wait_for_next_h4_candle() function has been removed as it's no longer needed
def process_symbol_cycle(symbol: str):
    # 0) Determine primary_tf and timeframe set to use
    primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, "H1")
    timeframes_to_use = TIMEFRAME_SET_BY_PRIMARY.get(primary_tf, ["H4", "D1", "W1"])

    # 1) Gefrom modelulti-timeframe data using data manager
    data_manager = EnhancedDataManager()
    multi_tf_data = data_manager.fetch_multi_timeframe_data(symbol, 5000, timeframes_to_use)

    # 2) Log sanity check
    logging.info(f"[Sanity] weekend={is_weekend()} sym={symbol} crypto={is_crypto_symbol(symbol)} primary_tf={primary_tf}")

    # 3) Candle synchronization: non-crypto must wait; crypto bypassed
    wait_for_next_primary_candle(primary_tf, symbol)

    # 4) Create features using data manager
    features = data_manager.create_enhanced_features(symbol)
    if features is None:
        logging.info(f"[Features] Skip {symbol}: feature build failed.")
        return

    # 5) Open trades (based on signals) - Trading logic implemented in RL strategy
    # Position opening is handled by the RL agent and Master Agent Coordinator

    # 6) Manage open positions - Position management implemented in RL strategy
    # Position management is handled by the RL agent and Master Agent Coordinator


import pytz

# Optional imports with error handling
try:
    import tradingeconomics as te
    TRADING_ECONOMICS_AVAILABLE = True  # Enable Trading Economics
    # Initialize Trading Economics API authentication
    try:
        api_key = globals().get("TRADING_ECONOMICS_API_KEY", "")
        logging.info(f"🔑 Trading Economics API key found: {'Yes' if api_key else 'No'}")
        if api_key and "YOUR_API_KEY" not in api_key:
            # Try multiple methods to ensure API key is properly set
            try:
                # Method 1: Direct login
                te.login(api_key)
                logging.info("Trading Economics API authenticated successfully")
            except Exception as login_error:
                logging.warning(f"⚠️ Primary login failed: {login_error}")
                # Method 2: Try setting glob.apikey if it exists
                try:
                    if hasattr(te, 'glob') and hasattr(te.glob, 'apikey'):
                        te.glob.apikey = api_key
                        logging.info("Trading Economics API key set via glob.apikey")
                    else:
                        logging.warning("⚠️ te.glob.apikey not available in this version")
                except Exception as glob_error:
                    logging.warning(f"⚠️ Glob method failed: {glob_error}")
        else:
            logging.warning("⚠️ Trading Economics API key not configured. Economic calendar features will be limited.")
    except Exception as auth_error:
        logging.warning(f"⚠️ Trading Economics API authentication failed: {auth_error}")
except ImportError:
    logging.warning("tradingeconomics package not available. Economic calendar features will be limited.")
    TRADING_ECONOMICS_AVAILABLE = False
    te = None

# Check for other optional packages
OPTIONAL_PACKAGES = {}

# Check for additional ML packages
try:
    import optuna
    OPTIONAL_PACKAGES['optuna'] = True
except ImportError:
    OPTIONAL_PACKAGES['optuna'] = False
    logging.warning("optuna not available. Hyperparameter optimization will be limited.")

try:
    from optuna.integration import LightGBMPruningCallback
    from optuna.pruners import MedianPruner
    from optuna.samplers import TPESampler
    OPTIONAL_PACKAGES['optuna_integrations'] = True
except ImportError:
    OPTIONAL_PACKAGES['optuna_integrations'] = False
    logging.warning("optuna integrations not available. Some optimization features disabled.")

# Check for financial data packages
try:
    import eod
    OPTIONAL_PACKAGES['eod'] = True
except ImportError:
    OPTIONAL_PACKAGES['eod'] = False
    logging.warning("eod package not available. EODHD news provider will be disabled.")

try:
    from newsapi import NewsApiClient
    OPTIONAL_PACKAGES['newsapi'] = True
except ImportError:
    OPTIONAL_PACKAGES['newsapi'] = False
    logging.warning("newsapi package not available. NewsAPI provider will be disabled.")

# Utilities
import time
import joblib
import os
import json
import logging
def validate_dataframe_freshness(
    multi_tf_data,
    primary_tf: str,
    *,
    symbol: str | None = None,
    max_stale_minutes: int | float | None = None,
    per_tf_override: dict | None = None
):
    """
    Enhanced data freshness validation with auto-recovery and flexible thresholds (UTC-aware version).
    Priority: per_tf_override > max_stale_minutes > DATA_FRESHNESS_LIMIT_MIN.
    Skip validation for **all crypto** on weekends. Returns True/False.
    """
    if not multi_tf_data or primary_tf not in multi_tf_data:
        logging.warning(f"[Data Freshness] No data available for {primary_tf}")
        return False

    # BYPASS for crypto on weekends
    sym = (symbol or "").upper()
    if sym and is_crypto_symbol(sym) and is_weekend():
        logging.info(f"[Data Freshness] Skipping check for {sym} (crypto) on weekends.")
        return True

    # Get freshness limits
    tf_limit = DATA_FRESHNESS_LIMIT_MIN.get(primary_tf, 1440)  # Default 24 hours
    
    if per_tf_override and primary_tf in per_tf_override:
        tf_limit = per_tf_override[primary_tf]
    elif max_stale_minutes is not None:
        tf_limit = max_stale_minutes

    # Simple freshness check
    df = multi_tf_data[primary_tf]
    if df is None or df.empty:
        logging.warning(f"[Data Freshness] No data for {primary_tf}")
        return False
    
    try:
        # Check if data is fresh enough
        last_timestamp = df.index[-1]
        now = datetime.now(pytz.UTC)
        time_diff = (now - last_timestamp).total_seconds() / 60  # minutes
        
        is_fresh = time_diff <= tf_limit
        if not is_fresh:
            logging.warning(f"[Data Freshness] {primary_tf} data is stale: {time_diff:.1f} minutes old (limit: {tf_limit})")
            # Log additional context for debugging
            logging.info(f"[Data Freshness] Last timestamp: {last_timestamp}, Current time: {now}")
        
        return is_fresh
    except Exception as e:
        logging.error(f"[Data Freshness] Error checking freshness: {e}")
        return False


def _attempt_data_recovery(symbol: str, timeframe: str, stale_minutes: float) -> bool:
    """Attempt to recover stale data using retry mechanism"""
    try:
        from io_clients import DataRecoveryClient
        
        logging.info(f"[Data Recovery] Attempting recovery for {symbol} {timeframe} (stale {stale_minutes:.0f} minutes)")

        # Use retry mechanism for data recovery
        recovery_client = DataRecoveryClient(use_async=False)
        success = recovery_client.attempt_data_recovery_sync(symbol, timeframe)
        
        if success:
            logging.info(f"[Data Recovery] Recovery successful for {symbol} {timeframe}")
        else:
            logging.warning(f"[Data Recovery] Recovery failed for {symbol} {timeframe}")
        
        return success
        
    except Exception as e:
        logging.error(f"[Data Recovery] Recovery error for {symbol} {timeframe}: {e}")
        return False


# ==============================================================================
# REFACTORED UTILITY FUNCTIONS
# ==============================================================================

def get_symbol_type(symbol: str) -> SymbolType:
    """
    Get the type of a trading symbol
    
    Args:
        symbol: Trading symbol to classify
        
    Returns:
        SymbolType enum value
    """
    symbol_upper = symbol.upper()
    
    if symbol_upper in CRYPTO_SYMBOLS:
        return SymbolType.CRYPTO
    elif symbol_upper in EQUITY_INDICES:
        return SymbolType.EQUITY
    elif symbol_upper in FOREX_PAIRS:
        return SymbolType.FOREX
    elif symbol_upper in COMMODITIES:
        return SymbolType.COMMODITY
    else:
        return SymbolType.UNKNOWN

def is_crypto_symbol(symbol: str) -> bool:
    """
    Check if a symbol represents cryptocurrency
    
    Args:
        symbol: Trading symbol to check
        
    Returns:
        True if symbol is cryptocurrency
    """
    return get_symbol_type(symbol) == SymbolType.CRYPTO

def is_equity_index_symbol(symbol: str) -> bool:
    """
    Check if symbol is an equity index
    
    Args:
        symbol: Trading symbol to check
        
    Returns:
        True if symbol is equity index
    """
    return get_symbol_type(symbol) == SymbolType.EQUITY

def is_forex_pair(symbol: str) -> bool:
    """
    Check if symbol is a forex pair
    
    Args:
        symbol: Trading symbol to check
        
    Returns:
        True if symbol is forex pair
    """
    return get_symbol_type(symbol) == SymbolType.FOREX

def is_commodity(symbol: str) -> bool:
    """
    Check if symbol is a commodity
    
    Args:
        symbol: Trading symbol to check
        
    Returns:
        True if symbol is commodity
    """
    return get_symbol_type(symbol) == SymbolType.COMMODITY

def get_primary_timeframe(symbol: str) -> str:
    """
    Get the primary timeframe for a given symbol
    
    Args:
        symbol: Trading symbol
        
    Returns:
        Primary timeframe string
    """
    symbol_type = get_symbol_type(symbol)
    
    if symbol_type == SymbolType.CRYPTO:
        return "H4"  # Crypto uses H4 as primary
    elif symbol_type == SymbolType.EQUITY:
        return "H4"  # Equity indices use H4
    elif symbol_type == SymbolType.FOREX:
        return "H4"  # Forex uses H4
    elif symbol_type == SymbolType.COMMODITY:
        return "H4"  # Commodities use H4
    else:
        return "H4"  # Default to H4

def validate_symbol(symbol: str) -> bool:
    """
    Validate if symbol is supported
    
    Args:
        symbol: Trading symbol to validate
        
    Returns:
        True if symbol is supported
    """
    return get_symbol_type(symbol) != SymbolType.UNKNOWN


# Replace old drift configuration with these lines
# === DRIFfrom modelANAGEMENT CONFIG ===
DRIFT_SAMPLE_SIZE = 8          # Number of random symbols to check in each cycle
DRIFT_WARNING_INCREMENT = 1    # Pointadded when drift is detected
DRIFT_SCORE_DECAY = 1          # Points subtracted (cooling down) after each check cycle
DRIFT_SCORE_THRESHOLD = 3      # Score threshold to trigger retraining
# ==============================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', encoding='utf-8', force=True)
OANDA_API_KEY = "814bb04d60580a8a9b0ce5542f70d5f7-b33dbed32efba816c1d16c393369ec8d"  # Replace with actual API key
OANDA_URL = "https://api-fxtrade.oanda.com/v3"
# DISCORD WEBHOOK - UPDATE THIS WITH YOUR WEBHOOK URL
# To create a new webhook:
# 1. Go to your Discord server
# 2. Right-click on the channel → Edit Channel
# 3. Go to Integrations → Webhooks
# 4. Click "Create Webhook"
# 5. Copy the webhook URL and replace the one below
DISCORD_WEBHOOK = "https://discord.com/api/webhooks/1410915486551511062/pzCWm4gbe0w-xFyI0pKbsy417sbsYwjwjg-iWMLhccIGRJR2FqJ4kUwlzIZAyw3C2Fhq"

# === DISCORD ALERT CONFIGURATION ===
DISCORD_CONFIG = {
    "RESEND_SIGNALS_ON_STARTUP": False,  # Enable resending active signals when starting bot
    "RESEND_TRAILING_STOPS": True,       # Include trailing stops
    "RESEND_POSITIONS": True,            # Resend current position information
    "RESEND_PERFORMANCE": False,          # Resend performance information
    "MAX_RESEND_MESSAGES": 10,           # Limit number of resend messages
    "RESEND_DELAY_SECONDS": 2            # Delay between messages to avoid spam
}
# Optimized symbol configuration with asset class metadata
# Symbol allocation based on forward test results - EXPANDED VERSION
SYMBOL_ALLOCATION = {
    # === COMMODITIES (Global Trading) ===
    "XAUUSD": {"weight": 0.08, "max_exposure": 0.05, "risk_multiplier": 0.8},  # Gold
    "USOIL": {"weight": 0.06, "max_exposure": 0.04, "risk_multiplier": 0.9},   # Crude Oil

    # === EQUITY INDICES (Session-based Trading) ===
    "SPX500": {"weight": 0.08, "max_exposure": 0.05, "risk_multiplier": 0.8},  # S&P 500
    "DE40": {"weight": 0.05, "max_exposure": 0.03, "risk_multiplier": 0.8},     # DAX 40

    # === FOREX MAJOR PAIRS ===
    "EURUSD": {"weight": 0.06, "max_exposure": 0.04, "risk_multiplier": 0.75},
    "AUDUSD": {"weight": 0.05, "max_exposure": 0.03, "risk_multiplier": 0.75},  # Australian Dollar
    "AUDNZD": {"weight": 0.04, "max_exposure": 0.025, "risk_multiplier": 0.8},  # AUD/NZD cross pair

    # === CRYPTOoldRRENCIES (24/7 Trading) ===
    "BTCUSD": {"weight": 0.05, "max_exposure": 0.03, "risk_multiplier": 1.0},  # Bitcoin
    "ETHUSD": {"weight": 0.04, "max_exposure": 0.025, "risk_multiplier": 1.0},  # Ethereum
}

# Active symbols list (all symbols in SYMBOL_ALLOCATION are considered active)
# Ensure consistent order for RL model to work correctly
SYMBOLS = ["BTCUSD", "ETHUSD", "XAUUSD", "USOIL", "SPX500", "DE40", "EURUSD", "AUDUSD", "AUDNZD"]

# Enhanced Entry/TP/SL Configuration for all symbols - EXPANDED VERSION
ENTRY_TP_SL_CONFIG = {
    # === COMMODITIES (Global Trading) ===
    "XAUUSD": {
        "entry_method": "fibonacci_confluence",
        "atr_multiplier_sl": 2.5,
        "atr_multiplier_tp": 4.0,
        "min_rr_ratio": 1.6,
        "max_rr_ratio": 3.5,
        "support_resistance_weight": 0.4,
        "volume_confirmation": False,
        "session_filter": False,
        "volatility_adjustment": True,
        "fibonacci_levels": [0.236, 0.382, 0.5, 0.618, 0.786],
        # Master Agent Trailing Stop Configuration
        "min_trailing_profit": 0.012,  # 1.2% minimum profit to activate trailing
        "trailing_atr_multiplier": 1.5,  # ATR multiplier for trailing distance
        "trailing_volatility_threshold": 0.6,  # Volatility threshold for activation
        "trailing_trend_strength_min": 0.7  # Minimum trend strength for trailing
    },
    "XAGUSD": {
        "entry_method": "fibonacci_confluence",
        "atr_multiplier_sl": 2.2,
        "atr_multiplier_tp": 3.5,
        "min_rr_ratio": 1.5,
        "max_rr_ratio": 3.2,
        "support_resistance_weight": 0.35,
        "volume_confirmation": False,
        "session_filter": False,
        "volatility_adjustment": True,
        "fibonacci_levels": [0.236, 0.382, 0.5, 0.618, 0.786],
        # Master Agent Trailing Stop Configuration
        "min_trailing_profit": 0.015,  # 1.5% minimum profit to activate trailing
        "trailing_atr_multiplier": 1.8,  # ATR multiplier for trailing distance
        "trailing_volatility_threshold": 0.5,  # Volatility threshold for activation
        "trailing_trend_strength_min": 0.6  # Minimum trend strength for trailing
    },
    "USOIL": {
        "entry_method": "trend_following",
        "atr_multiplier_sl": 2.0,
        "atr_multiplier_tp": 3.5,
        "min_rr_ratio": 1.5,
        "max_rr_ratio": 3.2,
        "support_resistance_weight": 0.3,
        "volume_confirmation": True,
        "session_filter": False,
        "volatility_adjustment": True,
        # Master Agent Trailing Stop Configuration
        "min_trailing_profit": 0.018,  # 1.8% minimum profit to activate trailing
        "trailing_atr_multiplier": 2.0,  # ATR multiplier for trailing distance
        "trailing_volatility_threshold": 0.7,  # Volatility threshold for activation
        "trailing_trend_strength_min": 0.8  # Minimum trend strength for trailing
    },

    # === EQUITY INDICES (Session-based Trading) ===
    "SPX500": {
        "entry_method": "breakout_confirmation",
        "atr_multiplier_sl": 2.0,
        "atr_multiplier_tp": 3.0,
        "min_rr_ratio": 1.5,
        "max_rr_ratio": 3.0,
        "support_resistance_weight": 0.3,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "DE40": {
        "entry_method": "european_session_breakout",
        "atr_multiplier_sl": 1.8,
        "atr_multiplier_tp": 2.8,
        "min_rr_ratio": 1.4,
        "max_rr_ratio": 2.8,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "NAS100": {
        "entry_method": "momentum_trending",
        "atr_multiplier_sl": 2.2,
        "atr_multiplier_tp": 3.5,
        "min_rr_ratio": 1.6,
        "max_rr_ratio": 3.2,
        "support_resistance_weight": 0.32,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "US30": {
        "entry_method": "trend_following",
        "atr_multiplier_sl": 1.8,
        "atr_multiplier_tp": 2.8,
        "min_rr_ratio": 1.4,
        "max_rr_ratio": 2.8,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "DE40": {
        "entry_method": "trend_following",
        "atr_multiplier_sl": 1.8,
        "atr_multiplier_tp": 2.7,
        "min_rr_ratio": 1.4,
        "max_rr_ratio": 2.8,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "UK100": {
        "entry_method": "breakout_confirmation",
        "atr_multiplier_sl": 1.6,
        "atr_multiplier_tp": 2.5,
        "min_rr_ratio": 1.3,
        "max_rr_ratio": 2.6,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "FR40": {
        "entry_method": "trend_following",
        "atr_multiplier_sl": 1.7,
        "atr_multiplier_tp": 2.6,
        "min_rr_ratio": 1.3,
        "max_rr_ratio": 2.7,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "JP225": {
        "entry_method": "asian_session_breakout",
        "atr_multiplier_sl": 1.6,
        "atr_multiplier_tp": 2.8,
        "min_rr_ratio": 1.4,
        "max_rr_ratio": 2.6,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "AU200": {
        "entry_method": "asian_session_breakout",
        "atr_multiplier_sl": 1.5,
        "atr_multiplier_tp": 2.4,
        "min_rr_ratio": 1.3,
        "max_rr_ratio": 2.5,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },

    # === FOREX MAJOR PAIRS ===
    "EURUSD": {
        "entry_method": "trend_following",
        "atr_multiplier_sl": 1.8,
        "atr_multiplier_tp": 2.7,
        "min_rr_ratio": 1.4,
        "max_rr_ratio": 2.8,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True,
        # Master Agent Trailing Stop Configuration
        "min_trailing_profit": 0.008,  # 0.8% minimum profit to activate trailing
        "trailing_atr_multiplier": 1.2,  # ATR multiplier for trailing distance
        "trailing_volatility_threshold": 0.4,  # Volatility threshold for activation
        "trailing_trend_strength_min": 0.6  # Minimum trend strength for trailing
    },
    "AUDUSD": {
        "entry_method": "asian_session_breakout",
        "atr_multiplier_sl": 1.6,
        "atr_multiplier_tp": 2.6,
        "min_rr_ratio": 1.4,
        "max_rr_ratio": 2.6,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "GBPUSD": {
        "entry_method": "breakout_confirmation",
        "atr_multiplier_sl": 2.0,
        "atr_multiplier_tp": 3.0,
        "min_rr_ratio": 1.5,
        "max_rr_ratio": 3.0,
        "support_resistance_weight": 0.3,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "USDJPY": {
        "entry_method": "asian_session_breakout",
        "atr_multiplier_sl": 1.6,
        "atr_multiplier_tp": 2.8,
        "min_rr_ratio": 1.4,
        "max_rr_ratio": 2.6,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "USDCHF": {
        "entry_method": "range_trading",
        "atr_multiplier_sl": 1.4,
        "atr_multiplier_tp": 2.2,
        "min_rr_ratio": 1.2,
        "max_rr_ratio": 2.3,
        "support_resistance_weight": 0.3,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": False
    },
    "AUDUSD": {
        "entry_method": "carry_trade_momentum",
        "atr_multiplier_sl": 1.5,
        "atr_multiplier_tp": 2.5,
        "min_rr_ratio": 1.3,
        "max_rr_ratio": 2.5,
        "support_resistance_weight": 0.2,
        "volume_confirmation": False,
        "session_filter": True,
        "volatility_adjustment": False
    },
    "USDCAD": {
        "entry_method": "trend_following",
        "atr_multiplier_sl": 1.7,
        "atr_multiplier_tp": 2.6,
        "min_rr_ratio": 1.4,
        "max_rr_ratio": 2.7,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "NZDUSD": {
        "entry_method": "carry_trade_momentum",
        "atr_multiplier_sl": 1.4,
        "atr_multiplier_tp": 2.3,
        "min_rr_ratio": 1.3,
        "max_rr_ratio": 2.4,
        "support_resistance_weight": 0.2,
        "volume_confirmation": False,
        "session_filter": True,
        "volatility_adjustment": False
    },

    # === FOREX MINOR PAIRS ===
    "EURGBP": {
        "entry_method": "range_trading",
        "atr_multiplier_sl": 1.3,
        "atr_multiplier_tp": 2.2,
        "min_rr_ratio": 1.2,
        "max_rr_ratio": 2.3,
        "support_resistance_weight": 0.3,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": False
    },
    "EURJPY": {
        "entry_method": "asian_session_breakout",
        "atr_multiplier_sl": 1.4,
        "atr_multiplier_tp": 2.3,
        "min_rr_ratio": 1.3,
        "max_rr_ratio": 2.4,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "GBPJPY": {
        "entry_method": "volatility_breakout",
        "atr_multiplier_sl": 1.6,
        "atr_multiplier_tp": 2.5,
        "min_rr_ratio": 1.4,
        "max_rr_ratio": 2.6,
        "support_resistance_weight": 0.3,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "AUDJPY": {
        "entry_method": "carry_trade_momentum",
        "atr_multiplier_sl": 1.3,
        "atr_multiplier_tp": 2.2,
        "min_rr_ratio": 1.2,
        "max_rr_ratio": 2.3,
        "support_resistance_weight": 0.2,
        "volume_confirmation": False,
        "session_filter": True,
        "volatility_adjustment": False
    },
    "CADJPY": {
        "entry_method": "asian_session_breakout",
        "atr_multiplier_sl": 1.2,
        "atr_multiplier_tp": 2.1,
        "min_rr_ratio": 1.1,
        "max_rr_ratio": 2.2,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "AUDNZD": {
        "entry_method": "carry_trade_momentum",
        "atr_multiplier_sl": 1.2,
        "atr_multiplier_tp": 2.0,
        "min_rr_ratio": 1.2,
        "max_rr_ratio": 2.2,
        "support_resistance_weight": 0.2,
        "volume_confirmation": False,
        "session_filter": True,
        "volatility_adjustment": False
    },
    "CHFJPY": {
        "entry_method": "asian_session_breakout",
        "atr_multiplier_sl": 1.5,
        "atr_multiplier_tp": 2.4,
        "min_rr_ratio": 1.3,
        "max_rr_ratio": 2.5,
        "support_resistance_weight": 0.25,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True
    },
    "EURCHF": {
        "entry_method": "range_trading",
        "atr_multiplier_sl": 1.2,
        "atr_multiplier_tp": 1.9,
        "min_rr_ratio": 1.1,
        "max_rr_ratio": 2.0,
        "support_resistance_weight": 0.3,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": False
    },
    "GBPCHF": {
        "entry_method": "range_trading",
        "atr_multiplier_sl": 1.3,
        "atr_multiplier_tp": 2.0,
        "min_rr_ratio": 1.2,
        "max_rr_ratio": 2.1,
        "support_resistance_weight": 0.3,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": False
    },

    # === FOREX EXOTIC PAIRS ===

    # === CRYPTOoldRRENCIES (24/7 Trading) ===
    "BTCUSD": {
        "entry_method": "volatility_breakout",
        "atr_multiplier_sl": 3.0,
        "atr_multiplier_tp": 6.0,
        "min_rr_ratio": 1.6,
        "max_rr_ratio": 3.5,
        "support_resistance_weight": 0.25,
        "volume_confirmation": False,
        "session_filter": False,
        "volatility_adjustment": True,
        # Master Agent Trailing Stop Configuration
        "min_trailing_profit": 0.016,  # 1.6% minimum profit to activate trailing
        "trailing_atr_multiplier": 2.5,  # ATR multiplier for trailing distance
        "trailing_volatility_threshold": 0.8,  # High volatility threshold for crypto
        "trailing_trend_strength_min": 0.7  # Minimum trend strength for trailing
    },
    "ETHUSD": {
        "entry_method": "volatility_breakout",
        "atr_multiplier_sl": 3.2,
        "atr_multiplier_tp": 6.4,
        "min_rr_ratio": 1.6,
        "max_rr_ratio": 3.5,
        "support_resistance_weight": 0.25,
        "volume_confirmation": False,
        "session_filter": False,
        "volatility_adjustment": True,
        # Master Agent Trailing Stop Configuration
        "min_trailing_profit": 0.018,  # 1.8% minimum profit to activate trailing
        "trailing_atr_multiplier": 2.8,  # ATR multiplier for trailing distance
        "trailing_volatility_threshold": 0.8,  # High volatility threshold for crypto
        "trailing_trend_strength_min": 0.7  # Minimum trend strength for trailing
    }
}

# Enhanced symbol metadata for optimization
SYMBOL_METADATA = {
    "XAUUSD": {
        "asset_class": "commodity",
        "region": "global",
        "session": "24h",
        "volatility_profile": "high",
        "correlation_group": "precious_metals",
        "pip_value": 0.1,
        "min_lot_size": 0.01,
        "spread_typical": 2.0,
        "trading_hours": {"start": 0, "end": 23, "timezone": "UTC"},
        "high_impact_events": ["Fed_Meetings", "Inflation", "Geopolitical", "USD_Strength"],
        "news_sources": ["financial_news", "economic_calendar", "geopolitical"],
        "technical_folds": ["trend_following", "support_resistance", "fibonacci"]
    },
     "EURUSD": {
         "asset_class": "forex",
        "region": "Europe",
         "session": "European",
         "volatility_profile": "medium",
         "correlation_group": "major_pairs",
         "pip_value": 0.0001,
         "min_lot_size": 0.01,
         "spread_typical": 0.8,
         "trading_hours": {"start": 8, "end": 17, "timezone": "Europe/London"},
         "high_impact_events": ["ECB", "NFP", "CPI", "GDP", "PMI"],
         "news_sources": ["financial_news", "economic_calendar"],
         "technical_folds": ["trend_following", "support_resistance", "momentum"]
     },
     "GBPUSD": {
         "asset_class": "forex",
         "region": "Europe",
         "session": "European",
         "volatility_profile": "high",
         "correlation_group": "major_pairs",
         "pip_value": 0.0001,
         "min_lot_size": 0.01,
         "spread_typical": 1.2,
         "trading_hours": {"start": 8, "end": 17, "timezone": "Europe/London"},
         "high_impact_events": ["BOE", "NFP", "CPI", "GDP", "Brexit"],
         "news_sources": ["financial_news", "economic_calendar"],
         "technical_folds": ["trend_following", "breakout", "support_resistance"]
     },
    "USDJPY": {
        "asset_class": "forex",
        "region": "Asia",
        "session": "Asian",
        "volatility_profile": "medium",
        "correlation_group": "major_pairs",
        "pip_value": 0.01,
        "min_lot_size": 0.01,
        "spread_typical": 0.9,
        "trading_hours": {"start": 0, "end": 8, "timezone": "Asia/Tokyo"},
        "high_impact_events": ["BOJ", "NFP", "CPI", "GDP", "USD_Strength"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["trend_following", "mean_reversion", "session_trading"]
    },
    "AUDUSD": {
        "asset_class": "forex",
        "region": "Pacific",
        "session": "Asian",
        "volatility_profile": "medium",
        "correlation_group": "commodity_currencies",
        "pip_value": 0.0001,
        "min_lot_size": 0.01,
        "spread_typical": 1.0,
        "trading_hours": {"start": 21, "end": 7, "timezone": "UTC"},
        "high_impact_events": ["RBA", "NFP", "CPI", "Commodity_Prices", "Risk_Sentiment"],
        "news_sources": ["financial_news", "economic_calendar", "commodity_news"],
        "technical_folds": ["carry_trade", "trend_following", "correlation_analysis"]
    },
    "USDCAD": {
        "asset_class": "forex",
        "region": "North_America",
        "session": "US",
        "volatility_profile": "medium",
        "correlation_group": "commodity_currencies",
        "pip_value": 0.0001,
        "min_lot_size": 0.01,
        "spread_typical": 1.1,
        "trading_hours": {"start": 13, "end": 22, "timezone": "US/Eastern"},
        "high_impact_events": ["BOC", "NFP", "CPI", "Oil_Prices", "USD_Strength"],
        "news_sources": ["financial_news", "economic_calendar", "commodity_news"],
        "technical_folds": ["trend_following", "correlation_analysis", "support_resistance"]
    },
    "NZDUSD": {
        "asset_class": "forex",
        "region": "Pacific",
        "session": "Asian",
        "volatility_profile": "medium",
        "correlation_group": "commodity_currencies",
        "pip_value": 0.0001,
        "min_lot_size": 0.01,
        "spread_typical": 1.3,
        "trading_hours": {"start": 21, "end": 7, "timezone": "UTC"},
        "high_impact_events": ["RBNZ", "NFP", "CPI", "Commodity_Prices", "Risk_Sentiment"],
        "news_sources": ["financial_news", "economic_calendar", "commodity_news"],
        "technical_folds": ["carry_trade", "trend_following", "correlation_analysis"]
    },
    "CHFJPY": {
        "asset_class": "forex",
        "region": "Europe",
        "session": "European",
        "volatility_profile": "low",
        "correlation_group": "cross_pairs",
        "pip_value": 0.01,
        "min_lot_size": 0.01,
        "spread_typical": 1.5,
        "trading_hours": {"start": 8, "end": 17, "timezone": "Europe/Zurich"},
        "high_impact_events": ["SNB", "BOJ", "CPI", "GDP", "Safe_Haven_Demand"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["range_trading", "mean_reversion", "safe_haven_correlation"]
    },
    "EURGBP": {
        "asset_class": "forex",
        "region": "Europe",
        "session": "European",
        "volatility_profile": "medium",
        "correlation_group": "cross_pairs",
        "pip_value": 0.0001,
        "min_lot_size": 0.01,
        "spread_typical": 1.0,
        "trading_hours": {"start": 8, "end": 17, "timezone": "Europe/London"},
        "high_impact_events": ["ECB", "BOE", "CPI", "GDP", "Brexit"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["range_trading", "mean_reversion", "correlation_analysis"]
    },
    "EURJPY": {
        "asset_class": "forex",
        "region": "Europe",
        "session": "European",
        "volatility_profile": "medium",
        "correlation_group": "cross_pairs",
        "pip_value": 0.01,
        "min_lot_size": 0.01,
        "spread_typical": 1.2,
        "trading_hours": {"start": 8, "end": 17, "timezone": "Europe/London"},
        "high_impact_events": ["ECB", "BOJ", "CPI", "GDP", "Risk_Sentiment"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["trend_following", "carry_trade", "risk_sentiment"]
    },
    "GBPJPY": {
        "asset_class": "forex",
        "region": "Europe",
        "session": "European",
        "volatility_profile": "high",
        "correlation_group": "cross_pairs",
        "pip_value": 0.01,
        "min_lot_size": 0.01,
        "spread_typical": 1.8,
        "trading_hours": {"start": 8, "end": 17, "timezone": "Europe/London"},
        "high_impact_events": ["BOE", "BOJ", "CPI", "GDP", "Risk_Sentiment"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["volatility_breakout", "carry_trade", "risk_sentiment"]
    },
    "AUDJPY": {
        "asset_class": "forex",
        "region": "Pacific",
        "session": "Asian",
        "volatility_profile": "medium",
        "correlation_group": "commodity_currencies",
        "pip_value": 0.01,
        "min_lot_size": 0.01,
        "spread_typical": 1.4,
        "trading_hours": {"start": 21, "end": 7, "timezone": "UTC"},
        "high_impact_events": ["RBA", "BOJ", "CPI", "Commodity_Prices", "Risk_Sentiment"],
        "news_sources": ["financial_news", "economic_calendar", "commodity_news"],
        "technical_folds": ["carry_trade", "trend_following", "correlation_analysis"]
    },
    "CADJPY": {
        "asset_class": "forex",
        "region": "North_America",
        "session": "US",
        "volatility_profile": "medium",
        "correlation_group": "commodity_currencies",
        "pip_value": 0.01,
        "min_lot_size": 0.01,
        "spread_typical": 1.6,
        "trading_hours": {"start": 13, "end": 22, "timezone": "US/Eastern"},
        "high_impact_events": ["BOC", "BOJ", "CPI", "Oil_Prices", "Risk_Sentiment"],
        "news_sources": ["financial_news", "economic_calendar", "commodity_news"],
        "technical_folds": ["carry_trade", "correlation_analysis", "oil_correlation"]
    },

     #=== CRYPTOoldRRENCIES (24/7 Trading) ===
     "BTCUSD": {
         "asset_class": "cryptocurrency",
         "region": "global",
         "session": "24/7",
         "volatility_profile": "very_high",
         "correlation_group": "crypto",
         "pip_value": 0.1,
         "min_lot_size": 0.001,
         "spread_typical": 20.0,
         "trading_hours": {"start": 0, "end": 23, "timezone": "UTC"},
         "high_impact_events": ["Crypto_Specific", "Liquidity_Shocks"],
         "news_sources": ["crypto_news", "onchain"],
         "technical_folds": ["volatility_breakout", "trend_following"]
     },
     "ETHUSD": {
         "asset_class": "cryptocurrency",
         "region": "global",
         "session": "24/7",
         "volatility_profile": "very_high",
         "correlation_group": "crypto",
         "pip_value": 0.01,
         "min_lot_size": 0.01,
         "spread_typical": 15.0,
         "trading_hours": {"start": 0, "end": 23, "timezone": "UTC"},
         "high_impact_events": ["Crypto_Specific", "Liquidity_Shocks"],
         "news_sources": ["crypto_news", "onchain"],
         "technical_folds": ["volatility_breakout", "trend_following"]
     },

    # === EQUITY INDICES ===
    "SPX500": {
        "asset_class": "equity_index",
        "region": "US",
        "session": "US",
        "volatility_profile": "medium",
        "correlation_group": "us_indices",
        "pip_value": 0.1,
        "min_lot_size": 0.1,
        "spread_typical": 0.5,
        "trading_hours": {"start": 14, "end": 21, "timezone": "UTC"},
        "high_impact_events": ["Fed_Meetings", "NFP", "CPI", "GDP", "Earnings"],
        "news_sources": ["financial_news", "economic_calendar", "earnings"],
        "technical_folds": ["trend_following", "support_resistance", "momentum"]
    },
    "NAS100": {
        "asset_class": "equity_index",
        "region": "US",
        "session": "US",
        "volatility_profile": "high",
        "correlation_group": "us_indices",
        "pip_value": 0.1,
        "min_lot_size": 0.1,
        "spread_typical": 0.8,
        "trading_hours": {"start": 14, "end": 21, "timezone": "UTC"},
        "high_impact_events": ["Fed_Meetings", "Tech_Earnings", "CPI", "GDP"],
        "news_sources": ["financial_news", "tech_news", "earnings"],
        "technical_folds": ["trend_following", "breakout", "momentum"]
    },
    "US30": {
        "asset_class": "equity_index",
        "region": "US",
        "session": "US",
        "volatility_profile": "medium",
        "correlation_group": "us_indices",
        "pip_value": 0.1,
        "min_lot_size": 0.1,
        "spread_typical": 0.6,
        "trading_hours": {"start": 14, "end": 21, "timezone": "UTC"},
        "high_impact_events": ["Fed_Meetings", "NFP", "CPI", "GDP", "Earnings"],
        "news_sources": ["financial_news", "economic_calendar", "earnings"],
        "technical_folds": ["trend_following", "support_resistance", "momentum"]
    },
    "DE40": {
        "asset_class": "equity_index",
        "region": "Europe",
        "session": "European",
        "volatility_profile": "medium",
        "correlation_group": "european_indices",
        "pip_value": 0.1,
        "min_lot_size": 0.1,
        "spread_typical": 0.7,
        "trading_hours": {"start": 8, "end": 16, "timezone": "Europe/Berlin"},
        "high_impact_events": ["ECB", "German_Economic_Data", "EU_Summit"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["trend_following", "support_resistance", "momentum"]
    },
    "UK100": {
        "asset_class": "equity_index",
        "region": "Europe",
        "session": "European",
        "volatility_profile": "medium",
        "correlation_group": "european_indices",
        "pip_value": 0.1,
        "min_lot_size": 0.1,
        "spread_typical": 0.8,
        "trading_hours": {"start": 8, "end": 16, "timezone": "Europe/London"},
        "high_impact_events": ["BOE", "UK_Economic_Data", "Brexit"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["trend_following", "support_resistance", "momentum"]
    },
    "FR40": {
        "asset_class": "equity_index",
        "region": "Europe",
        "session": "European",
        "volatility_profile": "medium",
        "correlation_group": "european_indices",
        "pip_value": 0.1,
        "min_lot_size": 0.1,
        "spread_typical": 0.9,
        "trading_hours": {"start": 8, "end": 16, "timezone": "Europe/Paris"},
        "high_impact_events": ["ECB", "French_Economic_Data", "EU_Summit"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["trend_following", "support_resistance", "momentum"]
    },
    "JP225": {
        "asset_class": "equity_index",
        "region": "Asia",
        "session": "Asian",
        "volatility_profile": "medium",
        "correlation_group": "asian_indices",
        "pip_value": 0.1,
        "min_lot_size": 0.1,
        "spread_typical": 1.0,
        "trading_hours": {"start": 0, "end": 6, "timezone": "Asia/Tokyo"},
        "high_impact_events": ["BOJ", "Japanese_Economic_Data", "Earnings"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["trend_following", "support_resistance", "momentum"]
    },
    "AU200": {
        "asset_class": "equity_index",
        "region": "Asia",
        "session": "Asian",
        "volatility_profile": "medium",
        "correlation_group": "asian_indices",
        "pip_value": 0.1,
        "min_lot_size": 0.1,
        "spread_typical": 1.2,
        "trading_hours": {"start": 22, "end": 6, "timezone": "Australia/Sydney"},
        "high_impact_events": ["RBA", "Australian_Economic_Data", "Commodity_Prices"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["trend_following", "support_resistance", "momentum"]
    },

    # === ADDITIONAL FOREX PAIRS ===
    "USDCHF": {
        "asset_class": "forex",
        "region": "Europe",
        "session": "European",
        "volatility_profile": "low",
        "correlation_group": "safe_haven_pairs",
        "pip_value": 0.0001,
        "min_lot_size": 0.01,
        "spread_typical": 1.0,
        "trading_hours": {"start": 8, "end": 17, "timezone": "Europe/Zurich"},
        "high_impact_events": ["SNB", "NFP", "CPI", "GDP", "Safe_Haven_Demand"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["range_trading", "support_resistance", "momentum"]
    },
    "EURCHF": {
        "asset_class": "forex",
        "region": "Europe",
        "session": "European",
        "volatility_profile": "low",
        "correlation_group": "safe_haven_pairs",
        "pip_value": 0.0001,
        "min_lot_size": 0.01,
        "spread_typical": 1.5,
        "trading_hours": {"start": 8, "end": 17, "timezone": "Europe/Zurich"},
        "high_impact_events": ["ECB", "SNB", "EU_Economic_Data", "Safe_Haven_Demand"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["range_trading", "support_resistance", "momentum"]
    },
    "GBPCHF": {
        "asset_class": "forex",
        "region": "Europe",
        "session": "European",
        "volatility_profile": "medium",
        "correlation_group": "safe_haven_pairs",
        "pip_value": 0.0001,
        "min_lot_size": 0.01,
        "spread_typical": 2.0,
        "trading_hours": {"start": 8, "end": 17, "timezone": "Europe/Zurich"},
        "high_impact_events": ["BOE", "SNB", "UK_Economic_Data", "Safe_Haven_Demand"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["range_trading", "support_resistance", "momentum"]
    },
    "AUDNZD": {
        "asset_class": "forex",
        "region": "Asia_Pacific",
        "session": "Asian",
        "volatility_profile": "medium",
        "correlation_group": "commodity_currencies",
        "pip_value": 0.0001,
        "min_lot_size": 0.01,
        "spread_typical": 1.8,
        "trading_hours": {"start": 21, "end": 7, "timezone": "UTC"},
        "high_impact_events": ["RBA", "RBNZ", "Commodity_Prices", "Risk_Sentiment"],
        "news_sources": ["financial_news", "economic_calendar"],
        "technical_folds": ["carry_trade", "trend_following", "correlation_analysis"]
    }
}

# Asset class groupings for correlation analysis - EXPANDED VERSION
ASSET_CLASS_GROUPS = {
    # === COMMODITIES ===
    "commodities": ["XAUUSD", "XAGUSD", "USOIL"],
    "precious_metals": ["XAUUSD", "XAGUSD"],
    "energy": ["USOIL"],

    # === EQUITY INDICES ===
    "equity_indices": ["SPX500", "NAS100", "US30", "DE40", "UK100", "FR40", "JP225", "AU200"],
    "us_indices": ["SPX500", "NAS100", "US30"],
    "european_indices": ["DE40", "UK100", "FR40"],
    "asian_indices": ["JP225", "AU200"],

    # === FOREX ===
    "forex": ["EURUSD", "GBPUSD", "USDJPY", "USDCHF", "AUDUSD", "USDCAD", "NZDUSD", "EURGBP", "EURJPY", "GBPJPY", "AUDJPY", "CADJPY", "CHFJPY", "EURCHF", "GBPCHF"],
    "major_pairs": ["EURUSD", "GBPUSD", "USDJPY", "USDCHF", "AUDUSD", "USDCAD", "NZDUSD"],
    "minor_pairs": ["EURGBP", "EURJPY", "GBPJPY", "AUDJPY", "CADJPY", "CHFJPY", "EURCHF", "GBPCHF"],
    "commodity_currencies": ["AUDUSD", "USDCAD", "NZDUSD", "AUDJPY", "CADJPY"],
    "cross_pairs": ["EURGBP", "EURJPY", "GBPJPY", "CHFJPY", "EURCHF", "GBPCHF"],
    "safe_haven_pairs": ["USDCHF", "CHFJPY", "EURCHF", "GBPCHF"],

    # === CRYPTOoldRRENCIES ===
    "crypto": ["BTCUSD", "ETHUSD"],
}
# (Add near other configuration variables like OANDA_API_KEY)
EODHD_API_KEY = " 68bafd7d44a7f025202650"
NEWSAPI_ORG_API_KEY = "abd8f43b808f42fdb8d28fb1c429af72"
FINNHUB_API_KEY = "d1b3ichr01qjhvtsbj70d1b3ichr01qjhvtsbj7g"
MARKETAUX_API_KEY = "CkuQmx9sPsjw0FRDeSkoO8U3O9Jj3HWnUYMJNEql"

# Auto-Retrain Configuration
AUTO_RETRAIN_CONFIG = {
    "ENABLED": True,  # Enable automatic retraining
    "TRIGGER_CONDITIONS": {
        "NEWS_FEATURES_CHANGED": True,  # Retrain when news featurefixre added/modified
        "FEATURE_ENGINEERING_UPDATED": True,  # Retrain when feature engineering changes
        "MODEL_ARCHITECTURE_CHANGED": True,  # Retrain when model structure changes
        "CONFIG_OPTIMIZATION_UPDATED": True,  # Retrain when optimization configs change
        "DATA_DRIFT_DETECTED": True,  # Retrain when data drift is detected
        "PERFORMANCE_DEGRADATION": True,  # Retrain when performance drops significantly
    },
    "RETRAIN_SETTINGS": {
        "IMMEDIATE_RETRAIN": True,  # Retrain immediately when triggered
        "RETRAIN_ALL_SYMBOLS": False,  # Only retrain affected symbols
        "FORCE_FULL_RETRAIN": False,  # Force complete retraining (not incremental)
        "BACKUP_BEFORE_RETRAIN": True,  # Backup fromodels before retraining
        "NOTIFY_ON_RETRAIN": True,  # Send notification when retraining starts
    },
    "PERFORMANCE_THRESHOLDS": {
        "MIN_ACCURACY_THRESHOLD": 0.55,  # Minimum accuracy to trigger retrain
        "MIN_F1_SCORE_THRESHOLD": 0.50,  # Minimum F1 score to trigger retrain
        "PERFORMANCE_DROP_THRESHOLD": 0.05,  # 5% performance drop triggers retrain
        "CONSECUTIVE_LOSSES_THRESHOLD": 5,  # 5 consecutive losses trigger retrain
    },
    "RETRAIN_SCHEDULE": {
        "DAILY_RETRAIN_CHECK": True,  # Check daily if retrain is needed
        "WEEKLY_FORCE_RETRAIN": False,  # Force retrain weekly regardless
        "MONTHLY_FULL_RETRAIN": True,  # Full retrain monthly
        "RETRAIN_TIME": "02:00",  # Preferred time for retraining (UTC)
    }
}

# You can change timeframefixnd main timeframe here
# --- CODE M I ---
#TIMEFRAMES = ["H4", "D1", "W1"]  # fallback if symbol doesn't have individual map
PRIMARY_TIMEFRAME = "H4"
# Add these 2 lines to configuration
# Add these thresholds to common configuration
MIN_F1_SCORE_GATE = 0.35       # Minimum F1-Score threshold (reduced for more symbols)
MAX_STD_F1_GATE = 0.20         # Maximum F1 standard deviation threshold (increased)
MIN_ACCURACY_GATE = 0.40       # Minimum win rate threshold (reduced for more symbols)
MIN_SAMPLES_GATE = 100         # Minimum sample count threshold for training
MAX_RETRAIN_ATTEMPTS = 3
ML_CONFIG = {
    "MIN_F1_SCORE": 0.35,  # Giảm từ 0.5 để linh hoạt hơn
    "MIN_ACCURACY": 0.40,  # Giảm từ 0.6 để linh hoạt hơn
    "MAX_STD_F1": 0.20,    # Tăng từ 0.1 để linh hoạt hơn
    "CV_N_SPLITS": 5,      # Gi m t10 dtang t c
    "CONFIDENCE_THRESHOLD": 0.6,  # Tang t0.5 dch t chhon
    "MIN_CONFIDENCE_TRADE": 0.50,  # Gi m t0.55 dlinh ho t hon
    "MIN_SAMPLES_FOR_TRAINING": 100,  # Gi m t300 dlinh ho t hon
    "MAX_CORRELATION_THRESHOLD": 0.85,  # Gi m t0.9 dch t chhon
    "EARLY_STOPPING_PATIENCE": 3,  # Ultra-strict: ttest results
    "REGULARIZATION_STRENGTH": 0.15,  # Ultra-sin: ttest results
    "DROPOUT_RATE": 0.7,  # Ultra-high: ttest results
    "BATCH_NORMALIZATION": True,  # Bật batch normalization
    "DATA_AUGMENTATION": True,  # Bật data augmentation
    "CROSS_VALIDATION_FOLDS": 10,  # Ttest results
    "OUT_OF_SAMPLE_TESTING": True,  # Bật out-of-sample testing
    "MAX_DEPTH_LIMIT": 4,  # Ultra-shallow: ttest results
    "MIN_SAMPLES_SPLIT_LIMIT": 50,  # Ultra-high: ttest results
    "MIN_SAMPLES_LEAF_LIMIT": 25,  # Ultra-high: ttest results
    "MAX_FEATURES_LIMIT": 0.3,  # Ultra-few: ttest results
    "CPU_THRESHOLD": 80,  # Ttest results
    "MEMORY_THRESHOLD": 85,  # Ttest results
    "ALERT_LIMIT": 5,  # Ttest results
    "OVERFITTING_DETECTION": True,  # Bật overfitting detection
    "VALIDATION_REQUIREMENTS": "STRICT",  # Strict validation mode
    "VALIDATION_THRESHOLD": 0.02,  # Ultra-strict validation threshold
    "GENERALIZATION_GAP_THRESHOLD": 0.02,  # Ultra-strict generalization gap
    "STABILITY_THRESHOLD": 0.02,  # Ultra-strict stability requirement

    # Feature selection and model configuration
    "FEATURE_SELECTION_TOP_K": 50,  # Top K features to select
    "FEATURE_IMPORTANCE_THRESHOLD": 0.01,  # Minimum feature importance
    "ENSEMBLE_MODELS": ["rf", "xgb", "lgb", "lstm"],  # Ensemble models
    "MODEL_STACKING_ENABLED": True,  # Enable model stacking

    # LSTM specific configuration
    "L2_REGULARIZATION": 0.001,  # L2 regularization strength
    "ATTENTION_MECHANISM": True,  # Enable attention mechanism
    "GRADIENT_CLIPPING": True,  # Enable gradient clipping
    "LEARNING_RATE_DECAY": 0.95,  # Learning rate decay factor
    "BATCH_SIZE": 64,  # Batch size for training
    "NOISE_INJECTION": 0.01  # Noise injection level for regularization
}

# === RISK MANAGEMENT BY ASSET CLASS ===
# (Configuration moved to top of file to avoid duplication)


# === TRADE FILTERS (new) ===
TRADE_FILTERS = {
    "SKIP_NEAR_HIGH_IMPACT_EVENTS": True,  # skip ±2h around major news
    "EVENT_BUFFER_HOURS": 2,
    "AVOID_WEEKEND": True,                  # don't open new trades near weekend
    "SEND_PRE_CHECK_STATUS_ALERT": True
}

# === BACKTEST & FORWARD TEST CONFIGURATION ===
# Removed unused BACKTEST_CONFIG and FORWARD_TEST_CONFIG - these were deactivated

# Performance thresholds for symbol selection
PERFORMANCE_THRESHOLDS = {
    "excellent": {
        "win_rate": 0.60,
        "profit_factor": 1.8,
        "sharpe_ratio": 1.0,
        "calmar_ratio": 0.8,
        "sortino_ratio": 1.2,
        "max_drawdown": 0.10,
        "avg_trade": 0.005,
        "consecutive_losses": 5,
        "recovery_factor": 1.2
    },
    "good": {
        "win_rate": 0.50,
        "profit_factor": 1.4,
        "sharpe_ratio": 0.7,
        "calmar_ratio": 0.5,
        "sortino_ratio": 0.8,
        "max_drawdown": 0.15,
        "avg_trade": 0.003,
        "consecutive_losses": 7,
        "recovery_factor": 1.0
    },
    "acceptable": {
        "win_rate": 0.45,
        "profit_factor": 1.2,
        "sharpe_ratio": 0.5,
        "calmar_ratio": 0.3,
        "sortino_ratio": 0.4,
        "max_drawdown": 0.20,
        "avg_trade": 0.001,
        "consecutive_losses": 8,
        "recovery_factor": 0.8
    }
}


# === OPTUNA (Enhanced Optimization) ===
OPTUNA_CONFIG = {
    "N_TRIALS": 200,        # Increase trials to find better parameters
    "TIMEOUT_SEC": 1800,    # Increase timeout to have enough time for optimization
    "PRUNING_ENABLED": True, # Enable pruning to remove poor trials
    "SAMPLER": "TPE",        # Tree-structured Parzen Estimator
    "EARLY_STOPPING_ROUNDS": 15,  # Increase early stopping rounds
    "DIRECTION": "maximize", # Optimization direction
    "LOAD_STUDY": True,     # Load study from file to continue optimization
    "STUDY_NAME": "trading_bot_optimization",  # Study name
    "STORAGE_URL": "sqlite:///optuna_study.db",  # Save study to Google Drive
    "N_STARTUP_TRIALS": 20,  # Number of initialization trials
    "N_WARMUP_STEPS": 10,    # Warmup steps for pruning
    "INTERMEDIATE_VALUES": True,  # Save intermediate values
    "MULTI_OBJECTIVE": False,     # Single objective optimization
    "CONSTRAINT_FUNCTIONS": [],   # Constraint functions if needed
    "PARALLEL_TRIALS": 4,         # Number of parallel trials
    "VERBOSE": True,              # Display detailed information
    "SHOW_PROGRESS_BAR": True,    # Display progress bar
    "BACKUP_STUDIES": True,       # Regular backup of studies
    "STUDY_CLEANUP": False        # Cleanup old studies
}

class OptunaStudyManager:
    """Manage Optuna studies with SQLite storage"""
    
    def __init__(self, storage_url=None):
        self.storage_url = storage_url or OPTUNA_CONFIG.get("STORAGE_URL", "sqlite:///optuna_study.db")
        self.storage = None
        self._ensure_directory()
        self._init_storage()
    
    def _ensure_directory(self):
        """Ensure Google Drive directory exists"""
        try:
            # Extract directory path from storage URL
            if "sqlite:///" in self.storage_url:
                db_path = self.storage_url.replace("sqlite:///", "")
                db_dir = os.path.dirname(db_path)
                
                if db_dir and not os.path.exists(db_dir):
                    os.makedirs(db_dir, exist_ok=True)
                    print(f"📁 [Optuna Manager] Created directory: {db_dir}")
                elif db_dir:
                    print(f"📁 [Optuna Manager] Directory exists: {db_dir}")
        except Exception as e:
            print(f"❌ [Optuna Manager] Directory creation failed: {e}")
    
    def _init_storage(self):
        """Khởi tạo storage"""
        try:
            import optuna
            self.storage = optuna.storages.RDBStorage(self.storage_url)
            print(f"✅ [Optuna Manager] Storage initialized: {self.storage_url}")
        except Exception as e:
            print(f"❌ [Optuna Manager] Storage initialization failed: {e}")
            self.storage = None
    
    def create_or_load_study(self, study_name, direction="maximize", sampler=None, pruner=None):
        """Create or load study with storage"""
        if not self.storage:
            print("⚠️ [Optuna Manager] No storage available, using in-memory study")
            return optuna.create_study(direction=direction, sampler=sampler, pruner=pruner)
        
        try:
            study = optuna.create_study(
                study_name=study_name,
                storage=self.storage,
                direction=direction,
                sampler=sampler,
                pruner=pruner,
                load_if_exists=True
            )
            print(f"✅ [Optuna Manager] Study '{study_name}' loaded/created")
            return study
        except Exception as e:
            print(f"❌ [Optuna Manager] Study creation failed: {e}")
            return optuna.create_study(direction=direction, sampler=sampler, pruner=pruner)
    
    def get_study_info(self, study_name):
        """Get study information"""
        if not self.storage:
            return None
        
        try:
            study = optuna.load_study(study_name=study_name, storage=self.storage)
            return {
                "name": study_name,
                "n_trials": len(study.trials),
                "best_value": study.best_value if study.trials else None,
                "best_params": study.best_params if study.trials else None,
                "direction": study.direction.name
            }
        except Exception as e:
            print(f"❌ [Optuna Manager] Failed to get study info: {e}")
            return None
    
    def list_studies(self):
        """List all studies"""
        if not self.storage:
            return []
        
        try:
            return optuna.get_all_study_summaries(self.storage)
        except Exception as e:
            print(f"❌ [Optuna Manager] Failed to list studies: {e}")
            return []
    
    def backup_study(self, study_name, backup_path=None):
        """Backup study"""
        if not self.storage:
            return False
        
        try:
            study = optuna.load_study(study_name=study_name, storage=self.storage)
            if not backup_path:
                backup_path = f"optuna_backup_{study_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
            
            # Export study data
            study_data = {
                "trials": [trial._trial_id for trial in study.trials],
                "best_value": study.best_value,
                "best_params": study.best_params,
                "direction": study.direction.name
            }
            
            import json
            with open(backup_path.replace('.db', '.json'), 'w') as f:
                json.dump(study_data, f, indent=2)
            
            print(f"💾 [Optuna Manager] Study '{study_name}' backed up to {backup_path}")
            return True
        except Exception as e:
            print(f"❌ [Optuna Manager] Backup failed: {e}")
            return False

# === TIMEFRAME MAPPING (EXPANDED) ===
# Keep old variable as default to avoid breaking other parts:
PRIMARY_TIMEFRAME_DEFAULT = PRIMARY_TIMEFRAME  # usually "H4"

# Optimized timeframe mapping based on asset characteristics
PRIMARY_TIMEFRAME_BY_SYMBOL = {
    # === COMMODITIES (Swing trading following trends) ===
    "XAUUSD": "H4",    # Gold - trend following, less noise
    "XAGUSD": "H4",    # Silver - trend following
    "USOIL": "H4",     # Crude Oil - trend following

    # === EQUITY INDICES (Fast reaction during session hours) ===
    "SPX500": "H1",    # S&P 500 - US market, fast reaction
    "NAS100": "H1",    # NASDAQ - tech-heavy, volatile
    "US30": "H1",      # Dow Jones - blue chip index
    "DE40": "H1",      # DAX - German market
    "UK100": "H1",     # FTSE 100 - UK market
    "FR40": "H1",      # CAC 40 - French market
    "JP225": "H1",     # Nikkei 225 - Japanese market
    "AU200": "H1",     # ASX 200 - Australian market

    # === FOREX MAJOR PAIRS (Swing trading with carry considerations) ===
    "EURUSD": "H4",    # Major pair - swing trading
    "GBPUSD": "H4",    # Major pair - swing trading
    "USDJPY": "H4",    # Major pair - swing trading
    "USDCHF": "H4",    # Major pair - swing trading
    "AUDUSD": "H4",    # Commodity currency - carry trade
    "USDCAD": "H4",    # Commodity currency - carry trade
    "NZDUSD": "H4",    # Commodity currency - carry trade

    # === FOREX MINOR PAIRS ===
    "EURGBP": "H4",    # Cross pair - swing trading
    "EURJPY": "H4",    # Cross pair - carry trade
    "GBPJPY": "H4",    # Cross pair - carry trade
    "AUDJPY": "H4",    # Cross pair - carry trade
    "CADJPY": "H4",    # Cross pair - carry trade
    "CHFJPY": "H4",    # Cross pair - carry trade
    "EURCHF": "H4",    # Cross pair - range trading
    "GBPCHF": "H4",    # Cross pair - range trading
    "AUDNZD": "H4",    # Cross pair - carry trade

    # === CRYPTOoldRRENCIES (24/7 Trading) ===
    "BTCUSD": "H1",    # Bitcoin - high frequency trading
    "ETHUSD": "H1",    # Ethereum - high frequency trading

}

# Secondary timeframes for multi-timeframe analysis
SECONDARY_TIMEFRAMES_BY_SYMBOL = {
    # === COMMODITIES ===
    "XAUUSD": ["H1", "D1"],    # Entry timing + trend
    "XAGUSD": ["H1", "D1"],    # Entry timing + trend
    "USOIL": ["H1", "D1"],     # Entry timing + trend

    # === EQUITY INDICES ===
    "SPX500": ["H4", "D1"],    # Trend context
    "NAS100": ["H4", "D1"],    # Trend context
    "US30": ["H4", "D1"],      # Trend context
    "DE40": ["H4", "D1"],      # Trend context
    "UK100": ["H4", "D1"],     # Trend context
    "FR40": ["H4", "D1"],      # Trend context
    "JP225": ["H4", "D1"],     # Trend context
    "AU200": ["H4", "D1"],     # Trend context

    # === FOREX ===
    "EURUSD": ["H1", "D1"],    # Entry timing + trend
    "GBPUSD": ["H1", "D1"],    # Entry timing + trend
    "USDJPY": ["H1", "D1"],    # Entry timing + trend
    "USDCHF": ["H1", "D1"],    # Entry timing + trend
    "AUDUSD": ["H1", "D1"],    # Entry timing + trend
    "USDCAD": ["H1", "D1"],    # Entry timing + trend
    "NZDUSD": ["H1", "D1"],    # Entry timing + trend
    "EURGBP": ["H1", "D1"],    # Entry timing + trend
    "EURJPY": ["H1", "D1"],    # Entry timing + trend
    "GBPJPY": ["H1", "D1"],    # Entry timing + trend
    "AUDJPY": ["H1", "D1"],    # Entry timing + trend
    "CADJPY": ["H1", "D1"],    # Entry timing + trend
    "CHFJPY": ["H1", "D1"],    # Entry timing + trend
    "EURCHF": ["H1", "D1"],    # Entry timing + trend
    "GBPCHF": ["H1", "D1"],    # Entry timing + trend
    "AUDNZD": ["H1", "D1"],    # Entry timing + trend
}

# Optimal lookback periods by asset class
LOOKBACK_PERIODS_BY_ASSET_CLASS = {
    "equity_index": [21, 55, 89, 144],  # Standard Fibonacci
    "commodity": [21, 55, 89, 144, 233],  # Longer trends
    "forex": [21, 55, 89],  # Shorter for forex
}

# Enhanced Timezone Management
TIMEZONE_CONFIG = {
    "DEFAULT_TIMEZONE": "Asia/Bangkok",  # Timezone mặc định dành cho bot
    "UTC_TIMEZONE": "UTC",
    "SYMBOL_TIMEZONES": {
        # === COMMODITIES ===
        "XAUUSD": "UTC",
        "XAGUSD": "UTC",
        "USOIL": "UTC",

        # === EQUITY INDICES ===
        "SPX500": "US/Eastern",
        "NAS100": "US/Eastern",
        "US30": "US/Eastern",
        "DE40": "Europe/Berlin",
        "UK100": "Europe/London",
        "FR40": "Europe/Paris",
        "JP225": "Asia/Tokyo",
        "AU200": "Australia/Sydney",

        # === FOREX ===
        "EURUSD": "Europe/London",
        "GBPUSD": "Europe/London",
        "USDJPY": "Asia/Tokyo",
        "USDCHF": "Europe/Zurich",
        "AUDUSD": "Australia/Sydney",
        "USDCAD": "US/Eastern",
        "NZDUSD": "Pacific/Auckland",
        "EURGBP": "Europe/London",
        "EURJPY": "Europe/London",
        "GBPJPY": "Europe/London",
        "AUDJPY": "Australia/Sydney",
        "CADJPY": "US/Eastern",
        "CHFJPY": "Europe/Zurich",
        "EURCHF": "Europe/Zurich",
        "GBPCHF": "Europe/Zurich"
    },
    "TRADING_SESSIONS": {
        "ASIAN": {"timezone": "Asia/Tokyo", "hours": [0, 8]},
        "EUROPEAN": {"timezone": "Europe/London", "hours": [8, 16]},
        "AMERICAN": {"timezone": "US/Eastern", "hours": [13, 21]},
        "CRYPTO": {"timezone": "UTC", "hours": [0, 23]}  # 24/7
    }
}

def get_current_time(timezone_name: str = "Asia/Bangkok") -> datetime:
    """
    Get current time in specified timezone with improved error handling
    
    Args:
        timezone_name: Name of the timezone (default: Asia/Bangkok)
        
    Returns:
        Current datetime object with timezone
        
    Raises:
        Exception: If timezone is invalid
    """
    try:
        import pytz
        tz = pytz.timezone(timezone_name)
        return datetime.now(tz)
    except Exception as e:
        logging.warning(f"⚠️ [Time] Timezone error for {timezone_name}: {e}")
        # Fallback to UTC
        try:
            return datetime.now(pytz.UTC)
        except:
            return datetime.now()

def convert_timezone(dt: Union[datetime, str], from_tz: str, to_tz: str) -> datetime:
    """
    Convert datetime from one timezone to another with improved error handling
    
    Args:
        dt: Datetime object or ISO string to convert
        from_tz: Source timezone name
        to_tz: Target timezone name
        
    Returns:
        Converted datetime object
        
    Raises:
        Exception: If timezone conversion fails
    """
    try:
        import pytz
        
        # Handle string input
        if isinstance(dt, str):
            dt = datetime.fromisoformat(dt)
        
        from_timezone = pytz.timezone(from_tz)
        to_timezone = pytz.timezone(to_tz)
        
        if dt.tzinfo is None:
            dt = from_timezone.localize(dt)
        
        return dt.astimezone(to_timezone)
    except Exception as e:
        logging.error(f"❌ [Time] Timezone conversion error from {from_tz} to {to_tz}: {e}")
        return dt if isinstance(dt, datetime) else datetime.now()

# Enhanced timeframe mapping with optimized session awareness
ENHANCED_TIMEFRAME_MAPPING = {
    "SPX500": {
        "primary": "H1",
        "secondary": ["H4", "D1"],
        "session_hours": {"start": 9, "end": 16, "timezone": "US/Eastern"},
        "weekend_guard": True,
        "pre_market": {"start": 4, "end": 9},
        "after_hours": {"start": 16, "end": 20},
        "high_volatility_periods": [{"start": 9, "end": 11}, {"start": 14, "end": 16}],
        "news_sensitivity": "high",
        "gap_risk": True
    },
    "DE40": {
        "primary": "H1",
        "secondary": ["H4", "D1"],
        "session_hours": {"start": 9, "end": 17, "timezone": "Europe/Berlin"},
        "weekend_guard": True,
        "lunch_break": {"start": 12, "end": 13},
        "high_volatility_periods": [{"start": 9, "end": 11}, {"start": 15, "end": 17}],
        "news_sensitivity": "high",
        "gap_risk": True
    },
    "XAUUSD": {
        "primary": "H4",
        "secondary": ["H1", "D1"],
        "session_hours": {"start": 0, "end": 23, "timezone": "UTC"},  # 24h market
        "weekend_guard": True,
        "high_activity_hours": [{"start": 8, "end": 10}, {"start": 13, "end": 15}, {"start": 20, "end": 22}],
        "news_sensitivity": "very_high",
        "gap_risk": False,
        "overnight_risk": True
    },
    "AUDNZD": {
        "primary": "H4",
        "secondary": ["H1", "D1"],
        "session_hours": {"start": 21, "end": 7, "timezone": "UTC"},  # Asian session
        "weekend_guard": True,
        "overlap_hours": [{"start": 21, "end": 1}, {"start": 7, "end": 9}],
        "news_sensitivity": "medium",
        "gap_risk": False,
        "carry_trade_folds": True
    },
    "BTCUSD": {
        "primary": "H1",
        "secondary": ["M15", "H4"],
        "session_hours": {"start": 0, "end": 23, "timezone": "UTC"},  # 24/7
        "weekend_guard": False,  # Crypto trades on weekends
        "high_volatility_hours": [{"start": 8, "end": 12}, {"start": 20, "end": 24}],
        "news_sensitivity": "very_high",
        "gap_risk": False,
        "weekend_trading": True
    },
    "ETHUSD": {
        "primary": "H1",
        "secondary": ["M15", "H4"],
        "session_hours": {"start": 0, "end": 23, "timezone": "UTC"},  # 24/7
        "weekend_guard": False,  # Crypto trades on weekends
        "high_volatility_hours": [{"start": 8, "end": 12}, {"start": 20, "end": 24}],
        "news_sensitivity": "very_high",
        "gap_risk": False,
        "weekend_trading": True
    },
    "JP225": {
        "primary": "H1",
        "secondary": ["H4", "D1"],
        "session_hours": {"start": 9, "end": 15, "timezone": "Asia/Tokyo"},
        "weekend_guard": True,
        "lunch_break": {"start": 11, "end": 12},
        "high_volatility_periods": [{"start": 9, "end": 11}, {"start": 13, "end": 15}],
        "news_sensitivity": "high",
        "gap_risk": True
    }
}

# TF set allowing pathising primary
TIMEFRAME_SET_BY_PRIMARY = {
    "H4": ["H4","D1","W1"],   # decision H4; D1/W1 for trend
    "H1": ["H1","H4","D1"],   # decision H1; H4/D1 for context
    "D1": ["D1","W1"]         # if using D1 as primary
}

# === TIMEFRAME-SPECIFIC OPTIMIZATION CONFIG ===
TIMEFRAME_ENTRY_CONFIG = {
    "H1": {
        "entry_method": "scalping_momentum",
        "atr_multiplier_sl": 1.5,
        "atr_multiplier_tp": 2.0,
        "min_rr_ratio": 1.3,
        "max_rr_ratio": 2.5,
        "support_resistance_weight": 0.2,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": False,
        "lookback_periods": [20, 50, 100],
        "features": ["momentum", "volatility", "session", "volume"],
        "model_type": "ensemble",
        "risk_multiplier": 0.8
    },
    "H4": {
        "entry_method": "swing_trading",
        "atr_multiplier_sl": 2.0,
        "atr_multiplier_tp": 3.0,
        "min_rr_ratio": 1.5,
        "max_rr_ratio": 3.0,
        "support_resistance_weight": 0.3,
        "volume_confirmation": True,
        "session_filter": True,
        "volatility_adjustment": True,
        "lookback_periods": [50, 100, 200],
        "features": ["trend", "momentum", "support_resistance", "session"],
        "model_type": "trend_following",
        "risk_multiplier": 1.0
    },
    "D1": {
        "entry_method": "position_trading",
        "atr_multiplier_sl": 2.5,
        "atr_multiplier_tp": 4.0,
        "min_rr_ratio": 1.6,
        "max_rr_ratio": 3.5,
        "support_resistance_weight": 0.4,
        "volume_confirmation": False,
        "session_filter": False,
        "volatility_adjustment": True,
        "lookback_periods": [100, 200, 500],
        "features": ["trend", "support_resistance", "fundamental", "seasonality"],
        "model_type": "position_trading",
        "risk_multiplier": 1.2
    },
    "W1": {
        "entry_method": "long_term_trend",
        "atr_multiplier_sl": 3.0,
        "atr_multiplier_tp": 5.0,
        "min_rr_ratio": 1.7,
        "max_rr_ratio": 4.0,
        "support_resistance_weight": 0.5,
        "volume_confirmation": False,
        "session_filter": False,
        "volatility_adjustment": True,
        "lookback_periods": [200, 500, 1000],
        "features": ["long_term_trend", "seasonality", "macro", "fundamental"],
        "model_type": "macro_trading",
        "risk_multiplier": 1.5
    }
}

# === TIMEFRAME-SPECIFIC MODEL TRAINING CONFIG ===
TIMEFRAME_MODEL_CONFIG = {
    "H1": {
        "ensemble_models": ["XGBoost", "LightGBM", "RandomForest"],
        "hyperparams": {
            "XGBoost": {
                "n_estimators": [100, 200, 300],
                "max_depth": [3, 4, 5],
                "learning_rate": [0.01, 0.05, 0.1],
                "subsample": [0.8, 0.9, 1.0]
            },
            "LightGBM": {
                "n_estimators": [100, 200, 300],
                "max_depth": [3, 4, 5],
                "learning_rate": [0.01, 0.05, 0.1],
                "num_leaves": [31, 50, 100]
            }
        },
        "validation_split": 0.2,
        "early_stopping_rounds": 50,
        "cross_validation_folds": 5
    },
    "H4": {
        "ensemble_models": ["XGBoost", "LightGBM", "LSTM"],
        "hyperparams": {
            "XGBoost": {
                "n_estimators": [200, 300, 500],
                "max_depth": [4, 5, 6],
                "learning_rate": [0.01, 0.05, 0.1],
                "subsample": [0.8, 0.9, 1.0]
            },
            "LSTM": {
                "units": [64, 128, 256],
                "layers": [2, 3, 4],
                "dropout": [0.2, 0.3, 0.5],
                "batch_size": [32, 64, 128],
                "seq_len": [60, 120, 240]
            }
        },
        "validation_split": 0.25,
        "early_stopping_rounds": 100,
        "cross_validation_folds": 7
    },
    "D1": {
        "ensemble_models": ["XGBoost", "LSTM", "PPO"],
        "hyperparams": {
            "XGBoost": {
                "n_estimators": [300, 500, 1000],
                "max_depth": [5, 6, 7],
                "learning_rate": [0.01, 0.05, 0.1],
                "subsample": [0.8, 0.9, 1.0]
            },
            "PPO": {
                "n_steps": [128, 256, 512],
                "gamma": [0.95, 0.99, 0.995],
                "gae_lambda": [0.9, 0.95, 0.99],
                "ent_coef": [0.01, 0.1, 0.2],
                "learning_rate": [0.0001, 0.0003, 0.001]
            }
        },
        "validation_split": 0.3,
        "early_stopping_rounds": 150,
        "cross_validation_folds": 10
    },
    "W1": {
        "ensemble_models": ["XGBoost", "PPO", "Transformer"],
        "hyperparams": {
            "XGBoost": {
                "n_estimators": [500, 1000, 2000],
                "max_depth": [6, 7, 8],
                "learning_rate": [0.01, 0.05, 0.1],
                "subsample": [0.8, 0.9, 1.0]
            },
            "PPO": {
                "n_steps": [256, 512, 1024],
                "gamma": [0.99, 0.995, 0.999],
                "gae_lambda": [0.95, 0.99, 0.995],
                "ent_coef": [0.1, 0.2, 0.3],
                "learning_rate": [0.0001, 0.0003, 0.001]
            }
        },
        "validation_split": 0.35,
        "early_stopping_rounds": 200,
        "cross_validation_folds": 15
    }
}

# === TIMEFRAME-SPECIFIC RISK MANAGEMENT ===
TIMEFRAME_RISK_CONFIG = {
    "H1": {
        "max_position_size": 0.02,  # 2% per trade
        "max_daily_loss": 0.05,    # 5% daily loss limit
        "max_concurrent_trades": 3,
        "correlation_threshold": 0.7,
        "volatility_adjustment": True,
        "session_risk_multiplier": {
            "asian": 0.8,
            "london": 1.0,
            "new_york": 1.2,
            "overlap": 1.5
        }
    },
    "H4": {
        "max_position_size": 0.03,  # 3% per trade
        "max_daily_loss": 0.08,    # 8% daily loss limit
        "max_concurrent_trades": 5,
        "correlation_threshold": 0.6,
        "volatility_adjustment": True,
        "session_risk_multiplier": {
            "asian": 0.9,
            "london": 1.0,
            "new_york": 1.1,
            "overlap": 1.3
        }
    },
    "D1": {
        "max_position_size": 0.05,  # 5% per trade
        "max_daily_loss": 0.12,    # 12% daily loss limit
        "max_concurrent_trades": 8,
        "correlation_threshold": 0.5,
        "volatility_adjustment": True,
        "session_risk_multiplier": {
            "asian": 1.0,
            "london": 1.0,
            "new_york": 1.0,
            "overlap": 1.0
        }
    },
    "W1": {
        "max_position_size": 0.08,  # 8% per trade
        "max_daily_loss": 0.15,     # 15% daily loss limit
        "max_concurrent_trades": 10,
        "correlation_threshold": 0.4,
        "volatility_adjustment": True,
        "session_risk_multiplier": {
            "asian": 1.0,
            "london": 1.0,
            "new_york": 1.0,
            "overlap": 1.0
        }
    }
}
WEEKEND_CLOSE_CONFIG = {
    "ENABLED": True,
    "CLOSE_DAY_UTC": 4,          # 4 = Friday (keep original)
    "CLOSE_HOUR_UTC": 17,        # Changed to 17:00 UTC
    "CLOSE_MINUTE_UTC": 0
}
MODEL_DIR = f"saved_models_{PRIMARY_TIMEFRAME.lower()}"
os.makedirs(MODEL_DIR, exist_ok=True)
# --- Step 1: Define common "Interface" for all providers ---
class NewsQualityScorer:
    """Enhanced news quality scoring system"""
    
    def __init__(self):
        # Source reliability scores (higher = more reliable)
        self.source_scores = {
            "Reuters": 0.95,
            "Bloomberg": 0.90,
            "Wall Street Journal": 0.90,
            "Financial Times": 0.85,
            "MarketWatch": 0.80,
            "Yahoo Finance": 0.75,
            "CNBC": 0.80,
            "Forbes": 0.70,
            "Finnhub": 0.75,
            "Marketaux": 0.70,
            "NewsAPI.org": 0.65,
            "EODHD": 0.60
        }
        
        # Keywords that indicate high-quality news
        self.quality_keywords = {
            "earnings": 0.8,
            "revenue": 0.7,
            "profit": 0.7,
            "merger": 0.9,
            "acquisition": 0.9,
            "partnership": 0.6,
            "guidance": 0.8,
            "forecast": 0.7,
            "analyst": 0.6,
            "upgrade": 0.7,
            "downgrade": 0.7,
            "rating": 0.6
        }
        
        # Spam/low-quality indicators
        self.spam_keywords = {
            "click here": -0.5,
            "free": -0.3,
            "guaranteed": -0.4,
            "limited time": -0.3,
            "act now": -0.4,
            "exclusive": -0.2,
            "breaking": 0.1,  # Can be legitimate
            "urgent": -0.2
        }
    
    def score_news_item(self, news_item):
        """Score a single news item for quality"""
        if not news_item:
            return 0.0
        
        score = 0.5  # Base score
        
        # Source reliability
        source = news_item.get('source', '').lower()
        for source_name, source_score in self.source_scores.items():
            if source_name.lower() in source:
                score = max(score, source_score)
                break
        
        # Title and summary analysis
        title = news_item.get('title', '').lower()
        summary = news_item.get('summary', '').lower()
        text = f"{title} {summary}"
        
        # Quality keywords bonus
        for keyword, weight in self.quality_keywords.items():
            if keyword in text:
                score += weight * 0.1
        
        # Spam keywords penalty
        for keyword, penalty in self.spam_keywords.items():
            if keyword in text:
                score += penalty
        
        # Length bonus (longer articles tend to be more informative)
        text_length = len(text)
        if text_length > 200:
            score += 0.1
        elif text_length < 50:
            score -= 0.2
        
        # Recency bonus (newer news is more relevant)
        published_at = news_item.get('published_at')
        if published_at:
            try:
                if isinstance(published_at, str):
                    published_at = pd.to_datetime(published_at)
                
                # Fix timezone issue: ensure both datetimes have same timezone awareness
                current_time = datetime.now()
                if published_at.tzinfo is not None:
                    # If published_at is timezone-aware, make current_time aware too
                    import pytz
                    current_time = current_time.replace(tzinfo=pytz.UTC)
                elif current_time.tzinfo is not None:
                    # If current_time is timezone-aware but published_at is not, make published_at aware
                    import pytz
                    published_at = published_at.replace(tzinfo=pytz.UTC)
                
                hours_ago = (current_time - published_at).total_seconds() / 3600
                if hours_ago < 24:
                    score += 0.1
                elif hours_ago > 168:  # 1 week
                    score -= 0.2
            except:
                pass
        
        # Normalize to 0-1 range
        return max(0.0, min(1.0, score))
    
    def filter_high_quality_news(self, news_items, min_score=0.6):
        """Filter news items by quality score"""
        if not news_items:
            return []
        
        scored_news = []
        for item in news_items:
            score = self.score_news_item(item)
            if score >= min_score:
                item['quality_score'] = score
                scored_news.append(item)
        
        # Sort by quality score (highest first)
        scored_news.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
        return scored_news

class APIMonitoringSystem:
    """Comprehensive API monitoring and health tracking system"""
    
    def __init__(self):
        self.api_stats = {
            "finnhub": {"calls": 0, "success": 0, "errors": 0, "last_call": None, "rate_limit": 60},
            "marketaux": {"calls": 0, "success": 0, "errors": 0, "last_call": None, "rate_limit": 100},
            "newsapi": {"calls": 0, "success": 0, "errors": 0, "last_call": None, "rate_limit": 1000},
            "eodhd": {"calls": 0, "success": 0, "errors": 0, "last_call": None, "rate_limit": 20},
            "trading_economics": {"calls": 0, "success": 0, "errors": 0, "last_call": None, "rate_limit": 100}
        }
        
        self.health_thresholds = {
            "min_success_rate": 0.8,  # 80% success rate minimum
            "max_error_rate": 0.2,    # 20% error rate maximum
            "max_response_time": 10.0,  # 10 seconds max response time
            "rate_limit_warning": 0.8   # Warn at 80% of rate limit
        }
        
        self.alerts_sent = set()  # Track sent alerts to avoid spam
        self.response_times = {}  # Track response times per API
    
    def log_api_call(self, api_name, success=True, response_time=None, error_msg=None):
        """Log API call statistics"""
        if api_name not in self.api_stats:
            return
        
        stats = self.api_stats[api_name]
        stats["calls"] += 1
        stats["last_call"] = datetime.now()
        
        if success:
            stats["success"] += 1
        else:
            stats["errors"] += 1
            if error_msg:
                print(f"❌ [API Monitor] {api_name} error: {error_msg}")
        
        if response_time:
            if api_name not in self.response_times:
                self.response_times[api_name] = []
            self.response_times[api_name].append(response_time)
            # Keep only last 100 response times
            if len(self.response_times[api_name]) > 100:
                self.response_times[api_name] = self.response_times[api_name][-100:]
    
    def check_api_health(self, api_name):
        """Check if API is healthy based on statistics"""
        if api_name not in self.api_stats:
            return True, "API not tracked"
        
        stats = self.api_stats[api_name]
        
        if stats["calls"] == 0:
            return True, "No calls made yet"
        
        success_rate = stats["success"] / stats["calls"]
        error_rate = stats["errors"] / stats["calls"]
        
        # Check success rate
        if success_rate < self.health_thresholds["min_success_rate"]:
            return False, f"Low success rate: {success_rate:.2%}"
        
        # Check error rate
        if error_rate > self.health_thresholds["max_error_rate"]:
            return False, f"High error rate: {error_rate:.2%}"
        
        # Check response time
        if api_name in self.response_times and self.response_times[api_name]:
            avg_response_time = np.mean(self.response_times[api_name])
            if avg_response_time > self.health_thresholds["max_response_time"]:
                return False, f"Slow response time: {avg_response_time:.2f}s"
        
        return True, "Healthy"
    
    def check_rate_limits(self, api_name):
        """Check if API ifixpproaching rate limits"""
        if api_name not in self.api_stats:
            return True, "API not tracked"
        
        stats = self.api_stats[api_name]
        rate_limit = stats["rate_limit"]
        
        # Check calls in lasfrom modelinute
        if stats["last_call"]:
            time_since_last = (datetime.now() - stats["last_call"]).total_seconds()
            if time_since_last < 60:  # Within lasfrom modelinute
                calls_per_minute = stats["calls"] / max(1, time_since_last / 60)
                if calls_per_minute > rate_limit * self.health_thresholds["rate_limit_warning"]:
                    return False, f"Approaching rate limit: {calls_per_minute:.1f}/{rate_limit} calls/min"
        
        return True, "Within rate limits"
    
    def get_api_status_report(self):
        """Generate comprehensive API status report"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "apis": {},
            "overall_health": "healthy",
            "recommendations": []
        }
        
        unhealthy_apis = []
        
        for api_name, stats in self.api_stats.items():
            is_healthy, health_msg = self.check_api_health(api_name)
            is_rate_ok, rate_msg = self.check_rate_limits(api_name)
            
            api_status = {
                "calls": stats["calls"],
                "success_rate": stats["success"] / max(1, stats["calls"]),
                "error_rate": stats["errors"] / max(1, stats["calls"]),
                "last_call": stats["last_call"].isoformat() if stats["last_call"] else None,
                "health_status": "healthy" if is_healthy else "unhealthy",
                "health_message": health_msg,
                "rate_limit_status": "ok" if is_rate_ok else "warning",
                "rate_limit_message": rate_msg
            }
            
            # Add response time if available
            if api_name in self.response_times and self.response_times[api_name]:
                api_status["avg_response_time"] = np.mean(self.response_times[api_name])
            
            report["apis"][api_name] = api_status
            
            if not is_healthy or not is_rate_ok:
                unhealthy_apis.append(api_name)
                report["recommendations"].append(f"Check {api_name}: {health_msg}")
        
        if unhealthy_apis:
            report["overall_health"] = "degraded"
            report["recommendations"].append("Consider switching to backup APIs or reducing call frequency")
        
        return report
    
    def send_health_alert(self, bot_instance, api_name, issue):
        """Send health alert for API issues"""
        alert_key = f"{api_name}_{issue}_{datetime.now().strftime('%Y%m%d%H')}"
        
        if alert_key in self.alerts_sent:
            return  # Already sent this hour
        
        try:
            message = f"🚨 **API HEALTH ALERT** 🚨\n- **API:** {api_name}\n- **Issue:** {issue}\n- **Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            bot_instance.send_discord_alert(message)
            self.alerts_sent.add(alert_key)
        except Exception as e:
            print(f"❌ [API Monitor] Failed to send alert: {e}")
    
    def reset_daily_stats(self):
        """Reset daily statistics (call this daily)"""
        for api_name in self.api_stats:
            self.api_stats[api_name]["calls"] = 0
            self.api_stats[api_name]["success"] = 0
            self.api_stats[api_name]["errors"] = 0
        
        self.alerts_sent.clear()
        print("🔄 [API Monitor] Daily statistics reset")

class ConceptDriftDetector:
    """Advanced concept drift detection system"""
    
    def __init__(self):
        self.drift_threshold = 0.2
        self.performance_window = 30  # 30 days
        self.data_window = 1000  # Last 1000 samples for comparison
        
        # Try to import drift detection libraries
        try:
            from frouros.detectors.concept_drift import DDM
            self.ddm_detector = DDM()
            self.frouros_available = True
        except ImportError:
            self.frouros_available = False
            print("⚠️ Frouros not available. Using basic drift detection.")
        
        # Try River first (modern alternative to scikit-multiflow)
        try:
            from river import drift
            self.adwin_detector = drift.ADWIN()
            self.river_available = True
            print("Using River ADWIN for drift detection")
        except ImportError:
            self.river_available = False
            print("⚠️ River not available. Trying scikit-multiflow...")
            
            # Fallback to scikit-multiflow
            try:
                from skmultiflow.drift_detection import ADWIN
                self.adwin_detector = ADWIN()
                self.skmultiflow_available = True
                print("Using scikit-multiflow ADWIN")
            except ImportError:
                self.skmultiflow_available = False
                print("⚠️ scikit-multiflow not available. Using simple drift detection.")
                
                # Final fallback: Simple drift detector
                class SimpleDriftDetector:
                    """Simple drift detector thay thcho ADWIN"""
                    
                    def __init__(self):
                        self.window = []
                        self.max_window = 1000
                        self.drift_threshold = 0.1
                    
                    def add_element(self, value):
                        """Add element to window"""
                        self.window.append(value)
                        if len(self.window) > self.max_window:
                            self.window.pop(0)
                    
                    def detected_change(self):
                        """Simple drift detection"""
                        if len(self.window) < 100:
                            return False
                        
                        # Simple variance-based drift detection
                        recent_var = np.var(self.window[-50:])
                        old_var = np.var(self.window[-100:-50])
                        
                        if old_var > 0:
                            change_ratio = abs(recent_var - old_var) / old_var
                            return change_ratio > self.drift_threshold
                        
                        return False
                
                self.adwin_detector = SimpleDriftDetector()
        
        # Fallback: Basic statistical drift detection
        self.performance_history = {}
        self.data_history = {}
    
    def detect_performance_drift(self, symbol, recent_performance, baseline_performance):
        """Detect performance-based concept drift"""
        if not recent_performance or not baseline_performance:
            return False, "Insufficient data"
        
        # Calculate performance degradation
        degradation = (baseline_performance - recent_performance) / baseline_performance
        
        if degradation > self.drift_threshold:
            return True, f"Performance degradation: {degradation:.2%}"
        
        return False, f"Performance stable: {degradation:.2%}"
    
    def detect_data_distribution_drift(self, symbol, new_data, old_data):
        """Detect data distribution drift using statistical tests"""
        if len(new_data) < 50 or len(old_data) < 50:
            return False, "Insufficient data for drift detection"
        
        try:
            from scipy import stats
            
            # Kolmogorov-Smirnov test
            ks_stat, p_value = stats.ks_2samp(old_data, new_data)
            
            if p_value < 0.05:  # Significant difference
                return True, f"Data drift detected: KS p-value={p_value:.4f}"
            
            return False, f"No data drift: KS p-value={p_value:.4f}"
            
        except ImportError:
            # Fallback: Simple variance comparison
            old_var = np.var(old_data)
            new_var = np.var(new_data)
            var_ratio = new_var / old_var if old_var > 0 else 1
            
            if abs(var_ratio - 1) > 0.5:  # 50% variance change
                return True, f"Variance drift: ratio={var_ratio:.2f}"
            
            return False, f"No variance drift: ratio={var_ratio:.2f}"
    
    def detect_feature_importance_drift(self, symbol, old_features, new_features):
        """Detect feature importance drift"""
        if not old_features or not new_features:
            return False, "No feature importance data"
        
        # Compare top feature rankings
        old_top_features = sorted(old_features.items(), key=lambda x: x[1], reverse=True)[:5]
        new_top_features = sorted(new_features.items(), key=lambda x: x[1], reverse=True)[:5]
        
        old_top_names = [f[0] for f in old_top_features]
        new_top_names = [f[0] for f in new_top_features]
        
        # Calculate overlap
        overlap = len(set(old_top_names) & set(new_top_names)) / len(old_top_names)
        
        if overlap < 0.6:  # Less than 60% overlap
            return True, f"Feature importance drift: overlap={overlap:.2%}"
        
        return False, f"Feature importance stable: overlap={overlap:.2%}"
    
    def detect_drift(self, symbol, performance_data=None, data_samples=None, feature_importance=None):
        """Comprehensive drift detection"""
        drift_results = {
            "performance_drift": False,
            "data_drift": False,
            "feature_drift": False,
            "overall_drift": False,
            "details": {}
        }
        
        # Performance drift detection
        if performance_data:
            recent_perf = performance_data.get('recent', 0)
            baseline_perf = performance_data.get('baseline', 0)
            
            perf_drift, perf_msg = self.detect_performance_drift(
                symbol, recent_perf, baseline_perf
            )
            drift_results["performance_drift"] = perf_drift
            drift_results["details"]["performance"] = perf_msg
        
        # Data distribution drift detection
        if data_samples and len(data_samples) >= 2:
            new_data = data_samples.get('new', [])
            old_data = data_samples.get('old', [])
            
            data_drift, data_msg = self.detect_data_distribution_drift(
                symbol, new_data, old_data
            )
            drift_results["data_drift"] = data_drift
            drift_results["details"]["data"] = data_msg
        
        # Feature importance drift detection
        if feature_importance:
            old_features = feature_importance.get('old', {})
            new_features = feature_importance.get('new', {})
            
            feature_drift, feature_msg = self.detect_feature_importance_drift(
                symbol, old_features, new_features
            )
            drift_results["feature_drift"] = feature_drift
            drift_results["details"]["features"] = feature_msg
        
        # Overall drift assessment
        drift_results["overall_drift"] = any([
            drift_results["performance_drift"],
            drift_results["data_drift"],
            drift_results["feature_drift"]
        ])
        
        return drift_results

class OnlineLearningManager:
    """Online learning manager for continuous model updates"""
    
    def __init__(self, bot_instance):
        self.bot = bot_instance
        self.update_frequency = "daily"  # daily, hourly, real-time
        self.batch_size = 100  # Number of samples per update
        self.learning_rate = 0.01
        self.models = {}  # Store online learning models per symbol
        
        # Try to import River for advanced online learning
        try:
            from river import linear_model, forest, tree
            self.river_available = True
            self.river_models = {
                'logistic': linear_model.LogisticRegression(),
                'ensemble': forest.ARFRegressor(),
                'tree': tree.HoeffdingTreeRegressor()
            }
            print("River models initialized for online learning")
        except ImportError:
            self.river_available = False
            print("⚠️ River not available. Using sklearn SGDClassifier.")
        
        # Fallback: sklearn SGDClassifier
        from sklearn.linear_model import SGDClassifier
        self.sgd_models = {}
    
    def _log_and_print(self, level: str, message: str, print_message: str = None):
        """
        Log and print message with specified level
        
        Args:
            level: Log level (debug, info, warning, error)
            message: Message to log
            print_message: Optional message to print (if different from log message)
        """
        try:
            # Use bot's logger if available
            if hasattr(self.bot, 'logger'):
                logger = self.bot.logger
            else:
                # Fallback to standard logging
                import logging
                logger = logging.getLogger('OnlineLearningManager')
            
            # Log the message
            if level.lower() == 'debug':
                logger.debug(message)
            elif level.lower() == 'info':
                logger.info(message)
            elif level.lower() == 'warning':
                logger.warning(message)
            elif level.lower() == 'error':
                logger.error(message)
            else:
                logger.info(message)  # Default to info
            
            # Print message if provided
            if print_message:
                print(print_message)
            else:
                print(message)
                
        except Exception as e:
            # Fallback to simple print if logging fails
            print(f"[{level.upper()}] {message}")
    
    def _safe_get_value(self, data, key, default=0.0):
        """
        Safely get value from data dictionary with fallback
        
        Args:
            data: Dictionary to get value from
            key: Key to look for
            default: Default value if key not found
            
        Returns:
            Value from data or default
        """
        try:
            if isinstance(data, dict) and key in data:
                value = data[key]
                # Convert to float if possible
                if isinstance(value, (int, float)):
                    return float(value)
                elif isinstance(value, str):
                    try:
                        return float(value)
                    except ValueError:
                        return default
                else:
                    return default
            else:
                return default
        except Exception:
            return default
    
    def _ensure_river_format(self, features):
        """Ensure features are in correct format for River models (dictionary)"""
        # River models expect a dictionary where keys are feature names
        if isinstance(features, dict):
            return features
        elif isinstance(features, (list, np.ndarray)):
            # Convert list/array to dictionary with feature names
            features_list = features if isinstance(features, list) else features.tolist()
            return {f'feature_{i}': float(val) for i, val in enumerate(features_list)}
        else:
            # Single value case
            return {'feature_0': float(features)}
    
    def _ensure_sklearn_format(self, features):
        """Ensure features are in correct format for sklearn models (numpy array)"""
        if isinstance(features, np.ndarray):
            return features
        elif isinstance(features, list):
            return np.array(features)
        else:
            return np.array([features])
    
    def initialize_all_online_models(self, active_symbols):
        """Initialize online learning models for ALL active symbols"""
        print("[Online Learning] Initializing models for ALL active symbols...")
        
        # Force initialize for ALL active symbols, regardless of action
        for symbol in active_symbols:
            # Always initialize, even if already exists (to ensure consistency)
            self.initialize_online_model(symbol)
            print(f"[Online Learning] ✅ Initialized River logistic model for {symbol}")
        
        print(f"[Online Learning] ✅ Completed initialization for {len(active_symbols)} symbols")
        print(f"[Online Learning] ✅ All active symbols now have Online Learning models")
    
    def initialize_online_model(self, symbol, model_type='logistic'):
        """Initialize online learning model for a symbol"""
        # Always create a new model instance to avoid sharing
        if self.river_available and model_type in self.river_models:
            # Create a fresh model instance for each symbol
            if model_type == 'logistic':
                from river import linear_model
                self.models[symbol] = linear_model.LogisticRegression()
            elif model_type == 'ensemble':
                from river import forest
                self.models[symbol] = forest.ARFRegressor()
            elif model_type == 'tree':
                from river import tree
                self.models[symbol] = tree.HoeffdingTreeRegressor()
            
            print(f"[Online Learning] ✅ Initialized River {model_type} model for {symbol}")
        else:
            # Fallback to SGDClassifier
            from sklearn.linear_model import SGDClassifier
            self.models[symbol] = SGDClassifier(
                learning_rate='adaptive',
                eta0=self.learning_rate,
                random_state=42
            )
            print(f"[Online Learning] ✅ Initialized SGDClassifier for {symbol}")
    
    def check_online_learning_status(self):
        """Check status of Online Learning models for all symbols"""
        print("\n🔍 ONLINE LEARNING STATUS CHECK")
        print("=" * 50)
        
        active_symbols = list(self.bot.active_symbols) if hasattr(self, 'bot') else []
        
        print(f"📊 Active symbols: {active_symbols}")
        print(f"🤖 Models initialized: {list(self.models.keys())}")
        
        missing_models = []
        for symbol in active_symbols:
            if symbol not in self.models:
                missing_models.append(symbol)
        
        if missing_models:
            print(f"⚠️ Missing models for: {missing_models}")
            print("🔄 Initializing missing models...")
            for symbol in missing_models:
                self.initialize_online_model(symbol)
        else:
            print("✅ All active symbols have Online Learning models")
        
        print(f"✅ Final model count: {len(self.models)}")
        print("=" * 50)
    
    def update_model_incremental(self, symbol, X_new, y_new):
        """Update model with new data incrementally"""
        if symbol not in self.models:
            self.initialize_online_model(symbol)
        
        model = self.models[symbol]
        
        try:
            if self.river_available and hasattr(model, 'learn_one'):
                # River online learning - handle dictionary format properly
                for x, y in zip(X_new, y_new):
                    # Ensure features are in the correct dictionary format for River
                    features_for_learn = self._ensure_river_format(x)
                    model.learn_one(features_for_learn, y)
                print(f"✅ [Online Learning] Updated River model for {symbol} with {len(X_new)} samples")
            else:
                # sklearn SGDClassifier - ensure X_new is numpy array
                if isinstance(X_new, list):
                    X_new = np.array(X_new)
                elif isinstance(X_new, pd.DataFrame):
                    X_new = X_new.values
                
                # Ensure y_new is properly formatted
                if isinstance(y_new, list):
                    y_new = np.array(y_new, dtype=np.int64)
                elif isinstance(y_new, pd.Series):
                    y_new = y_new.values.astype(np.int64)
                else:
                    y_new = np.array([y_new], dtype=np.int64)
                
                # Ensure X_new is 2D
                if X_new.ndim == 1:
                    X_new = X_new.reshape(-1, 1)
                
                model.partial_fit(X_new, y_new, classes=np.unique(y_new))
                print(f"✅ [Online Learning] Updated SGD model for {symbol} with {len(X_new)} samples")
            
            return True
            
        except Exception as e:
            print(f"[Online Learning] Failed to update model for {symbol}: {e}")
            logging.error(f"Error in update_model_incremental: {e}")
            logging.error(f"X_new type: {type(X_new)}, X_new value: {X_new}")
            logging.error(f"y_new type: {type(y_new)}, y_new value: {y_new}")
            return False
    
    def get_online_prediction(self, symbol, X):
        """Get prediction from online learning model"""
        if symbol not in self.models:
            return None
        
        model = self.models[symbol]
        
        try:
            if self.river_available and hasattr(model, 'predict_one'):
                # River prediction - handle list format properly
                # Check if X is a single sample (list of features) or multiple samples
                if isinstance(X, list) and len(X) > 0 and isinstance(X[0], (int, float)):
                    # Single sample: X is a list of features
                    features_for_pred = self._ensure_river_format(X)
                    return model.predict_one(features_for_pred)
                else:
                    # Multiple samples: X is a list of lists
                    predictions = []
                    for x in X:
                        if isinstance(x, list):
                            features_for_pred = x
                        elif isinstance(x, np.ndarray):
                            features_for_pred = x.tolist()
                        else:
                            features_for_pred = list(x) if hasattr(x, '__iter__') else [x]
                        predictions.append(model.predict_one(self._ensure_river_format(features_for_pred)))
                    return predictions
            else:
                # sklearn prediction - ensure X is numpy array
                if isinstance(X, list):
                    # Check if it's a single sample or multiple samples
                    if len(X) > 0 and isinstance(X[0], (int, float)):
                        # Single sample: reshape to (1, n_features)
                        X = np.array(X).reshape(1, -1)
                    else:
                        # Multiple samples
                        X = np.array(X)
                return model.predict(X)
                
        except Exception as e:
            print(f"[Online Learning] Prediction failed for {symbol}: {e}")
            logging.error(f"Error in get_online_prediction: {e}")
            logging.error(f"X type: {type(X)}, X value: {X}")
            return None
    
    def should_update_model(self, symbol):
        """Check if model should be updated based on frequency"""
        if self.update_frequency == "real-time":
            return True
        elif self.update_frequency == "hourly":
            # Check if last update was more than 1 hour ago
            return True  # Simplified for now
        elif self.update_frequency == "daily":
            # Check if last update was more than 1 day ago
            return True  # Simplified for now
        
        return False
    
    def process_decision_feedback(self, feedback_data):
        """Process decision feedback to update model in real-time"""
        try:
            symbol = feedback_data['symbol']
            final_decision = feedback_data['final_decision']
            final_confidence = feedback_data['final_confidence']
            decision_consistency = feedback_data.get('decision_consistency', 1.0)  # Default to 1.0 if missing
            
            # T o training sample from feedback
            training_sample = self._create_training_sample_from_feedback(feedback_data)
            
            if training_sample:
                # Update online learning model
                self._update_online_model_incremental(symbol, training_sample)
                
                # Cập nhật performance tracking
                self._update_performance_tracking(symbol, feedback_data)
                
                # Trigger concept drift detection if needs
                self._check_concept_drift_from_feedback(symbol, feedback_data)
                
                logging.info(f"✅ [Online Learning] Processed feedback for {symbol}: {final_decision} ({final_confidence:.2%})")
            
        except Exception as e:
            logging.error(f"Error processing decision feedback: {e}")
    
    def _create_training_sample_from_feedback(self, feedback_data):
        """T o training sample from feedback data"""
        try:
            symbol = feedback_data['symbol']
            
            # Get fromarket data to create features
            if hasattr(self.bot, 'data_manager'):
                current_data = self.bot.data_manager.get_latest_data(symbol)
                if current_data is None:
                    return None
                
                # Extract features tmarket data
                features = self._extract_features_from_market_data(current_data)
                
                # Create target from final decision and confidence
                target = self._create_target_from_decision(feedback_data)
                
                return {
                    'features': features,
                    'target': target,
                    'confidence': feedback_data['final_confidence'],
                    'consistency': feedback_data.get('decision_consistency', 1.0)
                }
            
            return None
            
        except Exception as e:
            logging.error(f"Error creating training sample: {e}")
            return None
    
    def _convert_market_data_to_dict(self, market_data: Any) -> Dict[str, Any]:
        """
        Convert various data types to dictionary format
        
        Args:
            market_data: Input data (DataFrame, Series, list, tuple, or dict)
            
        Returns:
            Dictionary representation of the data
        """
        if isinstance(market_data, dict):
            return market_data
        elif isinstance(market_data, pd.DataFrame):
            if market_data.empty:
                self._log_and_print("warning", "Empty DataFrame provided as market data")
                return {}
            return market_data.iloc[-1].to_dict()
        elif isinstance(market_data, pd.Series):
            return market_data.to_dict()
        elif isinstance(market_data, (list, tuple)):
            if len(market_data) >= 4:  # Assuming [open, high, low, close, ...]
                return {
                    'open': market_data[0] if len(market_data) > 0 else 0.0,
                    'high': market_data[1] if len(market_data) > 1 else 0.0,
                    'low': market_data[2] if len(market_data) > 2 else 0.0,
                    'close': market_data[3] if len(market_data) > 3 else 0.0,
                    'volume': market_data[4] if len(market_data) > 4 else 0.0
                }
            else:
                self._log_and_print("warning", f"Insufficient data in list/tuple: {len(market_data)} elements")
                return {}
        else:
            self._log_and_print("warning", f"Market data is not a dictionary: {type(market_data)}")
            return {}
    
    def _extract_technical_features(self, market_data: Dict[str, Any]) -> List[float]:
        """
        Extract technical indicator features
        
        Args:
            market_data: Dictionary containing market data
            
        Returns:
            List of technical features
        """
        features = []
        feature_keys = ['rsi', 'macd', 'macd_signal', 'trend_strength', 'volume_factor']
        
        for key in feature_keys:
            features.append(self._safe_get_value(market_data, key, 0.0))
        
        return features
    
    def _extract_price_features(self, market_data: Dict[str, Any]) -> List[float]:
        """
        Extract price-related features
        
        Args:
            market_data: Dictionary containing market data
            
        Returns:
            List of price features
        """
        features = []
        
        # Close price
        if 'close' in market_data:
            features.append(self._safe_get_value(market_data, 'close', 0.0))
        else:
            features.append(0.0)
        
        # High-Low range
        if 'high' in market_data and 'low' in market_data:
            high_val = self._safe_get_value(market_data, 'high', 0.0)
            low_val = self._safe_get_value(market_data, 'low', 0.0)
            features.append(high_val - low_val)
        else:
            features.append(0.0)
        
        return features
    
    def _extract_features_from_market_data(self, market_data: Any) -> np.ndarray:
        """Extract features from market data with enhanced error handling"""
        try:
            # Convert to dictionary format
            market_dict = self._convert_market_data_to_dict(market_data)
            
            if not market_dict:
                return np.zeros(FeatureConstants.MIN_CANDLES_FOR_ANALYSIS, dtype=np.float64)
            
            features = []
            
            # Extract technical features
            features.extend(self._extract_technical_features(market_dict))
            
            # Extract price features
            features.extend(self._extract_price_features(market_dict))
            
            # Ensure we have exactly 10 features
            while len(features) < 10:
                features.append(0.0)
            
            # Convert to numpy array with validation
            features_array = np.array(features[:10], dtype=np.float64)
            
            # Validate array shape and content
            if features_array.shape != (10,):
                self._log_and_print("warning", f"Invalid features shape: {features_array.shape}, expected (10,)")
                return np.zeros(10, dtype=np.float64)
            
            # Check for invalid values
            if np.any(np.isnan(features_array)) or np.any(np.isinf(features_array)):
                self._log_and_print("warning", "Features contain NaN or Inf values, using zeros")
                return np.zeros(10, dtype=np.float64)
            
            return features_array
            
        except Exception as e:
            self._log_and_print("error", f"Error extracting features: {e}")
            return np.zeros(10, dtype=np.float64)
    
    def _create_target_from_decision(self, feedback_data):
        """T o target tdecision feedback"""
        try:
            final_decision = feedback_data['final_decision']
            final_confidence = feedback_data['final_confidence']
            
            # Convert decision to numerical target
            decision_mapping = {
                'BUY': 1,
                'SELL': -1,
                'HOLD': 0
            }
            
            base_target = decision_mapping.get(final_decision, 0)
            
            # Adjust target based on confidence
            confidence_adjusted_target = base_target * final_confidence
            
            return confidence_adjusted_target
            
        except Exception as e:
            logging.error(f"Error creating target: {e}")
            return 0.0
    
    def _update_online_model_incremental(self, symbol, training_sample):
        """Update online learning model incrementally"""
        try:
            if symbol not in self.models:
                self.initialize_online_model(symbol)
            
            # Handle features properly - ensure it's properly formatted
            features_raw = training_sample['features']
            
            # Convert to numpy array first for consistency
            if isinstance(features_raw, np.ndarray):
                features_array = features_raw.astype(np.float64).reshape(1, -1)
            elif isinstance(features_raw, list):
                features_array = np.array(features_raw, dtype=np.float64).reshape(1, -1)
            else:
                features_array = np.array([features_raw], dtype=np.float64).reshape(1, -1)
            
            target = training_sample['target']
            
            # Update model based on type
            if self.river_available and symbol in self.models:
                # River online learning - convert to dictionary format
                model = self.models[symbol]
                features_for_learn = self._ensure_river_format(features_array.flatten())
                model.learn_one(features_for_learn, target)
                
            elif symbol in self.models:
                # SGD online learning - use numpy array
                model = self.models[symbol]
                # Ensure target is properly formatted as numpy array
                if isinstance(target, (list, tuple)):
                    target_array = np.array(target, dtype=np.int64)
                else:
                    target_array = np.array([target], dtype=np.int64)
                
                # Ensure features_array is 2D for sklearn
                if features_array.ndim == 1:
                    features_array = features_array.reshape(1, -1)
                
                model.partial_fit(features_array, target_array)
            
            logging.info(f"✅ [Online Learning] Updated model for {symbol} with confidence {training_sample['confidence']:.3f}")
            
        except Exception as e:
            logging.error(f"Error updating online model: {e}")
            if 'features_raw' in locals():
                logging.error(f"Features type: {type(features_raw)}, Features value: {features_raw}")
            else:
                logging.error("Features not available due to earlier error")
            logging.error(f"Training sample keys: {list(training_sample.keys()) if hasattr(training_sample, 'keys') else 'No keys'}")
    
    def _update_performance_tracking(self, symbol, feedback_data):
        """Cập nhật performance tracking từ feedback"""
        try:
            if not hasattr(self, 'performance_history'):
                self.performance_history = {}
            
            if symbol not in self.performance_history:
                self.performance_history[symbol] = []
            
            # Add feedback to performance history
            self.performance_history[symbol].append({
                'timestamp': feedback_data['timestamp'],
                'decision': feedback_data['final_decision'],
                'confidence': feedback_data['final_confidence'],
                'consistency': feedback_data.get('decision_consistency', 1.0)
            })
            
            # Gich100 records g n nh t
            if len(self.performance_history[symbol]) > 100:
                self.performance_history[symbol] = self.performance_history[symbol][-100:]
            
        except Exception as e:
            logging.error(f"Error updating performance tracking: {e}")
    
    def _check_concept_drift_from_feedback(self, symbol, feedback_data):
        """Check concept drift from feedback"""
        try:
            if hasattr(self.bot, 'auto_retrain_manager'):
                drift_detector = self.bot.auto_retrain_manager.drift_detector
                
                # T o performance data from feedback
                performance_data = {
                    'recent_decisions': [feedback_data['final_decision']],
                    'recent_confidences': [feedback_data['final_confidence']],
                    'consistency_scores': [feedback_data.get('decision_consistency', 1.0)]
                }
                
                # Check drift
                drift_result = drift_detector.detect_drift(symbol, performance_data=performance_data)
                
                if drift_result and drift_result.get('should_retrain', False):
                    logging.warning(f"⚠️ [Concept Drift] Detected drift for {symbol}: {drift_result.get('reason', 'Unknown')}")
                    # Trigger retraining
                    self.bot.auto_retrain_manager.trigger_retrain(symbol, drift_result.get('reason', 'Concept drift detected'))
            
        except Exception as e:
            logging.error(f"Error checking concept drift: {e}")
    
    def get_online_prediction_enhanced(self, symbol, market_data):
        """Get prediction from online learning model with enhanced features"""
        try:
            if symbol not in self.models:
                return "HOLD", 0.5
            
            # Extract features - now returns numpy array
            features = self._extract_features_from_market_data(market_data)
            
            # Ensure features is properly formatted - features is already numpy array
            if isinstance(features, np.ndarray):
                features_array = features.astype(np.float64).reshape(1, -1)
            elif isinstance(features, list):
                features_array = np.array(features, dtype=np.float64).reshape(1, -1)
            else:
                features_array = np.array([features], dtype=np.float64).reshape(1, -1)
            
            # Get prediction
            if self.river_available and symbol in self.models:
                model = self.models[symbol]
                # Convert numpy array to dictionary for River
                features_for_pred = self._ensure_river_format(features_array.flatten())
                prediction = model.predict_one(features_for_pred)
                
                # Convert prediction to decision
                if prediction > 0.3:
                    decision = "BUY"
                    confidence = min(prediction, 0.9)
                elif prediction < -0.3:
                    decision = "SELL"
                    confidence = min(abs(prediction), 0.9)
                else:
                    decision = "HOLD"
                    confidence = 0.5
                    
            elif symbol in self.models:
                model = self.models[symbol]
                # Use numpy array directly for sklearn
                prediction = model.predict(features_array)[0]
                
                # Convert prediction to decision
                if prediction > 0.5:
                    decision = "BUY"
                    confidence = prediction
                elif prediction < 0.5:
                    decision = "SELL"
                    confidence = 1 - prediction
                else:
                    decision = "HOLD"
                    confidence = 0.5
            
            else:
                return "HOLD", 0.5
            
            return decision, confidence
            
        except Exception as e:
            logging.error(f"Error getting online prediction: {e}")
            if 'features' in locals():
                logging.error(f"Features type: {type(features)}, Features value: {features}")
            else:
                logging.error("Features not available due to earlier error")
            logging.error(f"Market data keys: {list(market_data.keys()) if hasattr(market_data, 'keys') else 'No keys'}")
            return "HOLD", 0.5

class DynamicEnsembleManager:
    """Dynamic ensemble weight adjustment based on recent performance"""
    
    def __init__(self, bot_instance):
        self.bot = bot_instance
        self.model_weights = {}
        self.performance_history = {}
        self.weight_update_frequency = "daily"
        self.min_weight = 0.1  # Minimum weight for any model
        self.max_weight = 0.8  # Maximum weight for any model
    
    def initialize_weights(self, symbol, model_names):
        """Initialize equal weights for all models"""
        num_models = len(model_names)
        equal_weight = 1.0 / num_models
        
        self.model_weights[symbol] = {}
        for model_name in model_names:
            self.model_weights[symbol][model_name] = equal_weight
        
        print(f"[Dynamic Ensemble] Initialized weights for {symbol}: {self.model_weights[symbol]}")
    
    def update_model_performance(self, symbol, model_name, performance_score):
        """Update performance score for a specific model"""
        if symbol not in self.performance_history:
            self.performance_history[symbol] = {}
        
        if model_name not in self.performance_history[symbol]:
            self.performance_history[symbol][model_name] = []
        
        # Keep only last 30 performance scores
        self.performance_history[symbol][model_name].append(performance_score)
        if len(self.performance_history[symbol][model_name]) > 30:
            self.performance_history[symbol][model_name] = self.performance_history[symbol][model_name][-30:]
    
    def adjust_model_weights(self, symbol):
        """Adjusfrom modelodel weights based on recent performance"""
        if symbol not in self.performance_history:
            return
        
        # Calculate average performance for each model
        model_performances = {}
        for model_name, scores in self.performance_history[symbol].items():
            if scores:
                model_performances[model_name] = np.mean(scores)
        
        if not model_performances:
            return
        
        # Calculate new weights based on performance
        total_performance = sum(model_performances.values())
        if total_performance == 0:
            return
        
        new_weights = {}
        for model_name, performance in model_performances.items():
            # Weight proportional to performance
            weight = performance / total_performance
            
            # Apply min/max constraints
            weight = max(self.min_weight, min(self.max_weight, weight))
            new_weights[model_name] = weight
        
        # Normalize weights to sum to 1
        total_weight = sum(new_weights.values())
        if total_weight > 0:
            for model_name in new_weights:
                new_weights[model_name] /= total_weight
        
        self.model_weights[symbol] = new_weights
        
        print(f"⚖️ [Dynamic Ensemble] Updated weights for {symbol}: {new_weights}")
    
    def get_model_weights(self, symbol):
        """Get fromodel weights for a symbol"""
        return self.model_weights.get(symbol, {})
    
    def get_weighted_prediction(self, symbol, model_predictions):
        """Get weighted ensemble prediction"""
        weights = self.get_model_weights(symbol)
        
        if not weights or not model_predictions:
            return None
        
        weighted_prediction = 0
        total_weight = 0
        
        for model_name, prediction in model_predictions.items():
            if model_name in weights:
                weight = weights[model_name]
                weighted_prediction += prediction * weight
                total_weight += weight
        
        if total_weight > 0:
            return weighted_prediction / total_weight
        
        return None

class AutoRetrainManager:
    """Enhanced manager for automatic retraining with concept drift detection"""
    
    def __init__(self, bot_instance):
        self.bot = bot_instance
        self.config = AUTO_RETRAIN_CONFIG
        self.last_retrain_time = {}
        self.performance_history = {}
        self.consecutive_losses = {}
        self.feature_version = "1.0"  # Track feature engineering version
        self.model_version = "1.0"    # Track model architecture version
        
        # Initialize concept drift detection
        self.drift_detector = ConceptDriftDetector()
        
        # Initialize online learning
        self.online_learning = OnlineLearningManager(bot_instance)
        
        # Initialize online learning models for all active symbols
        self.online_learning.initialize_all_online_models(list(bot_instance.active_symbols))
        
        # Check status of all Online Learning models
        self.online_learning.check_online_learning_status()
        
        # Initialize dynamic ensemble
        self.dynamic_ensemble = DynamicEnsembleManager(bot_instance)
        
    def check_retrain_triggers(self, symbol):
        """Enhanced retrain trigger checking with concept drift detection"""
        if not self.config["ENABLED"]:
            return False, "Auto-retrain disabled"
        
        # 1. Check concept drift first
        drift_result = self.check_concept_drift(symbol)
        if drift_result["should_retrain"]:
            return True, f"Concept drift detected: {drift_result['reason']}"
        
        # 2. Check performance degradation
        perf_result = self.check_performance_degradation(symbol)
        if perf_result["should_retrain"]:
            return True, f"Performance degradation: {perf_result['reason']}"
        
        # 3. Check consecutive losses
        loss_result = self.check_consecutive_losses(symbol)
        if loss_result["should_retrain"]:
            return True, f"Consecutive losses: {loss_result['reason']}"
        
        # 4. Check scheduled retraining
        self.check_scheduled_retrain()  # This method handlefixll symbols internally
        
        return False, "No retrain triggers detected"
    
    def check_concept_drift(self, symbol):
        """Check for concept drift using advanced detection"""
        try:
            # Get recent performance data
            recent_performance = self.get_recent_performance(symbol, days=7)
            baseline_performance = self.get_baseline_performance(symbol)
            
            # Get recent data samples for drift detection
            recent_data = self.get_recent_data_samples(symbol, samples=100)
            old_data = self.get_historical_data_samples(symbol, samples=100)
            
            # Get feature importance data
            feature_importance = self.get_feature_importance_data(symbol)
            
            # Perforlevelomprehensive drift detection
            drift_results = self.drift_detector.detect_drift(
                symbol=symbol,
                performance_data={
                    'recent': recent_performance,
                    'baseline': baseline_performance
                },
                data_samples={
                    'new': recent_data,
                    'old': old_data
                },
                feature_importance=feature_importance
            )
            
            if drift_results["overall_drift"]:
                print(f"⚠️ [Concept Drift] {symbol}: Drift detected!")
                for drift_type, details in drift_results["details"].items():
                    print(f"   - {drift_type}: {details}")
                
                return {
                    "should_retrain": True,
                    "reason": f"Concept drift detected: {drift_results['details']}",
                    "drift_details": drift_results
                }
            
            return {
                "should_retrain": False,
                "reason": "No concept drift detected",
                "drift_details": drift_results
            }
            
        except Exception as e:
            print(f"❌ [Concept Drift] Error checking drift for {symbol}: {e}")
            return {
                "should_retrain": False,
                "reason": f"Error in drift detection: {e}",
                "drift_details": {}
            }
    
    def get_recent_data_samples(self, symbol, samples=100):
        """Get recent data samples for drift detection"""
        try:
            # This would typically get recenfrom modelarket data
            # For now, return dummy data
            return np.random.normal(0, 1, samples)
        except Exception as e:
            print(f"❌ [Data Samples] Error getting recent data for {symbol}: {e}")
            return []
    
    def get_historical_data_samples(self, symbol, samples=100):
        """Get historical data samples for comparison"""
        try:
            # This would typically get historical market data
            # For now, return dummy data
            return np.random.normal(0, 1, samples)
        except Exception as e:
            print(f"❌ [Data Samples] Error getting historical data for {symbol}: {e}")
            return []
    
    def get_feature_importance_data(self, symbol):
        """Get feature importance data for drift detection"""
        try:
            # This would typically get feature importance from the model
            # For now, return dummy data
            return {
                'old': {'feature1': 0.3, 'feature2': 0.25, 'feature3': 0.2},
                'new': {'feature1': 0.28, 'feature2': 0.27, 'feature3': 0.19}
            }
        except Exception as e:
            print(f"❌ [Feature Importance] Error getting feature data for {symbol}: {e}")
            return None
    
    def check_performance_degradation(self, symbol):
        """Check if performance has degraded significantly"""
        try:
            recent_performance = self.get_recent_performance(symbol, days=7)
            baseline_performance = self.get_baseline_performance(symbol)
            
            if recent_performance < baseline_performance * 0.85:  # 15% degradation
                return {
                    "should_retrain": True,
                    "reason": f"Performance degraded: {recent_performance:.2%} vs {baseline_performance:.2%}"
                }
            return {
                "should_retrain": False,
                "reason": f"Performance stable: {recent_performance:.2%}"
            }
        except Exception as e:
            print(f"❌ [Auto-Retrain] Error checking performance degradation for {symbol}: {e}")
            return {
                "should_retrain": False,
                "reason": f"Error: {e}"
            }
    
    def check_consecutive_losses(self, symbol):
        """Check if consecutive losses exceed threshold"""
        try:
            consecutive = self.consecutive_losses.get(symbol, 0)
            threshold = self.config["PERFORMANCE_THRESHOLDS"]["CONSECUTIVE_LOSSES_THRESHOLD"]
            
            if consecutive >= threshold:
                return {
                    "should_retrain": True,
                    "reason": f"Consecutive losses: {consecutive} >= {threshold}"
                }
            return {
                "should_retrain": False,
                "reason": f"Consecutive losses: {consecutive} < {threshold}"
            }
        except Exception as e:
            print(f"❌ [Auto-Retrain] Error checking consecutive losses for {symbol}: {e}")
            return {
                "should_retrain": False,
                "reason": f"Error: {e}"
            }
    
    
    def get_recent_performance(self, symbol, days=7):
        """Get recent performance for a symbol"""
        try:
            # This would typically calculate recent performance
            # For now, return a dummy value
            return 0.65  # 65% performance
        except Exception as e:
            print(f"❌ [Auto-Retrain] Error getting recent performance for {symbol}: {e}")
            return 0.0
    
    def get_baseline_performance(self, symbol):
        """Get baseline performance for a symbol"""
        try:
            # This would typically calculate baseline performance
            # For now, return a dummy value
            return 0.75  # 75% baseline performance
        except Exception as e:
            print(f"❌ [Auto-Retrain] Error getting baseline performance for {symbol}: {e}")
            return 0.0
    
    def _check_data_drift(self, symbol):
        """Check for data drift using the existing driffrom modelonitor"""
        if hasattr(self.bot, 'drift_monitor') and self.bot.drift_monitor:
            drift_score = self.bot.drift_monitor.get_drift_score(symbol)
            threshold = self.bot.drift_monitor.drift_threshold
            
            return drift_score >= threshold
        
        return False
    
    def _check_feature_changes(self, symbol):
        """Check if news features have beeifpdated"""
        # This would be triggered when news featurefixre added/modified
        # For now, we'll use a simple version check
        current_version = "1.1"  # Updated version with news features
        return current_version != self.feature_version
    
    def _check_model_changes(self, symbol):
        """Check if model architecture has changed"""
        # This would be triggered when model structure changes
        # For now, we'll use a simple version check
        current_version = "1.1"  # Updated version with new architecture
        return current_version != self.model_version
    
    def trigger_retrain(self, symbol, reason):
        """Trigger retraining for a symbol"""
        if not self.config["RETRAIN_SETTINGS"]["IMMEDIATE_RETRAIN"]:
            return False
        
        print(f"🔄 [Auto-Retrain] Triggering retrain for {symbol}: {reason}")
        
        # Backup fromodels if enabled
        if self.config["RETRAIN_SETTINGS"]["BACKUP_BEFORE_RETRAIN"]:
            self._backup_models(symbol)
        
        # Send notification if enabled
        if self.config["RETRAIN_SETTINGS"]["NOTIFY_ON_RETRAIN"]:
            self._send_retrain_notification(symbol, reason)
        
        # Perform retraining
        try:
            if self.config["RETRAIN_SETTINGS"]["FORCE_FULL_RETRAIN"]:
                # Force complete retraining
                self.bot.load_or_train_models(force_retrain_symbols=[symbol])
            else:
                # Incremental retraining - fetch data first
                try:
                    print(f"📊 [Auto-Retrain] Fetching data for {symbol}...")
                    df_retrain = self.bot.data_manager.create_enhanced_features(symbol)
                    print(f"📊 [Auto-Retrain] Data fetch result for {symbol}: {type(df_retrain)}, length: {len(df_retrain) if df_retrain is not None else 'None'}")
                    if df_retrain is not None and len(df_retrain) >= 100:
                        print(f"[Auto-Retrain] Data fetched: {len(df_retrain)} candles for {symbol}")
                        result = self.bot.train_enhanced_model(symbol, df_retrain)
                        if result is None:
                            print(f"[Auto-Retrain] train_enhanced_model returned None for {symbol}")
                            return False
                        print(f"[Auto-Retrain] train_enhanced_model completed for {symbol}")
                    else:
                        print(f"[Auto-Retrain] Insufficient data for {symbol}: {len(df_retrain) if df_retrain is not None else 0} candles")
                        return False
                except Exception as data_error:
                    print(f"[Auto-Retrain] Failed to fetch data for {symbol}: {data_error}")
                    import traceback
                    traceback.print_exc()
                    return False
            
            # Update version tracking
            self.feature_version = "1.1"
            self.model_version = "1.1"
            self.last_retrain_time[symbol] = datetime.now()
            
            print(f"[Auto-Retrain] Successfully retrained {symbol}")
            return True
            
        except Exception as e:
            print(f"[Auto-Retrain] Failed to retrain {symbol}: {e}")
            return False
    
    def _backup_models(self, symbol):
        """Backup fromodels before retraining"""
        try:
            backup_dir = f"model_backups/{symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            os.makedirs(backup_dir, exist_ok=True)
            
            # Copy fromodel files
            model_files = glob.glob(f"saved_models_*/{symbol}*")
            for file_path in model_files:
                shutil.copy2(file_path, backup_dir)
            
            print(f"💾 [Auto-Retrain] Backed up models for {symbol} to {backup_dir}")
            
        except Exception as e:
            print(f"❌ [Auto-Retrain] Failed to backup models for {symbol}: {e}")
    
    def _send_retrain_notification(self, symbol, reason):
        """Send notification about retraining"""
        try:
            message = f"🔄 **AUTO-RETRAIN TRIGGERED** 🔄\n- **Symbol:** {symbol}\n- **Reason:** {reason}\n- **Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            self.bot.send_discord_alert(message)
        except Exception as e:
            print(f"❌ [Auto-Retrain] Failed to send notification: {e}")
    
    def update_performance(self, symbol, trade_result):
        """Update performance history for a symbol"""
        if symbol not in self.performance_history:
            self.performance_history[symbol] = []
        
        # Add trade result (1 for profit, 0 for loss)
        result = 1 if trade_result > 0 else 0
        self.performance_history[symbol].append(result)
        
        # Keep only last 50 trades
        if len(self.performance_history[symbol]) > 50:
            self.performance_history[symbol] = self.performance_history[symbol][-50:]
        
        # Update consecutive losses counter
        if result == 0:  # Loss
            self.consecutive_losses[symbol] = self.consecutive_losses.get(symbol, 0) + 1
        else:  # Profit
            self.consecutive_losses[symbol] = 0
    
    def check_scheduled_retrain(self):
        """Check if scheduled retraining is needed"""
        if not self.config["RETRAIN_SCHEDULE"]["DAILY_RETRAIN_CHECK"]:
            return
        
        current_time = datetime.now()
        retrain_time = self.config["RETRAIN_SCHEDULE"]["RETRAIN_TIME"]
        
        # Check if it's time for daily retrain check
        if current_time.strftime("%H:%M") == retrain_time:
            for symbol in self.bot.active_symbols:
                should_retrain, reason = self.check_retrain_triggers(symbol)
                if should_retrain:
                    self.trigger_retrain(symbol, reason)

class NewsProvider(ABC):
    def __init__(self, api_key):
        self.api_key = api_key
        if not self.api_key or self.api_key == "DEMO_KEY" or len(self.api_key) < 10:
            self.enabled = False
        else:
            self.enabled = True

    @abstractmethod
    async def fetch_news(self, session, symbol: str, stock_map: dict):
        """
        Get news for a specific symbol. Must return a list of dictionaries
        in standardized format.
        """
        pass

    def _standardize_news(self, source: str, title: str, summary: str, url: str, published_at: datetime):
        """Helper function to create news dictionary in common format."""
        return {
            "source": source,
            "title": title,
            "summary": summary,
            "url": url,
            "published_at": published_at
        }

class FinnhubProvider(NewsProvider):
    async def fetch_news(self, session, symbol: str, stock_map: dict):
        if not self.enabled: return []
        stock_symbol = stock_map.get(symbol, symbol)
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
        url = f"https://finnhub.io/api/v1/company-newssymbol={stock_symbol}&from={start_date}&to={end_date}&token={self.api_key}"
        
        start_time = time.time()
        try:
            async with session.get(url, timeout=10) as response:
                response_time = time.time() - start_time
                
                if response.status != 200: 
                    # Log API call with error
                    if hasattr(self, 'api_monitor'):
                        self.api_monitor.log_api_call("finnhub", success=False, 
                                                    response_time=response_time, 
                                                    error_msg=f"HTTP {response.status}")
                    return []
                
                news_items = await response.json()
                
                # Log successful API call
                if hasattr(self, 'api_monitor'):
                    self.api_monitor.log_api_call("finnhub", success=True, response_time=response_time)
                
                return [self._standardize_news(
                    source="Finnhub", title=item.get('headline'), summary=item.get('summary'),
                    url=item.get('url'), published_at=datetime.fromtimestamp(item.get('datetime'))
                ) for item in news_items[:10] if item.get('headline')]
        except Exception as e:
            response_time = time.time() - start_time
            # Log API call with error
            if hasattr(self, 'api_monitor'):
                self.api_monitor.log_api_call("finnhub", success=False, 
                                            response_time=response_time, 
                                            error_msg=str(e))
            logging.error(f"Finnhub error: {e}")
            return []

# TM V THAY THTON BL P this

class EODHDProvider(NewsProvider):
    def __init__(self, api_key):
        super().__init__(api_key)
        if self.enabled:
            try:
                # <<< THAY  I Y: used eod thay v eodhd >>>
                self.client = eod.EodHistoricalData(api_key)
            except Exception as e:
                logging.error(f"EODH client initialization error: {e}")
                self.enabled = False

    async def fetch_news(self, session, symbol: str, stock_map: dict):
        if not self.enabled: return []
        stock_symbol = stock_map.get(symbol, symbol)

        # EODHD không cung cấp tin tức qua API này, mà là data cơ bản
        # Giữ nguyên logic cũ để lấy data cơ bản
        try:
            loop = asyncio.get_running_loop()
            # Using a function more suitable if available, or use get_fundamental_equity
            fundamental_data = await loop.run_in_executor(
                None,
                self.client.get_fundamental_equity,
                stock_symbol
            )

            market_cap = fundamental_data.get('Highlights', {}).get('MarketCapitalization', 'N/A')
            pe_ratio = fundamental_data.get('Highlights', {}).get('PERatio', 'N/A')
            beta = fundamental_data.get('Technicals', {}).get('Beta', 'N/A')
            summary = f"Vốn hóa: {market_cap}, P/E: {pe_ratio}, Beta: {beta}"

            return [self._standardize_news(
                source="EODHD", title=f"Data cơ bản cho {stock_symbol}",
                summary=summary, url="", published_at=datetime.now()
            )]
        except Exception as e:
            # Lỗi này thường xảy ra khi gói Free không hợp lệ (ví dụ Forex)
            # print(f"ℹ️ Note EODHD: {e}") # Commented for debug if needed
            return []

class MarketauxProvider(NewsProvider):
    async def fetch_news(self, session, symbol: str, stock_map: dict):
        if not self.enabled: return []
        stock_symbol = stock_map.get(symbol, symbol)
        url = f"https://api.marketaux.com/v1/news/all symbols={stock_symbol}&filter_entities=true&limit=5&api_token={self.api_key}"
        try:
            async with session.get(url, timeout=10) as response:
                if response.status != 200: return []
                news_data = await response.json()
                news_items = news_data.get('data', [])
                return [self._standardize_news(
                    source="Marketaux", title=item.get('title'), summary=item.get('snippet'),
                    url=item.get('url'), published_at=pd.to_datetime(item.get('published_at')).tz_localize(None).to_pydatetime()
                ) for item in news_items if item.get('title')]
        except Exception as e:
            logging.error(f"Marketaux error: {e}")
            return []
# DN TON BL P this VO FILE of B N

class NewsApiOrgProvider(NewsProvider):
    def __init__(self, api_key):
        super().__init__(api_key)
        if self.enabled:
            try:
                # This library doesn't support async, so we'll initialize client here
                self.client = NewsApiClient(api_key=api_key)
            except Exception as e:
                logging.error(f"NewsApiClient initialization error: {e}")
                self.enabled = False

    async def fetch_news(self, session, symbol: str, stock_map: dict):
        if not self.enabled: return []

        # L y query suitable, v dSPX500 th tm "S&P 500"
        query = stock_map.get(symbol, symbol)

        try:
            loop = asyncio.get_running_loop()

            # Chạy Function get_everything (vì nó là Function sử dụng blocking) trong một executor riêng
            # để không làm block vòng lặp async chính
            news_data = await loop.run_in_executor(
                None,
                lambda: self.client.get_everything(q=query, language='en', sort_by='publishedAt', page_size=10)
            )

            news_items = news_data.get('articles', [])

            # Chu n ha k t qutrv 
            return [self._standardize_news(
                source="NewsAPI.org",
                title=item.get('title'),
                summary=item.get('description'),
                url=item.get('url'),
                published_at=pd.to_datetime(item.get('publishedAt')).tz_localize(None).to_pydatetime()
            ) for item in news_items if item.get('title')]

        except Exception as e:
            logging.error(f"NewsAPI.org error: {e}")
            return []
# L p this not thay d i
# === ENHANCED NEWS FILTER ===
class EnhancedNewsFilter:
    """Enhanced news filter with mandatory integration into trading flow"""

    def __init__(self):
        self.high_impact_events = []
        self.filter_buffer_hours = 2  # 2 hours buffer
        self.blocked_symbols = set()

    def check_news_filter(self, symbol, current_time=None):
        """Enhanced news filter with mandatory integration"""
        if current_time is None:
            current_time = datetime.now()

        # Get upcoming high-impact events
        upcoming_events = self._get_upcoming_high_impact_events(symbol, current_time)

        if upcoming_events:
            # Check if any event is within buffer period
            for event in upcoming_events:
                event_time = event.get("time")
                time_diff = abs((event_time - current_time).total_seconds() / 3600)

                if time_diff <= self.filter_buffer_hours:
                    self.blocked_symbols.add(symbol)
                    return {
                        "block_trading": True,
                        "reason": f"High-impact event: {event.get('title')}",
                        "event_time": event_time,
                        "buffer_hours": self.filter_buffer_hours,
                        "time_diff_hours": time_diff
                    }

        # Remove from blocked symbols if no events
        if symbol in self.blocked_symbols:
            self.blocked_symbols.remove(symbol)

        return {"block_trading": False, "reason": "No high-impact events"}

    def _get_upcoming_high_impact_events(self, symbol, current_time):
        """Get upcoming high-impact events for symbol"""
        # This should integrate with existing NewsEconomicManager
        # For now, return empty list - will be implemented with actual newfixPI
        return []

    def is_symbol_blocked(self, symbol):
        """Check if symbol is currently blocked by news filter"""
        return symbol in self.blocked_symbols

class NewsEconomicManager:
    def __init__(self):
        print("🔄 [NewsEconomicManager] Starting initialization...")
        
        try:
            # Initialize caching attributes
            self.last_calendar_fetch_date = None
            self.economic_calendar_cache = None

            # Enhanced news cache with multiple timeout levels
            self.news_cache = {}
            self.news_cache_timeout = 1800  # 30 minutes for regular news
            self.high_impact_cache_timeout = 300  # 5 minutes for high-impact events
            self.cache_hit_count = {}
            self.cache_miss_count = {}
            print("[NewsEconomicManager] Cache attributes initialized")
            
            # Initialize news file storage system
            self.news_storage_dir = "/workspace/news_data"
            self.daily_news_file = None
            self.news_file_lock = threading.Lock()  # Use proper thread-safe lock
            self._setup_news_storage()
            print("✅ [NewsEconomicManager] News file storage system initialized")
        
            # Initialize news quality scorer
            print("🔄 [NewsEconomicManager] Initializing NewsQualityScorer...")
            self.quality_scorer = NewsQualityScorer()
            print("✅ [NewsEconomicManager] NewsQualityScorer initialized")
        
            # Initialize LLM analyzer with better error handling
            print("🔄 [NewsEconomicManager] Initializing LLM Analyzer...")
            api_key = os.getenv("GOOGLE_AI_API_KEY", "AIzaSyBCexoODvgrN2QRG8_iKv3p5VTJ5jaJ_B0")
            if not api_key or "DEFAULT_API_KEY" in api_key or len(api_key) < 10:
                print("⚠️ [NewsEconomicManager] Google AI API Key not configured properly")
                print("   - Set GOOGLE_AI_API_KEY environment variable")
                print("   - Master Agent will auto-approve all trades")
                self.llm_analyzer = None
            else:
                print(f"[NewsEconomicManager] Google AI API Key found (length: {len(api_key)})")
                try:
                    self.llm_analyzer = LLMSentimentAnalyzer(api_key)
                    print("[NewsEconomicManager] LLMSentimentAnalyzer initialized")
                except Exception as e:
                    print(f"[NewsEconomicManager] Failed to initialize LLMSentimentAnalyzer: {e}")
                    self.llm_analyzer = None
        except Exception as e:
            print(f"[NewsEconomicManager] Error in __init__: {e}")
            import traceback
            traceback.print_exc()
            raise e
        
        try:
            # Test Discord webhook connectivity on startup (silent test - no message sent)
            print("🔄 [Discord] Testing webhook connectivity...")
            try:
                # Silent test - just check connectivity without sending message
                test_payload = {
                    "content": "🔧 Discord webhook test - Bot is working!",  # Non-empty test message
                    "username": "Trading Bot"
                }
                response = requests.post(DISCORD_WEBHOOK, json=test_payload, timeout=10)
                
                print(f"📊 [Discord] Response status: {response.status_code}")
                print(f"📊 [Discord] Response headers: {dict(response.headers)}")
                
                if response.status_code == 200:
                    print("[Discord] Webhook test successful")
                elif response.status_code == 204:
                    print("[Discord] Webhook test successful (204 - No Content)")
                    print("   - Discord webhook 204 means 'No Content' but connection is working")
                elif response.status_code == 401:
                    print("[Discord] Webhook unauthorized - check webhook URL")
                elif response.status_code == 404:
                    print("[Discord] Webhook not found - webhook may be deleted")
                elif response.status_code == 429:
                    print("⚠️ [Discord] Rate limited - too many requests")
                else:
                    print(f"[Discord] Webhook test failed: {response.status_code}")
                    print(f"   Response text: {response.text[:200]}")
                    
            except Exception as e:
                print(f"[Discord] Webhook test error: {e}")
                print("   - Check internet connection")
                print("   - Verify webhook URL format")
        
            # Initialize API monitoring
            print("🔄 [NewsEconomicManager] Initializing API monitoring...")
            self.api_monitor = APIMonitoringSystem()
            print("[NewsEconomicManager] API monitoring initialized")
        
            # Initialize news providers
            print("🔄 [NewsEconomicManager] Initializing news providers...")
            self.news_providers = []
            self.stock_map = {}
            
            # Initialize news providers with API keys
            self._initialize_news_providers()
            print("✅ [NewsEconomicManager] News providers initialized")
        
            # API rate limiting
            self.api_call_times = {}
            self.api_rate_limits = {
            "finnhub": 60,  # calls per minute
            "marketaux": 100,
            "newsapi": 1000,
            "eodhd": 20
        }
            print("✅ [NewsEconomicManager] API rate limiting configured")
            
        except Exception as e:
            print(f"❌ [NewsEconomicManager] Error in second part of __init__: {e}")
            import traceback
            traceback.print_exc()
            raise e
    
    def _initialize_news_providers(self):
        """Initialize all news providers with their API keys"""
        print("🔄 [NewsEconomicManager] Initializing news providers...")
        
        # Using API keys đã configuration in Bot
        finnhub_key = API_KEYS.get('FINHUB', 'd1b3ichr01qjhvtsbj8g')
        marketaux_key = API_KEYS.get('MARKETAUX', 'CkuQmx9sPsjw0FRDeSkoO8U3O9Jj3HWnUYMJNEql')
        newsapi_key = API_KEYS.get('NEWSAPI', 'abd8f43b808f42fdb8d28fb1c429af72')
        eodhd_key = API_KEYS.get('EODHD', '68bafd7d44a7f0.25202650')
        
        print(f"🔑 [NewsEconomicManager] API Keys loaded:")
        print(f"   - Finnhub: {finnhub_key[:10]}...")
        print(f"   - Marketaux: {marketaux_key[:10]}...")
        print(f"   - NewsAPI: {newsapi_key[:10]}...")
        print(f"   - EODHD: {eodhd_key[:10]}...")
        
        # Initialize providers
        providers = [
            ("Finnhub", FinnhubProvider(finnhub_key)),
            ("Marketaux", MarketauxProvider(marketaux_key)),
            ("NewsAPI.org", NewsApiOrgProvider(newsapi_key)),
            ("EODHD", EODHDProvider(eodhd_key))
        ]
        
        enabled_count = 0
        for name, provider in providers:
            if provider.enabled:
                self.news_providers.append(provider)
                enabled_count += 1
                print(f"✅ [NewsEconomicManager] {name} provider enabled (API key: {provider.api_key[:10]}...)")
            else:
                print(f"⚠️ [NewsEconomicManager] {name} provider disabled")
                print(f"   - API key: {provider.api_key[:20]}...")
                print(f"   - Key length: {len(provider.api_key)}")
                print(f"   - Reason: {'Empty key' if not provider.api_key else 'Invalid key format' if len(provider.api_key) < 10 else 'Unknown'}")
        
        print(f"✅ [NewsEconomicManager] {enabled_count}/{len(providers)} news providers enabled")
        
        # Test API connectivity if c providers enabled
        if enabled_count > 0:
            print("🔄 [NewsEconomicManager] Testing API connectivity...")
            self._test_api_connectivity()
        
        # Initialize stock mapping for better symbol matching
        self.stock_map = {
            "SPX500": "SPY",  # S&P 500
            "EURUSD": "EURUSD=X",  # Forex
            "GBPUSD": "GBPUSD=X",
            "USDJPY": "USDJPY=X", 
            "XAUUSD": "GC=F",  # Gold futures
            "BTCUSD": "BTC-USD",  # Bitcoin
            "ETHUSD": "ETH-USD",  # Ethereum
            "AAPL": "AAPL",
            "GOOGL": "GOOGL",
            "MSFT": "MSFT",
            "TSLA": "TSLA"
        }
    
    def _test_api_connectivity(self):
        """Test API connectivity for enabled providers"""
        try:
            # Test Finhub API
            if any(isinstance(p, FinnhubProvider) and p.enabled for p in self.news_providers):
                test_url = f"https://finhub.io/api/v1/quotesymbol=AAPL&token={API_KEYS['FINHUB']}"
                response = requests.get(test_url, timeout=5)
                if response.status_code == 200:
                    print("✅ [API Test] Finnhub API: Connected")
                else:
                    print(f"📊 [API Test] Finnhub API: HTTP {response.status_code}")
            
            # Test Marketaux API
            if any(isinstance(p, MarketauxProvider) and p.enabled for p in self.news_providers):
                test_url = f"https://api.marketaux.com/v1/news/allsymbols=AAPL&api_token={API_KEYS['MARKETAUX']}"
                response = requests.get(test_url, timeout=5)
                if response.status_code == 200:
                    print("✅ [API Test] Marketaux API: Connected")
                else:
                    print(f"📊 [API Test] Marketaux API: HTTP {response.status_code}")
            
            # Test NewsAPI
            if any(isinstance(p, NewsApiOrgProvider) and p.enabled for p in self.news_providers):
                test_url = f"https://newsapi.org/v2/top-headlinescountry=us&apiKey={API_KEYS['NEWSAPI']}"
                response = requests.get(test_url, timeout=5)
                if response.status_code == 200:
                    print("✅ [API Test] NewsAPI: Connected")
                else:
                    print(f"📊 [API Test] NewsAPI: HTTP {response.status_code}")
            
            # Test EODHD API
            if any(isinstance(p, EODHDProvider) and p.enabled for p in self.news_providers):
                test_url = f"https://eodhistoricaldata.com/api/eod/AAPL.USapi_token={API_KEYS['EODHD']}&fmt=json"
                response = requests.get(test_url, timeout=5)
                if response.status_code == 200:
                    print("✅ [API Test] EODHD API: Connected")
                else:
                    print(f"📊 [API Test] EODHD API: HTTP {response.status_code}")
                    
        except Exception as e:
            print(f" [API Test] Error testing API connectivity: {e}")
    
    def get_economic_calendar(self, init_date=None, end_date=None):
        """
        Lấy lịch kinh tế từ Trading Economics.
        IMPORTANT: Integrate caching to call API once per day.
        """
        # <<< LOGIC CACHING BẮT ĐẦU TẠI ĐÂY >>>
        today = datetime.utcnow().date()
        # if cache đã có và đã lấy trong ngày hôm nay, sử dụng lại cache
        if self.last_calendar_fetch_date == today and self.economic_calendar_cache:
            # print("   [Cache] Using lịch kinh tế từ cache.")
            return self.economic_calendar_cache
        # <<< KẾT THÚC LOGIC CACHING >>>

        try:
            # if không có cache, tiến hành gọi API như bình thường
            logging.info("   [API Call] Fetching new economic calendar for today...")
            today_utc = datetime.utcnow()
            if init_date is None:
                init_date = (today_utc - timedelta(days=2)).strftime('%Y-%m-%d')
            if end_date is None:
                end_date = (today_utc + timedelta(days=2)).strftime('%Y-%m-%d')

            # Check if Trading Economics is available
            try:
                trading_economics_available = globals().get('TRADING_ECONOMICS_AVAILABLE', False)
            except:
                trading_economics_available = False
                
            if not trading_economics_available or not te:
                logging.info("Trading Economics not available - returning empty calendar")
                return []

            # Try to get calendar data with additional error handling
            try:
                calendar_raw = te.getCalendarData(initDate=init_date, endDate=end_date)
            except AttributeError as attr_error:
                if "apikey" in str(attr_error):
                    logging.warning("⚠️ Trading Economics API key not properly set - returning empty calendar")
                    return []
                else:
                    raise attr_error
            except Exception as api_error:
                error_msg = str(api_error)
                if "403" in error_msg or "Forbidden" in error_msg:
                    logging.warning("⚠️ Trading Economic API access forbidden (403) - API key may be invalid or rate limited")
                    logging.warning("⚠️ Bot will continue without Trading Economics data")
                    # Disable Trading Economics for this session to avoid repeated errors
                    try:
                        globals()['TRADING_ECONOMICS_AVAILABLE'] = False
                        # Also disable the module to prevent further calls
                        if hasattr(te, 'login'):
                            te.login = lambda x: None  # Disable login function
                    except:
                        pass
                    return []
                elif "401" in error_msg or "Unauthorized" in error_msg:
                    logging.warning("⚠️ Trading Economics API unauthorized (401) - API key may be invalid")
                    return []
                elif "429" in error_msg or "Too Many Requests" in error_msg:
                    logging.warning("⚠️ Trading Economic API rate limited (429) - too many requests")
                    return []
                else:
                    logging.warning(f"⚠️ Trading Economics API error: {api_error}")
                    return []
            if not isinstance(calendar_raw, list):
                return []

            high_impact_keywords = ["NFP", "CPI", "FOMC", "Interest Rate", "GDP"]
            filtered_events = []
            for event in calendar_raw:
                importance = str(event.get("Importance", "")).lower()
                event_name = str(event.get("Event", "")).lower()
                is_high_importance = (importance == "high")
                is_keyword_match = any(keyword.lower() in event_name for keyword in high_impact_keywords)

                if is_high_importance or is_keyword_match:
                    filtered_events.append(event)

            # <<< LUU K T QUVO CACHE SAU KHI G I API THNH CNG >>>
            self.economic_calendar_cache = filtered_events
            self.last_calendar_fetch_date = today
            # <<< K T THC LUU CACHE >>>

            return filtered_events
        except Exception as e:
            logging.error(f"Error fetching economic calendar from Trading Economics: {e}")
            # If error, return old cache (if available) instead of empty list
            return self.economic_calendar_cache if self.economic_calendar_cache else []
    async def get_aggregated_news(self, symbol: str):
        """
        Get news from ALL providerfixsynchronously,
        aggregate and remove duplicates.
        """
        # Check cache first
        cached_data = self.news_cache.get(symbol)
        if cached_data:
            # Fix timezone issue: ensure both datetimes have same timezone awareness
            current_time = datetime.now()
            cache_timestamp = cached_data['timestamp']
            if cache_timestamp.tzinfo is not None:
                # If cache_timestamp is timezone-aware, make current_time aware too
                import pytz
                current_time = current_time.replace(tzinfo=pytz.UTC)
            elif current_time.tzinfo is not None:
                # If current_time is timezone-aware but cache_timestamp is not, make cache_timestamp aware
                import pytz
                cache_timestamp = cache_timestamp.replace(tzinfo=pytz.UTC)
            
            if (current_time - cache_timestamp).total_seconds() < NEWS_CACHE_TIMEOUT:
                return cached_data['news']

        logging.info(f"📰 Starting news aggregation for {symbol} from multiple sources...")
        async with aiohttp.ClientSession() as session:
            tasks = [provider.fetch_news(session, symbol, self.stock_map) for provider in self.news_providers if provider.enabled]
            results = await asyncio.gather(*tasks, return_exceptions=True)

        all_news = []
        for res in results:
            if isinstance(res, list):
                all_news.extend(res)

        # type ball bi bo trng l p d a trn tiu d 
        unique_news = list({item['title'].lower(): item for item in all_news}.values())

        # S p x p tin t fromheo th i gian mi nh t
        unique_news.sort(key=lambda x: x['published_at'], reverse=True)

        print(f" Summary completed, found {len(unique_news)} news articles for {symbol}.")

        # Luu vo cache
        self.news_cache[symbol] = {'news': unique_news, 'timestamp': datetime.now()}

        # Save news to file system
        self._save_news_to_file(symbol, unique_news)
        
        return unique_news
    
    def _setup_news_storage(self):
        """Setup news storage directory and file system"""
        try:
            # Create news storage directory
            if not os.path.exists(self.news_storage_dir):
                os.makedirs(self.news_storage_dir, exist_ok=True)
                print(f"📁 [News Storage] Created directory: {self.news_storage_dir}")
            
            # Create today's news file
            today = datetime.now().strftime("%Y-%m-%d")
            self.daily_news_file = os.path.join(self.news_storage_dir, f"news_{today}.json")
            
            # Initialize daily news file if it doesn't exist or is corrupted
            if not os.path.exists(self.daily_news_file):
                self._create_fresh_news_file()
            else:
                # Validate existing file
                try:
                    with open(self.daily_news_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if not content:
                            raise json.JSONDecodeError("Empty file", "", 0)
                        data = json.loads(content)
                        if not isinstance(data, dict) or "symbols" not in data or "summary" not in data:
                            raise ValueError("Invalid structure")
                except (json.JSONDecodeError, ValueError) as e:
                    print(f"⚠️ [News Storage] Existing file corrupted ({e}), recreating...")
                    import time
                    backup_file = f"{self.daily_news_file}.corrupted_{int(time.time())}"
                    shutil.copy2(self.daily_news_file, backup_file)
                    self._create_fresh_news_file()
            
            # Create news index file
            self.news_index_file = os.path.join(self.news_storage_dir, "news_index.json")
            self._update_news_index()
            
        except Exception as e:
            print(f"❌ [News Storage] Error setting up storage: {e}")
    
    def _save_news_to_file(self, symbol, news_items):
        """Save news items to daily file with atomic writes and proper error handling"""
        import tempfile
        import shutil
        
        with self.news_file_lock:  # Use context manager for thread-safe locking
            try:
                self._save_news_to_file_atomic(symbol, news_items)
            except Exception as e:
                print(f"❌ [News Storage] Error saving news for {symbol}: {e}")
                print(f"📋 [News Storage] Attempting recovery...")
                self._recover_corrupted_news_file(symbol, news_items)
    
    def _save_news_to_file_atomic(self, symbol, news_items):
        """Atomic save with validation"""
        import tempfile
        import shutil
        
        try:
            # Check if we need to create a new daily file
            today = datetime.now().strftime("%Y-%m-%d")
            expected_file = os.path.join(self.news_storage_dir, f"news_{today}.json")
            
            if expected_file != self.daily_news_file:
                self.daily_news_file = expected_file
                self._setup_news_storage()
            
            # Load existing data with robust error handling
            data = self._load_news_data_safely()
            
            # Add/update symbol news
            if symbol not in data["symbols"]:
                data["symbols"][symbol] = {
                    "news_items": [],
                    "last_updated": datetime.now().isoformat(),
                    "total_items": 0
                }
            
            # Update news items (avoid duplicates)
            existing_titles = {item.get('title', '') for item in data["symbols"][symbol]["news_items"]}
            new_items = [item for item in news_items if item.get('title', '') not in existing_titles]
            
            if new_items:
                data["symbols"][symbol]["news_items"].extend(new_items)
                data["symbols"][symbol]["last_updated"] = datetime.now().isoformat()
                data["symbols"][symbol]["total_items"] = len(data["symbols"][symbol]["news_items"])
                
                # Update summary
                data["summary"]["total_news"] = sum(
                    symbol_data["total_items"] for symbol_data in data["symbols"].values()
                )
                data["summary"]["symbols_covered"] = len(data["symbols"])
                data["summary"]["last_updated"] = datetime.now().isoformat()
                
                # Atomic save with validation
                self._atomic_json_write(self.daily_news_file, data)
                
                print(f"💾 [News Storage] Saved {len(new_items)} new items for {symbol}")
                
                # Update index
                self._update_news_index()
        
        except Exception as e:
            raise e  # Re-raise for parent handler
    
    def _load_news_data_safely(self):
        """Safely load news data with error recovery"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if not os.path.exists(self.daily_news_file):
                    self._setup_news_storage()
                
                with open(self.daily_news_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if not content:
                        raise json.JSONDecodeError("Empty file", "", 0)
                    data = json.loads(content)
                    
                # Validate data structure
                if not isinstance(data, dict) or "symbols" not in data or "summary" not in data:
                    raise ValueError("Invalid news data structure")
                
                return data
                
            except (json.JSONDecodeError, FileNotFoundError, ValueError) as e:
                print(f"⚠️ [News Storage] Attempt {attempt + 1}/{max_retries} - Error loading {self.daily_news_file}: {e}")
                if attempt < max_retries - 1:
                    print(f"🔧 [News Storage] Recreating file with fresh structure...")
                    self._setup_news_storage()
                else:
                    print(f"❌ [News Storage] Failed to load after {max_retries} attempts, creating fresh file")
                    self._create_fresh_news_file()
                    return self._load_news_data_safely()
    
    def _atomic_json_write(self, target_file, data):
        """Write JSON data atomically to prevent corruption"""
        import tempfile
        import shutil
        
        # Create temporary file in the same directory
        temp_dir = os.path.dirname(target_file)
        with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', 
                                       dir=temp_dir, delete=False, suffix='.tmp') as temp_file:
            try:
                # Validate data can be serialized
                json_str = json.dumps(data, indent=2, ensure_ascii=False, default=self._json_serializer)
                
                # Validate JSON is parseable
                json.loads(json_str)
                
                # Write to temp file
                temp_file.write(json_str)
                temp_file.flush()
                os.fsync(temp_file.fileno())  # Force write to disk
                
                temp_filename = temp_file.name
            
            except Exception as e:
                os.unlink(temp_file.name)  # Clean up temp file
                raise ValueError(f"JSON serialization failed: {e}")
        
        # Atomic move
        try:
            shutil.move(temp_filename, target_file)
        except Exception as e:
            if os.path.exists(temp_filename):
                os.unlink(temp_filename)
            raise IOError(f"Atomic move failed: {e}")
    
    def _create_fresh_news_file(self):
        """Create a fresh news file with proper structure"""
        today = datetime.now().strftime("%Y-%m-%d")
        initial_data = {
            "date": today,
            "created_at": datetime.now().isoformat(),
            "symbols": {},
            "summary": {
                "total_news": 0,
                "symbols_covered": 0,
                "last_updated": datetime.now().isoformat()
            }
        }
        self._atomic_json_write(self.daily_news_file, initial_data)
        print(f"📝 [News Storage] Created fresh daily news file: {self.daily_news_file}")
    
    def _recover_corrupted_news_file(self, symbol, news_items):
        """Recover from corrupted news file"""
        try:
            print(f"🔧 [News Storage] Attempting to recover corrupted file for {symbol}")
            
            # Try to backup corrupted file
            import time
            backup_file = f"{self.daily_news_file}.corrupted_{int(time.time())}"
            if os.path.exists(self.daily_news_file):
                shutil.copy2(self.daily_news_file, backup_file)
                print(f"📋 [News Storage] Backed up corrupted file to: {backup_file}")
            
            # Create fresh file
            self._create_fresh_news_file()
            
            # Try to save the news items again
            self._save_news_to_file_atomic(symbol, news_items)
            print(f"✅ [News Storage] Successfully recovered and saved news for {symbol}")
            
        except Exception as recovery_error:
            print(f"❌ [News Storage] Recovery failed: {recovery_error}")
            print(f"🆘 [News Storage] Manual intervention may be required for {self.daily_news_file}")
    
    def _json_serializer(self, obj):
        """Custom JSON serializer to handle pandas Timestamp and datetime objects"""
        if hasattr(obj, 'isoformat'):
            return obj.isoformat()
        elif hasattr(obj, 'to_pydatetime'):
            return obj.to_pydatetime().isoformat()
        raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
    
    def _update_news_index(self):
        """Update the news index file"""
        try:
            index_data = {
                "last_updated": datetime.now().isoformat(),
                "available_dates": [],
                "total_files": 0,
                "symbols_tracked": set()
            }
            
            # Scan news directory for files
            if os.path.exists(self.news_storage_dir):
                for filename in os.listdir(self.news_storage_dir):
                    if filename.startswith("news_") and filename.endswith(".json"):
                        date_str = filename.replace("news_", "").replace(".json", "")
                        index_data["available_dates"].append(date_str)
                        
                        # Load file to get symbols
                        try:
                            file_path = os.path.join(self.news_storage_dir, filename)
                            with open(file_path, 'r', encoding='utf-8') as f:
                                file_data = json.load(f)
                                index_data["symbols_tracked"].update(file_data.get("symbols", {}).keys())
                        except Exception:
                            pass
            
            index_data["available_dates"].sort(reverse=True)
            index_data["total_files"] = len(index_data["available_dates"])
            index_data["symbols_tracked"] = sorted(list(index_data["symbols_tracked"]))
            
            # Save index atomically
            self._atomic_json_write(self.news_index_file, index_data)
                
        except Exception as e:
            print(f"❌ [News Storage] Error updating index: {e}")
    
    def get_news_from_file(self, symbol=None, date=None):
        """
        Get news from stored files - Bot can use this to read news
        """
        try:
            # Use today's date if not specified
            if date is None:
                date = datetime.now().strftime("%Y-%m-%d")
            
            news_file = os.path.join(self.news_storage_dir, f"news_{date}.json")
            
            if not os.path.exists(news_file):
                print(f"📰 [News Reader] No news file found for {date}")
                return None
            
            try:
                with open(news_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except json.JSONDecodeError as e:
                print(f"❌ [News Reader] Error parsing JSON in {news_file}: {e}")
                return None
            
            if symbol is None:
                # Return all news for the date
                print(f"📖 [News Reader] Retrieved all news for {date}: {data['summary']['total_news']} items")
                return data
            else:
                # Return news for specific symbol
                symbol_news = data.get("symbols", {}).get(symbol)
                if symbol_news:
                    print(f"📖 [News Reader] Retrieved {symbol_news['total_items']} news items for {symbol} on {date}")
                    return symbol_news["news_items"]
                else:
                    print(f"📰 [News Reader] No news found for {symbol} on {date}")
                    return []
                    
        except Exception as e:
            print(f"❌ [News Reader] Error reading news from file: {e}")
            return None
    
    def get_latest_news_summary(self):
        """Get summary of latest news for bot consumption"""
        try:
            if not os.path.exists(self.news_index_file):
                return None
            
            try:
                with open(self.news_index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
            except json.JSONDecodeError as e:
                print(f"❌ [News Index] Error parsing JSON in {self.news_index_file}: {e}")
                print(f"🔧 [News Index] Recreating index file...")
                self._update_news_index()
                with open(self.news_index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
            
            if not index_data["available_dates"]:
                return None
            
            # Get latest date
            latest_date = index_data["available_dates"][0]
            latest_news = self.get_news_from_file(date=latest_date)
            
            if latest_news:
                summary = {
                    "date": latest_date,
                    "total_news": latest_news["summary"]["total_news"],
                    "symbols_covered": latest_news["summary"]["symbols_covered"],
                    "symbols": list(latest_news["symbols"].keys()),
                    "last_updated": latest_news["summary"]["last_updated"]
                }
                print(f"📊 [News Summary] Latest: {summary['total_news']} news items for {summary['symbols_covered']} symbols")
                return summary
            
            return None
            
        except Exception as e:
            print(f"❌ [News Summary] Error getting summary: {e}")
            return None

    def analyze_sentiment(self, news_items):
        if not news_items:
            return {"score": 0.0, "reasoning": "No newfixvailable."}
        return self.llm_analyzer.analyze_sentiment_of_news(news_items)

    def add_news_sentiment_features(self, df, symbol):
        """
        IMPORTANT: All news sentiment data is integrated into DataFrame.
        Phase 2: News Sentiment Features - needs RETRAIN MODEL
        """
        print(f" [NewsFeatures] Starting to add news sentiment features for {symbol}...")
        logging.info(f"   [Features] Starting to add news sentiment features for {symbol}...")
        
        try:
            # Initialize news sentiment features
            df["news_sentiment_score"] = 0.0
            df["news_sentiment_volume"] = 0
            df["news_quality_score"] = 0.0
            df["news_timing_score"] = 0.0
            df["news_sentiment_trend"] = 0.0
            df["news_impact_score"] = 0.0
            
            print(f"[NewsFeatures] Initialized news features columns for {symbol}")
            
            # Get news data for the time period
            start_date = df.index.min()
            end_date = df.index.max()
            
            print(f" [NewsFeatures] Processing period: {start_date} to {end_date}")
            
            # Simulate news sentiment data (in real implementation, this would fetcontainsctual news)
            # For now, we'll create synthetic features based on market conditions
            
            for i, (timestamp, row) in enumerate(df.iterrows()):
                try:
                    # Simulate news sentiment based on price movement
                    price_change = row.get('close', 0) - df.iloc[max(0, i-1)].get('close', row.get('close', 0))
                    
                    # Initialize sentiment_score with default value
                    sentiment_score = 0.0
                    
                    # News sentiment score (-1 to 1)
                    if abs(price_change) > 0:
                        sentiment_score = np.tanh(price_change / row.get('close', 1)) * 0.5
                        df.at[timestamp, "news_sentiment_score"] = sentiment_score
                    
                    # News volume (number of news items)
                    news_volume = max(0, int(np.random.poisson(3)))  # Simulate Poisson distribution
                    df.at[timestamp, "news_sentiment_volume"] = news_volume
                    
                    # News quality score (0 to 1)
                    quality_score = np.random.beta(2, 2)  # Beta distribution centered around 0.5
                    df.at[timestamp, "news_quality_score"] = quality_score
                    
                    # News timing score (recent news more important)
                    # Simplified timezone handling
                    current_time = datetime.now()
                    time_diff = (current_time - timestamp.replace(tzinfo=None)).total_seconds()
                    timing_score = max(0, 1 - (time_diff / (24 * 3600)))  # Decay over 24 hours
                    df.at[timestamp, "news_timing_score"] = timing_score
                    
                    # News sentiment trend (moving average of sentiment)
                    if i >= 5:  # Need at least 5 previous values
                        recent_sentiments = df.iloc[i-5:i]["news_sentiment_score"].mean()
                        df.at[timestamp, "news_sentiment_trend"] = recent_sentiments
                    
                    # News impact score (combination of quality and timing)
                    impact_score = quality_score * timing_score
                    df.at[timestamp, "news_impact_score"] = impact_score
        
                except Exception as e:
                    print(f" [NewsFeatures] Error processing row {i} for {symbol}: {e}")
                    continue
        
            print(f"[NewsFeatures] Successfully added news sentiment features for {symbol}")
            logging.info(f"   [Features] News sentiment feature added for {symbol}")
            return df
            
        except Exception as e:
            print(f"[NewsFeatures] Error adding news sentiment features for {symbol}: {e}")
            logging.error(f"Error adding news sentiment features for {symbol}: {e}")
            import traceback
            traceback.print_exc()
            
            # Return df with default values if error occurs
            df["news_sentiment_score"] = 0.0
            df["news_sentiment_volume"] = 0
            df["news_quality_score"] = 0.0
            df["news_timing_score"] = 0.0
            df["news_sentiment_trend"] = 0.0
            df["news_impact_score"] = 0.0
        return df

    def add_economic_event_features(self, df, symbol):
        """
        IMPORTANT: All economic data is integrated into DataFrame,
        để xử lý đúng dữ liệu cho từng loại tài sản.
        """
        logging.info(f"   [Features] Starting to add economic event features for {symbol}...")
        df["is_near_high_impact_event"] = 0
        
        # Fallback: Add basic economic event features without Trading Economics
        self._add_basic_economic_features(df, symbol)

        # L y l ch kinh tcho kho ng th i gian of DataFrame
        start_date = df.index.min().strftime("%Y-%m-%d")
        end_date = df.index.max().strftime("%Y-%m-%d")

        try:
            # Check if Trading Economics is available
            try:
                trading_economics_available = globals().get('TRADING_ECONOMICS_AVAILABLE', False)
            except:
                trading_economics_available = False
                
            if not trading_economics_available or not te:
                logging.info("Trading Economics not available - skipping economic event features")
                return df

            # Enhanced error handling for Trading Economics API
            try:
                calendar_raw = te.getCalendarData(initDate=start_date, endDate=end_date)
                if not isinstance(calendar_raw, list) or not calendar_raw:
                    logging.info("No economic calendar data available for the specified period")
                    return df
            except Exception as api_error:
                error_msg = str(api_error)
                if "403" in error_msg or "Forbidden" in error_msg:
                    logging.warning("⚠️ Trading Economic API access forbidden (403) - API key may be invalid or rate limited")
                    logging.warning("⚠️ Bot will continue without Trading Economics data")
                    # Note: Trading Economics API is not available, continuing without it
                    return df
                elif "401" in error_msg or "Unauthorized" in error_msg:
                    logging.warning("🔑 Trading Economics API unauthorized (401) - check API key")
                    return df
                else:
                    logging.warning(f"⚠️ Trading Economics API error: {error_msg}")
                    return df
        except Exception as e:
            logging.warning(f"   [Features]  Unexpected error accessing Trading Economics: {e}")
            return df

    def _add_basic_economic_features(self, df, symbol):
        """Add basic economic event featurefixs fallback when Trading Economics is unavailable"""
        try:
            # Add basic economic calendar features based on common patterns
            df["is_near_high_impact_event"] = 0
            
            # Add day-of-week effects (common economic events happen on specifihas dataays)
            df["is_friday"] = (df.index.dayofweek == 4).astype(int)
            df["is_monday"] = (df.index.dayofweek == 0).astype(int)
            
            # Add month-end effects (common for economihas dataata releases)
            df["is_month_end"] = (df.index.day >= 25).astype(int)
            
            # Add quarterly effects (Q1, Q2, Q3, Q4)
            df["quarter"] = df.index.quarter
            df["is_q1"] = (df.index.quarter == 1).astype(int)
            df["is_q2"] = (df.index.quarter == 2).astype(int)
            df["is_q3"] = (df.index.quarter == 3).astype(int)
            df["is_q4"] = (df.index.quarter == 4).astype(int)
            
            # Add holiday proximity effects (simplified)
            df["is_holiday_proximity"] = 0  # Placeholder for holiday effects
            
            # Ensure all required features exist
            required_features = ['is_q1', 'is_q2', 'is_q3', 'is_q4', 'is_month_end', 'is_holiday_proximity']
            for feature in required_features:
                if feature not in df.columns:
                    df[feature] = 0
                    logging.warning(f"Missing required feature '{feature}' for {symbol}, created with default value 0")
            
            logging.info(f"   [Features] Added basic economic features for {symbol} (fallback mode)")
            
        except Exception as e:
            logging.warning(f"   [Features] Error adding basic economic features: {e}")
        
        # Try to get Trading Economics data, but don't fail if unavailable
        try:
            # L y l ch kinh tcho kho ng th i gian of DataFrame
            start_date = df.index.min().strftime("%Y-%m-%d")
            end_date = df.index.max().strftime("%Y-%m-%d")

            # Check if Trading Economics is available
            try:
                trading_economics_available = globals().get('TRADING_ECONOMICS_AVAILABLE', False)
            except:
                trading_economics_available = False
                
            if trading_economics_available and te:
                # Enhanced error handling for Trading Economics API
                try:
                    calendar_raw = te.getCalendarData(initDate=start_date, endDate=end_date)
                    if not isinstance(calendar_raw, list) or not calendar_raw:
                        logging.info("No economic calendar data available for the specified period")
                        return df
                except Exception as api_error:
                    error_msg = str(api_error)
                    if "403" in error_msg or "Forbidden" in error_msg:
                        logging.warning(" Trading Economic API access forbidden (403) - using fallback features only")
                        # Note: Trading Economics API is not available, continuing without it
                        return df
                    elif "401" in error_msg or "Unauthorized" in error_msg:
                        logging.warning(" Trading Economics API unauthorized (401) - using fallback features only")
                        return df
                    else:
                        logging.warning(f" Trading Economics API error: {error_msg} - using fallback features only")
                        return df
            else:
                logging.info("Trading Economics not available - using fallback features only")
                return df

            # Chu n ha all s kin kinh ttTrading Economics
            high_impact_events = []
            high_impact_keywords = ["NFP", "CPI", "FOMC", "Interest Rate", "GDP"]
            for event in calendar_raw:
                try:
                    is_high_impact = event.get("Importance") == "High" or any(
                        keyword.lower() in event.get("Event", "").lower()
                        for keyword in high_impact_keywords
                    )
                    if is_high_impact:
                        event_time = pd.to_datetime(event["Date"]).tz_localize("UTC")
                        event_currency = event.get("currency", "").upper()
                        high_impact_events.append({"time": event_time, "currency": event_currency})
                except:
                    continue

            if not high_impact_events:
                return df

            # <<< LOGIC fix L I: Xhas data nh used money tlin quan >>>
            SYMBOL_oldRRENCY_MAP = {
                "SPX500": ["USD"], "DE40": ["EUR"], "JP225": ["JPY"],
                "XAUUSD": ["USD"], "ETHUSD": ["USD"]
            }
            relevant_currencies = []
            if symbol in SYMBOL_oldRRENCY_MAP:
                relevant_currencies = SYMBOL_oldRRENCY_MAP[symbol]
            elif len(symbol) >= 6:
                relevant_currencies.extend([symbol[:3].upper(), symbol[3:].upper()])
            # <<< K T THC fix L I >>>

            if not relevant_currencies:
                return df

            # Đánh dấu dữ liệu các hàng gần sự kiện
            df_utc_index = df.index.tz_convert("UTC")
            for event in high_impact_events:
                # Chprocessing if s kin lin quan dn symbol
                if event["currency"] in relevant_currencies:
                    event_start_window = event["time"] - timedelta(hours=2)
                    event_end_window = event["time"] + timedelta(hours=2)
                    mask = (df_utc_index >= event_start_window) & (df_utc_index <= event_end_window)
                    df.loc[mask, "is_near_high_impact_event"] = 1

            logging.info(f"   [Features] Economic event features processing completed. {df['is_near_high_impact_event'].sum()} candles marked.")
            return df
            
        except Exception as e:
            logging.warning(f"   [Features] Error processing economic events: {e}")
            return df


# === DAILY NEWS SCHEDULER ===
class DailyNewsScheduler:
    """
    Scheduler to fetch news every morning and maintain news files
    """
    
    def __init__(self, news_manager, symbols_to_track=None):
        self.news_manager = news_manager
        self.symbols_to_track = symbols_to_track or SYMBOLS  # Default to ALL symbols
        self.scheduler_active = False
        self.last_fetch_date = None
        self.fetch_times = [7, 8, 12, 16, 20]  # Hours to fetch news (UTC)
        self.scheduler_thread = None
        print(f"📅 [Daily News Scheduler] Initialized with {len(self.symbols_to_track)} symbols: {', '.join(self.symbols_to_track)}")
    
    def start_scheduler(self):
        """Start the daily news scheduling"""
        if self.scheduler_active:
            print("📅 [Daily News Scheduler] Already running")
            return
        
        self.scheduler_active = True
        self.scheduler_thread = threading.Thread(target=self._scheduler_loop, daemon=True)
        self.scheduler_thread.start()
        print("🚀 [Daily News Scheduler] Started - will fetch news at 07:00, 08:00, 12:00, 16:00, 20:00 UTC")
    
    def stop_scheduler(self):
        """Stop the daily news scheduling"""
        self.scheduler_active = False
        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=5)
        print("⏹️ [Daily News Scheduler] Stopped")
    
    def fetch_news_now(self):
        """Manually trigger news fetch for all symbols immediately"""
        print("🚀 [Daily News Scheduler] Manual news fetch triggered...")
        self._fetch_daily_news()
        return True
    
    def _scheduler_loop(self):
        """Main scheduler loop"""
        while self.scheduler_active:
            try:
                current_time = datetime.now()
                current_hour = current_time.hour
                current_date = current_time.strftime("%Y-%m-%d")
                
                # Check if it's time to fetch news
                should_fetch = (
                    current_hour in self.fetch_times and
                    self.last_fetch_date != f"{current_date}_{current_hour}"
                )
                
                if should_fetch:
                    print(f"⏰ [Daily News Scheduler] Time to fetch news: {current_time.strftime('%Y-%m-%d %H:%M')} UTC")
                    self._fetch_daily_news()
                    self.last_fetch_date = f"{current_date}_{current_hour}"
                
                # Sleep for 30 minutes before checking again
                time.sleep(1800)  # 30 minutes
                
            except Exception as e:
                print(f"❌ [Daily News Scheduler] Error in scheduler loop: {e}")
                time.sleep(300)  # Sleep 5 minutes on error
    
    def _fetch_daily_news(self):
        """Fetch news for all tracked symbols"""
        try:
            print("📰 [Daily News Scheduler] Starting daily news fetch...")
            fetch_start_time = datetime.now()
            
            total_news = 0
            successful_symbols = 0
            failed_symbols = []
            
            for symbol in self.symbols_to_track:
                try:
                    print(f"🔍 [Daily News Scheduler] Fetching news for {symbol}...")
                    
                    # Use asyncio to fetch news
                    news_items = asyncio.run(self.news_manager.get_aggregated_news(symbol))
                    
                    if news_items:
                        total_news += len(news_items)
                        successful_symbols += 1
                        print(f"✅ [Daily News Scheduler] {symbol}: {len(news_items)} news items")
                    else:
                        print(f"⚠️ [Daily News Scheduler] {symbol}: No news found")
                    
                    # Small delay between symbols to avoid rate limiting
                    time.sleep(2)
                    
                except Exception as e:
                    error_msg = f"Error fetching news for {symbol}: {str(e)}"
                    print(f"❌ [Daily News Scheduler] {error_msg}")
                    failed_symbols.append(symbol)
                    
                    # Log detailed error for debugging
                    import traceback
                    print(f"🔍 [Daily News Scheduler] Detailed error for {symbol}:")
                    print(f"    Error type: {type(e).__name__}")
                    print(f"    Error details: {str(e)}")
                    if hasattr(e, '__traceback__'):
                        tb_lines = traceback.format_tb(e.__traceback__)
                        for line in tb_lines[-2:]:  # Show last 2 traceback lines
                            print(f"    {line.strip()}")
            
            fetch_duration = (datetime.now() - fetch_start_time).total_seconds()
            
            # Create daily summary
            summary = {
                "timestamp": datetime.now().isoformat(),
                "total_symbols_attempted": len(self.symbols_to_track),
                "successful_symbols": successful_symbols,
                "failed_symbols": failed_symbols,
                "total_news_items": total_news,
                "fetch_duration_seconds": fetch_duration,
                "symbols_tracked": self.symbols_to_track
            }
            
            # Save daily summary
            self._save_daily_summary(summary)
            
            # Enhanced summary display matching user requirements
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M UTC")
            print("\n" + "="*50)
            print("📰 Daily News Update")
            print(f"News fetch completed at {current_time}")
            print("📊 Summary")
            print(f"Symbols: {successful_symbols}/{len(self.symbols_to_track)}")
            print(f"News Items: {total_news}")
            print(f"Duration: {fetch_duration:.1f}s")
            print("Trading Bot - Daily News Scheduler")
            print("="*50)
            
            if failed_symbols:
                print(f"⚠️ Failed symbols: {', '.join(failed_symbols)}")
            
            # Send Discord notification if configured
            self._send_daily_news_notification(summary)
            
        except Exception as e:
            print(f"❌ [Daily News Scheduler] Error in daily news fetch: {e}")
    
    def _save_daily_summary(self, summary):
        """Save daily fetch summary to file"""
        try:
            summary_dir = "/workspace/news_data/summaries"
            if not os.path.exists(summary_dir):
                os.makedirs(summary_dir, exist_ok=True)
            
            today = datetime.now().strftime("%Y-%m-%d")
            summary_file = os.path.join(summary_dir, f"daily_summary_{today}.json")
            
            # Load existing summaries if file exists
            summaries = []
            if os.path.exists(summary_file):
                try:
                    with open(summary_file, 'r', encoding='utf-8') as f:
                        summaries = json.load(f)
                except json.JSONDecodeError as e:
                    print(f"⚠️ [Daily News Scheduler] Error parsing summary file {summary_file}: {e}")
                    print(f"🔧 [Daily News Scheduler] Starting with empty summaries list...")
                    summaries = []
            
            # Add new summary
            summaries.append(summary)
            
            # Save updated summaries atomically
            self.news_manager._atomic_json_write(summary_file, summaries)
            
            print(f"💾 [Daily News Scheduler] Summary saved to {summary_file}")
            
        except Exception as e:
            print(f"❌ [Daily News Scheduler] Error saving summary: {e}")
    
    def _send_daily_news_notification(self, summary):
        """Send daily news notification to Discord"""
        try:
            if not DISCORD_WEBHOOK:
                return
            
            # Create notification message
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M UTC")
            
            embed = {
                "title": "📰 Daily News Update",
                "description": f"News fetch completed at {current_time}",
                "color": 0x00ff00 if summary['successful_symbols'] == summary['total_symbols_attempted'] else 0xffaa00,
                "fields": [
                    {
                        "name": "📊 Summary",
                        "value": f"**Symbols:** {summary['successful_symbols']}/{summary['total_symbols_attempted']}\n**News Items:** {summary['total_news_items']}\n**Duration:** {summary['fetch_duration_seconds']:.1f}s",
                        "inline": True
                    }
                ],
                "footer": {
                    "text": "Trading Bot - Daily News Scheduler"
                },
                "timestamp": datetime.now().isoformat()
            }
            
            if summary['failed_symbols']:
                embed["fields"].append({
                    "name": "⚠️ Failed Symbols",
                    "value": ", ".join(summary['failed_symbols']),
                    "inline": False
                })
            
            payload = {
                "embeds": [embed],
                "username": "Trading Bot News"
            }
            
            response = requests.post(DISCORD_WEBHOOK, json=payload, timeout=10)
            if response.status_code in [200, 204]:
                print("✅ [Daily News Scheduler] Discord notification sent")
            else:
                print(f"⚠️ [Daily News Scheduler] Discord notification failed: {response.status_code}")
                
        except Exception as e:
            print(f"❌ [Daily News Scheduler] Error sending notification: {e}")
    
    def force_fetch_now(self):
        """Force immediate news fetch for testing"""
        print("🚀 [Daily News Scheduler] Forcing immediate news fetch...")
        self._fetch_daily_news()
    
    def get_scheduler_status(self):
        """Get current scheduler status"""
        return {
            "active": self.scheduler_active,
            "symbols_tracked": len(self.symbols_to_track),
            "symbols": self.symbols_to_track,
            "fetch_times": self.fetch_times,
            "last_fetch_date": self.last_fetch_date,
            "next_fetch_hours": [h for h in self.fetch_times if h > datetime.now().hour] or [self.fetch_times[0]]
        }

    


class LLMSentimentAnalyzer:
    def __init__(self, api_key):
        # Use provided API key or fallback to default
        if not api_key or "DN_API_KEY" in api_key:
            api_key = "AIzaSyBCexoODvgrN2QRG8_iKv3p5VTJ5jaJ_B0"
            print(" [LLMSentimentAnalyzer] Using fallback API key")
        
        if not api_key or len(api_key) < 10:
            logging.warning(" Google AI API Key not configured. LLM features will be disabled.")
            self.model = None
            return

        try:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-1.5-flash-latest')
            logging.info("Successfully connected to Gemini API.")
            print("[LLMSentimentAnalyzer] Gemini API connection established")
        except Exception as e:
            logging.error(f"Error connecting to Gemini API: {e}")
            print(f"[LLMSentimentAnalyzer] Connection failed: {e}")
            self.model = None

    def analyze_sentiment_of_news(self, news_items: list):
        if not self.model:
            return {"score": 0.0, "reasoning": "LLM notoperational."}

        # 1. Định dạng lại tin tức để đưa vào prompt
        formatted_news = ""
        for i, item in enumerate(news_items[:5]): # Chỉ lấy 5 tin tức đầu tiên
            formatted_news += f"Tin {i+1}: {item.get('title', '')}\nNội dung: {item.get('summary', '')}\n\n"

        if not formatted_news:
            return {"score": 0.0, "reasoning": "No news available for analysis."}

        # 2. Thiết lập Prompt (câu lệnh) cho LLM
        prompt = f"""
        Bạn là một nhà phân tích tài chính chuyên nghiệp cho một quỹ phòng hộ.
        Please analyze the data and evaluate the market conditions.

        News:
        ---
        {formatted_news}
        ---

        Based on the received news, please provide an analysis in the following JSON format.
        Chỉ trả về duy nhất một khối JSON, không giải thích gì thêm.

        {{
          "sentiment_score": <a float from -1.0 (very negative) to 1.0 (very positive)>,
          "reasoning": "<mô tả ngắn gọn lý do cho điểm số của bạn>"
        }}
        """

        # 3. Gọi yêu cầu tới Gemini và xử lý kết quả 
        try:
            response = self.model.generate_content(prompt)
            # Trích xuất phần JSON từ text trả về 
            json_text = response.text.strip().replace('```json', '').replace('```', '')
            result = json.loads(json_text)

            # Đảm bảo all keys tồn tại
            result.setdefault('score', result.get('sentiment_score', 0.0))
            result.setdefault('reasoning', 'No reasoning provided.')

            return result

        except Exception as e:
            logging.error(f"Error analyzing sentiment with LLM: {e}")
            return {"score": 0.0, "reasoning": "LLM processing error."}


# L p this not thay d i
class AdvancedFeatureEngineer:
    def __init__(self):
        self.validation_threshold = 0.02  # Ultra-strict: from test results
        self.early_stopping_patience = 3   # Very early stopping
        self.regularization_strength = 0.15  # Very sin regularization from test results
        self.dropout_rate = 0.7  # Very high dropout from test results
        self.batch_normalization = True
        self.data_augmentation = True
        self.cross_validation_folds = 10  # More folds from test results
        self.out_of_sample_testing = True
        self.max_depth_limit = 4  # Very shallow trees from test results
        self.min_samples_split_limit = 50  # Very high minimum from test results
        self.min_samples_leaf_limit = 25   # Very high minimum from test results
        self.max_features_limit = 0.3  # Very few features from test results
        self.cpu_threshold = 80  # CPU optimization from test results
        self.memory_threshold = 85  # Memory optimization from test results
        self.alert_limit = 5  # Reduced alert limit from test results
        self.overfitting_detection = True  # Enable overfitting detection
        self.validation_requirements = "STRICT"  # Strict validation

        # Initialize symbol configurations
        self.symbol_configs = self._initialize_symbol_configs()
    def _get_default_data(self, api_type):
        """Trvdata mc dnh data nh khi API fail"""
        defaults = {
            'fear_greed': {'value': 50, 'classification': 'Neutral'},
            'vix': {'current_price': 20, 'change_percent': 0},
            'crypto': {'hash_rate': 0, 'gas_price': 20},
            'commodity': {'usd_index': 100, 'usd_change': 0}
        }
        return defaults.get(api_type, {})

    def _initialize_symbol_configs(self):
        """Initialize optimized symbol-specific feature configurations"""
        return {
            "SPX500": {
                # Core indicators for US equity index
                "rsi_periods": [14, 21],
                "ema_periods": [5, 10, 20, 50, 100, 200],
                "atr_period": 14,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "volume_weight": 0.8,
                "session_features": True,
                "gap_features": True,
                "volatility_adjustment": True,

                # SPX500 specific features
                "vix_correlation": True,
                "sector_rotation": True,
                "support_resistance": True,
                "volume_profile": True,
                "market_breadth": True,

                # Removed indicators
                "use_bollinger": False,
                "use_stochastic": False,
                "use_adx": True  # Keep for trend strength
            },
            "DE40": {
                # Core indicators for European equity index
                "rsi_periods": [14, 21],
                "ema_periods": [5, 10, 20, 50, 100, 200],
                "atr_period": 14,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "volume_weight": 0.8,
                "session_features": True,
                "gap_features": True,
                "volatility_adjustment": True,

                # DE40 specific features
                "vstoxx_correlation": True,
                "european_economic": True,
                "cross_asset_correlation": True,
                "bund_yield_correlation": True,

                # Removed indicators
                "use_bollinger": False,
                "use_stochastic": False,
                "use_adx": True
            },
            "XAUUSD": {
                # Core indicators for gold commodity
                "rsi_periods": [14, 21, 34],
                "ema_periods": [5, 10, 20, 50, 100, 200, 300],
                "atr_period": 21,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "volume_weight": 0.3,
                "session_features": False,
                "gap_features": False,
                "volatility_adjustment": True,
                "fibonacci_levels": True,

                # Gold specific features
                "usd_index_correlation": True,
                "real_interest_rates": True,
                "gold_silver_ratio": True,
                "central_bank_reserves": True,
                "geopolitical_risk": True,
                "inflation_hedge": True,

                # Removed indicators
                "use_bollinger": False,
                "use_stochastic": False,
                "use_adx": False
            },
            "AUDUSD": {
                # Core indicators for Australian Dollar forex pair
                "rsi_periods": [14, 21],
                "ema_periods": [5, 10, 20, 50, 100, 200],
                "atr_period": 14,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "volume_weight": 0.5,
                "session_features": True,
                "gap_features": False,
                "volatility_adjustment": True,

                # AUDUSD specific features
                "commodity_currency_index": True,
                "risk_on_risk_off": True,
                "asian_session_impact": True,
                "economic_calendar_impact": True,
                "central_bank_policy": True,
                "china_economic_data": True,
                "iron_ore_correlation": True,

                # Removed indicators
                "use_bollinger": False,
                "use_stochastic": False,
                "use_adx": False
            },
            "USOIL": {
                # Core indicators for crude oil commodity
                "rsi_periods": [14, 21, 34],
                "ema_periods": [5, 10, 20, 50, 100, 200],
                "atr_period": 21,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "volume_weight": 0.7,
                "session_features": False,
                "gap_features": False,
                "volatility_adjustment": True,

                # USOIL specific features
                "supply_demand_balance": True,
                "opec_decisions": True,
                "geopolitical_risk": True,
                "dollar_correlation": True,
                "economic_growth_correlation": True,
                "inventory_data": True,
                "seasonal_patterns": True,

                # Removed indicators
                "use_bollinger": False,
                "use_stochastic": False,
                "use_adx": True  # Keep for trend strength in oil
            },
            "AUDNZD": {
                # Core indicators for forex carry trade
                "rsi_periods": [14, 21],
                "ema_periods": [5, 10, 20, 50, 100, 200],
                "atr_period": 14,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "volume_weight": 0.5,
                "session_features": True,
                "gap_features": False,
                "volatility_adjustment": False,

                # Forex specific features
                "interest_rate_differential": True,
                "commodity_currency_index": True,
                "risk_on_risk_off": True,
                "carry_trade_momentum": True,
                "economic_calendar_impact": True,
                "central_bank_policy": True,

                # Removed indicators
                "use_bollinger": False,
                "use_stochastic": False,
                "use_adx": False
            },
            "BTCUSD": {
                # Core indicators for Bitcoin cryptocurrency
                "rsi_periods": [14, 21],
                "ema_periods": [5, 10, 20, 50, 100, 200],
                "atr_period": 14,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "volume_weight": 0.6,
                "session_features": False,
                "gap_features": False,
                "volatility_adjustment": True,

                # Bitcoin specific features
                "fear_greed_index": True,
                "whale_movements": True,
                "mining_difficulty": True,
                "hash_rate": True,
                "exchange_flows": True,
                "institutional_adoption": True,
                "regulatory_news": True,
                "macro_correlation": True,

                # Removed indicators
                "use_bollinger": False,
                "use_stochastic": False,
                "use_adx": False
            },
            "ETHUSD": {
                # Core indicators for Ethereulevelryptocurrency
                "rsi_periods": [14, 21],
                "ema_periods": [5, 10, 20, 50, 100, 200],
                "atr_period": 14,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "volume_weight": 0.6,
                "session_features": False,
                "gap_features": False,
                "volatility_adjustment": True,

                # Ethereum specific features
                "gas_fees": True,
                "defi_tvl": True,
                "staking_rewards": True,
                "network_activity": True,
                "developer_activity": True,
                "upgrade_events": True,
                "btc_correlation": True,
                "altcoin_season": True,

                # Removed indicators
                "use_bollinger": False,
                "use_stochastic": False,
                "use_adx": False
            },
            "ETHUSD": {
                # Core indicators for Ethereulevelryptocurrency
                "rsi_periods": [14, 21],
                "ema_periods": [5, 10, 20, 50, 100, 200],
                "atr_period": 14,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "volume_weight": 0.6,
                "session_features": False,
                "gap_features": False,
                "volatility_adjustment": True,

                # Ethereum specific features
                "gas_fees": True,
                "defi_tvl": True,
                "staking_rewards": True,
                "network_activity": True,
                "developer_activity": True,
                "upgrade_events": True,
                "btc_correlation": True,
                "altcoin_season": True,

                # Removed indicators
                "use_bollinger": False,
                "use_stochastic": False,
                "use_adx": False
            },
            "JP225": {
                "rsi_periods": [14, 21],
                "ema_periods": [5, 10, 20, 50, 100, 200],
                "bb_periods": [10, 20, 50],
                "atr_period": 14,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "stoch_params": {"k": 14, "d": 3},
                "volume_weight": 0.8,
                "session_features": True,
                "gap_features": True,
                "volatility_adjustment": True
            },

            # === CRYPTOoldRRENCIES (24/7 Trading) ===
            "BTCUSD": {
                "rsi_periods": [14, 21],
                "ema_periods": [5, 10, 20, 50, 100, 200],
                "bb_periods": [10, 20, 50],
                "atr_period": 14,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "stoch_params": {"k": 14, "d": 3},
                "volume_weight": 0.6,  # Lower weight for crypto volume
                "session_features": False,  # No session for 24/7 crypto
                "gap_features": False,  # No gaps for crypto
                "volatility_adjustment": True,

                # Crypto specific features
                "crypto_sentiment": True,
                "volatility_breakout": True,
                "support_resistance": True,
                "momentum_features": True,
                "use_bollinger": True,
                "use_stochastic": True,
                "use_adx": True
            },
            "ETHUSD": {
                "rsi_periods": [14, 21],
                "ema_periods": [5, 10, 20, 50, 100, 200],
                "bb_periods": [10, 20, 50],
                "atr_period": 14,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "stoch_params": {"k": 14, "d": 3},
                "volume_weight": 0.6,  # Lower weight for crypto volume
                "session_features": False,  # No session for 24/7 crypto
                "gap_features": False,  # No gaps for crypto
                "volatility_adjustment": True,

                # Crypto specific features
                "crypto_sentiment": True,
                "volatility_breakout": True,
                "support_resistance": True,
                "momentum_features": True,
                "use_bollinger": True,
                "use_stochastic": True,
                "use_adx": True
            }
        }

    def _get_symbol_config(self, symbol):
        """Get configuration for specific symbol"""
        return self.symbol_configs.get(symbol, self.symbol_configs["SPX500"])

    def add_technical_indicators(df):
        # if bn c logic t ci g d da vo duc. t model th cho qua li:
        return df

    # <<< fix L I D T I M 1/2 >>>
    # Function ny d fix d duc tnh t RSI public receive a.
    # N sto ra from 'rsi' chung, gip code linh ho t hon.
    def create_technical_features(self, df, symbol=None):
        """Create advanced technical indicators optimized for specific symbol"""
        config = self._get_symbol_config(symbol) if symbol else self.symbol_configs["SPX500"]
        metadata = SYMBOL_METADATA.get(symbol, {}) if symbol else {}

        # Basic Price Features
        df["hl2"] = (df["high"] + df["low"]) / 2
        df["hlc3"] = (df["high"] + df["low"] + df["close"]) / 3
        df["ohlc4"] = (df["open"] + df["high"] + df["low"] + df["close"]) / 4

        # Volatility Features with symbol-specific periods
        atr_period = config["atr_period"]
        atr = AverageTrueRange(df["high"], df["low"], df["close"], window=atr_period)
        df["atr"] = atr.average_true_range()
        df["atr_normalized"] = df["atr"] / df["close"]

        # Asset class specific volatility adjustments
        if config["volatility_adjustment"]:
            df["atr_percentile"] = df["atr_normalized"].rolling(100).rank(pct=True)
            df["volatility_regime"] = pd.qcut(df["atr_percentile"],
                                           q=3,
                                           labels=["low", "normal", "high"])

        # Bollinger Bands - only for symbols that use them
        if config.get("use_bollinger", True):
            bb_periods = config.get("bb_periods", [20])
            for period in bb_periods:
                bb = BollingerBands(df["close"], window=period, window_dev=2)
                df[f"bb_upper_{period}"] = bb.bollinger_hband()
                df[f"bb_lower_{period}"] = bb.bollinger_lband()
                df[f"bb_width_{period}"] = bb.bollinger_wband()
                df[f"bb_position_{period}"] = (df["close"] - bb.bollinger_lband()) / (
                    bb.bollinger_hband() - bb.bollinger_lband()
                )
                # Squeeze detection
                df[f"bb_squeeze_{period}"] = (df[f"bb_width_{period}"] < df[f"bb_width_{period}"].rolling(20).mean() * 0.8).astype(int)

        # Moving Averages with symbol-specific periods
        for period in config["ema_periods"]:
            ema = EMAIndicator(df["close"], window=period)
            df[f"ema_{period}"] = ema.ema_indicator()
            if period > 5:
                df[f"ema_ratio_{period}"] = df["close"] / df[f"ema_{period}"]
                df[f"ema_distance_{period}"] = (df["close"] - df[f"ema_{period}"]) / df["atr"]

        # EMA crossovers
        if len(config["ema_periods"]) >= 2:
            fast_period = min(config["ema_periods"])
            slow_period = max([p for p in config["ema_periods"] if p > fast_period])
            df[f"ema_cross_{fast_period}_{slow_period}"] = (
                (df[f"ema_{fast_period}"] > df[f"ema_{slow_period}"]) &
                (df[f"ema_{fast_period}"].shift(1) <= df[f"ema_{slow_period}"].shift(1))
            ).astype(int)

        # RSI with multiple periods
        for period in config["rsi_periods"]:
            rsi = RSIIndicator(df["close"], window=period)
            df[f"rsi_{period}"] = rsi.rsi()
            df[f"rsi_oversold_{period}"] = (df[f"rsi_{period}"] < 30).astype(int)
            df[f"rsi_overbought_{period}"] = (df[f"rsi_{period}"] > 70).astype(int)

            # RSI divergence detection
            df[f"rsi_divergence_{period}"] = self._detect_rsi_divergence(df, period)

        # Primary RSI (for compatibility)
        primary_rsi_period = config["rsi_periods"][0]
        df["rsi"] = df[f"rsi_{primary_rsi_period}"]
        df["rsi_oversold"] = df[f"rsi_oversold_{primary_rsi_period}"]
        df["rsi_overbought"] = df[f"rsi_overbought_{primary_rsi_period}"]

        # MACD with symbol-specific parameters
        macd_params = config["macd_params"]
        macd = MACD(df["close"],
                   window_fast=macd_params["fast"],
                   window_slow=macd_params["slow"],
                   window_sign=macd_params["signal"])
        df["macd"] = macd.macd()
        df["macd_signal"] = macd.macd_signal()
        df["macd_histogram"] = macd.macd_diff()
        df["macd_cross"] = ((df["macd"] > df["macd_signal"]) &
                          (df["macd"].shift(1) <= df["macd_signal"].shift(1))).astype(int)

        # ADX Trend Strength - only for symbols that use it
        if config.get("use_adx", True):
            adx = ADXIndicator(df["high"], df["low"], df["close"])
            df["adx"] = adx.adx()
            df["adx_trending"] = (df["adx"] > 25).astype(int)
            df["adx_sin_trend"] = (df["adx"] > 50).astype(int)

        # Stochastic Oscillator - only for symbols that use it
        if config.get("use_stochastic", True):
            stoch_params = config.get("stoch_params", {"k": 14, "d": 3})
            stoch = StochasticOscillator(df["high"], df["low"], df["close"],
                                       window=stoch_params["k"], smooth_window=stoch_params["d"])
            df["stoch_k"] = stoch.stoch()
            df["stoch_d"] = stoch.stoch_signal()
            df["stoch_oversold"] = (df["stoch_k"] < 20).astype(int)
            df["stoch_overbought"] = (df["stoch_k"] > 80).astype(int)

        # Asset class specific features
        if config.get("fibonacci_levels") and symbol == "XAUUSD":
            df = self._add_fibonacci_features(df)

        if config.get("crypto_specific") and symbol in ["BTCUSD", "ETHUSD"]:
            df = self._add_crypto_features(df)

        if config.get("carry_trade_features") and symbol == "AUDNZD":
            df = self._add_carry_trade_features(df)

        if config.get("equity_specific") and symbol in ["SPX500", "DE40"]:
            df = self._add_equity_features(df, symbol)

        if config.get("commodity_specific") and symbol == "XAUUSD":
            df = self._add_commodity_features(df, symbol)

        # Ensure required indicators exist
        required_indicators = ["rsi", "atr", "macd", "adx"]
        for indicator in required_indicators:
            if indicator not in df.columns:
                df[indicator] = 0

        return df

    def _detect_rsi_divergence(self, df, period):
        """Detect RSI divergence patterns"""
        rsi_col = f"rsi_{period}"
        if rsi_col not in df.columns:
            return pd.Series(0, index=df.index)

        # Look for divergence over 20 periods
        lookback = 20
        divergence = pd.Series(0, index=df.index)

        for i in range(lookback, len(df)):
            price_slice = df["close"].iloc[i-lookback:i]
            rsi_slice = df[rsi_col].iloc[i-lookback:i]

            # Bullish divergence: lower lows in price, higher lows in RSI
            price_low_idx = price_slice.idxmin()
            rsi_low_idx = rsi_slice.idxmin()

            if price_low_idx != rsi_low_idx:
                recent_price_low = price_slice.iloc[-5:].min()
                recent_rsi_low = rsi_slice.iloc[-5:].min()

                if recent_price_low < price_slice.min() and recent_rsi_low > rsi_slice.min():
                    divergence.iloc[i] = 1

        return divergence

    def _add_fibonacci_features(self, df):
        """Add Fibonacci retracement features for Gold"""
        # Calculate swing highs and lows
        swing_high = df["high"].rolling(20).max()
        swing_low = df["low"].rolling(20).min()

        # Fibonaalli levels
        fib_levels = [0.236, 0.382, 0.5, 0.618, 0.786]
        for level in fib_levels:
            df[f"fib_{level}"] = swing_low + (swing_high - swing_low) * level
            df[f"near_fib_{level}"] = (abs(df["close"] - df[f"fib_{level}"]) < df["atr"]).astype(int)

        return df


    def _add_carry_trade_features(self, df):
        """Add array trade specific features for forex pairs"""
        # Interest rate differential proxy (simplified)
        df["carry_signal"] = (df["close"] > df["close"].rolling(50).mean()).astype(int)

        # Risk-on/Risk-off sentiment
        df["risk_sentiment"] = df["close"].rolling(20).mean() / df["close"].rolling(100).mean()

        return df

    def _add_equity_features(self, df, symbol):
        """Sync wrapper for equity features"""
        # Use simulated data for now, can be enhanced with async calls later
        df["market_breadth"] = np.random.uniform(0.3, 0.7, len(df))
        df["sector_rotation"] = np.random.uniform(-1, 1, len(df))
        df["volume_profile"] = df["volume"] / df["volume"].rolling(20).mean()
        df["support_level"] = df["low"].rolling(20).min()
        df["resistance_level"] = df["high"].rolling(20).max()
        df["price_position"] = (df["close"] - df["support_level"]) / (df["resistance_level"] - df["support_level"])

        # Add VIX simulation for SPX500
        if symbol == "SPX500":
            df["vix_level"] = np.random.uniform(15, 35, len(df))
            df["fear_level"] = np.where(df["vix_level"] > 30, 1, 0)

        return df

    def _add_commodity_features(self, df, symbol):
        """Sync wrapper for commodity features"""
        # Use simulated data for now
        df["usd_correlation"] = np.random.uniform(-0.8, -0.3, len(df))
        df["real_rates_impact"] = np.random.uniform(-0.5, 0.5, len(df))

        if symbol == "XAUUSD":
            df["usd_index"] = np.random.uniform(95, 105, len(df))
            df["usd_change"] = np.random.uniform(-0.02, 0.02, len(df))

        return df

    async def _add_equity_features_async(self, df, symbol, session=None):
        """Add equity-specific features for SPX500 and DE40 with real API data"""
        if session is None:
            # Fallback to simulated data if no session
            df["market_breadth"] = np.random.uniform(0.3, 0.7, len(df))
            df["sector_rotation"] = np.random.uniform(-1, 1, len(df))
            df["volume_profile"] = df["volume"] / df["volume"].rolling(20).mean()
            df["support_level"] = df["low"].rolling(20).min()
            df["resistance_level"] = df["high"].rolling(20).max()
            df["price_position"] = (df["close"] - df["support_level"]) / (df["resistance_level"] - df["support_level"])
            return df

        # Fetch real VIX data for SPX500
        if symbol == "SPX500":
            vix_data = await self._fetch_api_data(session, 'vix')
            if vix_data:
                df["vix_level"] = vix_data['current_price']
                df["vix_change"] = vix_data['change_percent']
                df["fear_level"] = np.where(df["vix_level"] > 30, 1, 0)

        # Volume profile analysis
        df["volume_profile"] = df["volume"] / df["volume"].rolling(20).mean()

        # Support/Resistance levels
        df["support_level"] = df["low"].rolling(20).min()
        df["resistance_level"] = df["high"].rolling(20).max()
        df["price_position"] = (df["close"] - df["support_level"]) / (df["resistance_level"] - df["support_level"])

        return df

    async def _add_crypto_features_async(self, df, symbol, session=None):
        """Add cryptocurrency-specific features with real API data"""
        if session is None:
            # Fallback to simulated data
            df["fear_greed_index"] = np.random.uniform(0, 100, len(df))
            df["whale_movements"] = np.random.uniform(-1, 1, len(df))
            df["network_activity"] = np.random.uniform(0, 1, len(df))
            return df

        # Fetch Fear & Greed Index
        fng_data = await self._fetch_api_data(session, 'fear_greed')
        if fng_data:
            df["fear_greed_index"] = fng_data['value']
            df["fear_greed_classification"] = fng_data['classification']

        # Fetch crypto-specifihas dataata
        crypto_data = await self._fetch_api_data(session, 'crypto', symbol)
        if crypto_data:
            if symbol == "BTCUSD":
                df["hash_rate"] = crypto_data.get('hash_rate', 0)
                df["mining_difficulty"] = crypto_data.get('difficulty', 0)
                df["network_activity"] = crypto_data.get('transaction_volume', 0) / 1000000  # Normalize
            elif symbol == "ETHUSD":
                df["gas_fees"] = crypto_data.get('gas_price', 20)
                df["gas_fees_impact"] = np.where(df["gas_fees"] > 50, -0.3, 0.1)

        return df

    async def _add_commodity_features_async(self, df, symbol, session=None):
        """Add commodity-specific features with real API data"""
        if session is None:
            # Fallback to simulated data
            df["usd_correlation"] = np.random.uniform(-0.8, -0.3, len(df))
            df["real_rates_impact"] = np.random.uniform(-0.5, 0.5, len(df))
            return df

        # Fetch USD Index data for gold
        if symbol == "XAUUSD":
            usd_data = await self._fetch_api_data(session, 'commodity', symbol)
            if usd_data:
                df["usd_index"] = usd_data.get('usd_index', 100)
                df["usd_change"] = usd_data.get('usd_change', 0)
                df["usd_correlation"] = -0.7  # Gold typically inverse to USD

        return df

    #  t Function this bn in clasfixdvancedFeatureEngineer of b n
    def create_wyckoff_features(self, df, atr_multiplier=DEFAULT_ATR_MULTIPLIER, range_window=DEFAULT_RANGE_WINDOW):
        """
        Create numerical features based on Wyckoff method principles.
        """
        logging.info(f"   [Features] Starting Wyckoff features creation...")
        if "atr" not in df.columns:
            df["atr"] = AverageTrueRange(df["high"], df["low"], df["close"]).average_true_range()

        # 1. Identify Trading Range
        df['rolling_max'] = df['high'].rolling(window=range_window).max()
        df['rolling_min'] = df['low'].rolling(window=range_window).min()
        df['is_in_range'] = (df['close'] < df['rolling_max']) & (df['close'] > df['rolling_min'])

        # 2. Quantify Key Events (Spring & Upthrust)
        # Spring: Price breaks below support zone then quickly returns
        df['spring_signal'] = (
            (df['is_in_range'].shift(1) == True) & # Previous candle was in range
            (df['low'] < df['rolling_min'].shift(1)) &      # current candle breaks below
            (df['close'] > df['rolling_min'].shift(1))     # and closes back inside range
        ).astype(int)

        # Upthrust: Price breakfixbove resistance zone then quickly returns
        df['upthrust_signal'] = (
            (df['is_in_range'].shift(1) == True) & # Previous candle was in range
            (df['high'] > df['rolling_max'].shift(1)) &     # current candle breakfixbove
            (df['close'] < df['rolling_max'].shift(1))     # and closes back inside range
        ).astype(int)

        # 3. Volume Spread Analysis (VSA Features)
        df['price_spread'] = df['high'] - df['low']
        df['volume_ema_50'] = df['volume'].ewm(span=50, adjust=False).mean()
        df['is_high_volume'] = (df['volume'] > df['volume_ema_50'] * 2).astype(int)

        # No Demand signal: Rising candle, narrow spread, low volume -> weak buying
        df['no_demand_signal'] = (
            (df['close'] > df['open']) &
            (df['price_spread'] < df['price_spread'].rolling(20).mean() * 0.7) &
            (df['volume'] < df['volume_ema_50'] * 0.8)
        ).astype(int)

        # No Supply signal: Falling candle, narrow spread, low volume -> weak selling
        df['no_supply_signal'] = (
            (df['close'] < df['open']) &
            (df['price_spread'] < df['price_spread'].rolling(20).mean() * 0.7) &
            (df['volume'] < df['volume_ema_50'] * 0.8)
        ).astype(int)

        # 4. Identify Phases (simplified)
        # Phase 1/3 (Accumulation/Distribution): Low volatility
        df['atr_normalized'] = df['atr'] / df['close']
        df['low_volatility_phase'] = (df['atr_normalized'] < df['atr_normalized'].rolling(range_window).quantile(0.25)).astype(int)

        # Phase 2/4 (Mark-up/Mark-down): High volatility, trending
        df['high_volatility_phase'] = (df['atr_normalized'] > df['atr_normalized'].rolling(range_window).quantile(0.75)).astype(int)

        logging.info(f"   [Features] Wyckoff features creation completed.")
        return df
    def create_statistical_features(self, df):
        """Create statistical features"""
        # Returns with multiple periods
        for period in [1, 3, 5, 10, 20]:
            df[f"returns_{period}"] = df["close"].pct_change(period)
            df[f"log_returns_{period}"] = np.log(
                df["close"] / df["close"].shift(period)
            )

        # Rolling Statistics
        for window in [10, 20, 50]:
            df[f"rolling_mean_{window}"] = df["close"].rolling(window).mean()
            df[f"rolling_std_{window}"] = df["close"].rolling(window).std()
            df[f"rolling_skew_{window}"] = df["close"].rolling(window).skew()
            df[f"rolling_kurt_{window}"] = df["close"].rolling(window).kurt()

            # Z-Score
            df[f"zscore_{window}"] = (df["close"] - df[f"rolling_mean_{window}"]) / df[
                f"rolling_std_{window}"
            ]

        # High-Low Ratios
        for period in [5, 10, 20]:
            df[f"high_low_ratio_{period}"] = (
                df["high"].rolling(period).max() / df["low"].rolling(period).min()
            )
            df[f"close_position_{period}"] = (
                df["close"] - df["low"].rolling(period).min()
            ) / (df["high"].rolling(period).max() - df["low"].rolling(period).min())

        return df

    def create_pattern_features(self, df):
        """Create pattern recognition features"""
        # --- G I  C I THI N 4: NNG C P receive DI N levelonfiguration ---
        # Solve models configuration old
        df["doji"] = (
            abs(df["open"] - df["close"]) <= (df["high"] - df["low"]) * 0.1
        ).astype(int)
        df["hammer"] = (
            (df["close"] > df["open"])
            & ((df["close"] - df["open"]) <= (df["high"] - df["low"]) * 0.3)
            & ((df["open"] - df["low"]) >= (df["high"] - df["low"]) * 0.6)
        ).astype(int)

        # Add model configurationnh n engulfing (Engulfing)
        prev_body = abs(df["open"].shift(1) - df["close"].shift(1))
        current_body = abs(df["open"] - df["close"])
        df["bullish_engulfing"] = (
            (df["close"] > df["open"])
            & (df["close"].shift(1) < df["open"].shift(1))
            & (df["close"] > df["open"].shift(1))
            & (df["open"] < df["close"].shift(1))
            & (current_body > prev_body)
        ).astype(int)
        df["bearish_engulfing"] = (
            (df["close"] < df["open"])
            & (df["close"].shift(1) > df["open"].shift(1))
            & (df["close"] < df["open"].shift(1))
            & (df["open"] > df["close"].shift(1))
            & (current_body > prev_body)
        ).astype(int)

        # Support/Resistance Levels
        df["support_level"] = df["low"].rolling(20).min()
        df["resistance_level"] = df["high"].rolling(20).max()
        df["near_support"] = (
            abs(df["close"] - df["support_level"]) <= df["atr"]
        ).astype(int)
        df["near_resistance"] = (
            abs(df["close"] - df["resistance_level"]) <= df["atr"]
        ).astype(int)

        # Trend Features
        df["higher_highs"] = (df["high"] > df["high"].shift(1)).rolling(5).sum()
        df["lower_lows"] = (df["low"] < df["low"].shift(1)).rolling(5).sum()
        df["trend_strength"] = df["higher_highs"] - df["lower_lows"]

        return df

    def create_volume_features(self, df):
        """Create volume-based features with EMA"""
        if "volume" in df.columns:
            # Volume Exponential Moving Average
            df["volume_ema"] = df["volume"].ewm(span=20, adjust=False).mean()
            df["volume_ratio"] = df["volume"] / df["volume_ema"]

            # Price-Volume Relationship
            df["pv_trend"] = np.where(
                df["close"] > df["close"].shift(1), df["volume"], -df["volume"]
            )
            df["pv_cumulative"] = df["pv_trend"].rolling(20).sum()

        return df

    def create_market_microstructure_features(self, df):
        """Create markefrom modelicrostructure features"""
        # Bid-Ask Spread Proxy
        df["spread_proxy"] = (df["high"] - df["low"]) / df["close"]
        df["spread_volatility"] = df["spread_proxy"].rolling(20).std()

        # Intraday Patterns
        if isinstance(df.index, pd.DatetimeIndex):
            df["hour"] = df.index.hour
            df["day_of_week"] = df.index.dayofweek
            # Session Features (Asian, European, American)
            df["asian_session"] = ((df["hour"] >= 0) & (df["hour"] < 8)).astype(int)
            df["european_session"] = ((df["hour"] >= 8) & (df["hour"] < 16)).astype(int)
            df["american_session"] = ((df["hour"] >= 16) & (df["hour"] < 24)).astype(
                int
            )
        else:
            df["hour"] = 0
            df["day_of_week"] = 0
            df["asian_session"] = 0
            df["european_session"] = 0
            df["american_session"] = 0

        return df

    # Bn in clasfixdvancedFeatureEngineer

    # <<< fix L I Function this in L P AdvancedFeatureEngineer >>>
    def create_all_features(self, df):
        """Create all features"""
        df = self.create_technical_features(df)
        df = self.create_statistical_features(df)
        df = self.create_pattern_features(df)
        df = self.create_volume_features(df)
        df = self.create_market_microstructure_features(df)
        df = self.create_wyckoff_features(df)

        # <<< C I money FEATURE ENGINEERING: G i Function to feature tempty thi thtru ng >>>
        df = self.create_market_regime_feature(df)

        zones = self._find_sd_zones(df)
        print(
            f"   Found total public {len(zones)} zones for {df.name if hasattr(df, 'name') else 'current symbol'}."
        )
        df = self.create_supply_demand_features(df, zones)
        df = self.create_market_structure_signals(df)

        for horizon in [1, 3, 5]:
            df[f"label_{horizon}"] = (df["close"].shift(-horizon) > df["close"]).astype(
                int
            )

        return df
    #  t bn in clasfixdvancedFeatureEngineer
    def _find_sd_zones(self, df, atr_multiplier=1.2):
        """
        Function to identify all supply and demand zones.
        Includes all reversal and continuation zones.
        """
        if "atr" not in df.columns or df["atr"].isnull().all():
            # print("   [Zones] ATR is null or invalid. Skip zone detection.")
            return []

        zones = []
        # Start from 2nd candle and end before last candle to access i-1 and i+1
        for i in range(1, len(df) - 1):
            # Xhas data receive n "base" (thn nh )
            is_base_candle = (
                abs(df["close"].iloc[i] - df["open"].iloc[i])
                < (df["high"].iloc[i] - df["low"].iloc[i]) * 0.6
            )
            if not is_base_candle:
                continue

            # Xhas data receive n m nh before v sau n n base
            move_before = abs(df["close"].iloc[i - 1] - df["open"].iloc[i - 1])
            move_after = abs(df["close"].iloc[i + 1] - df["open"].iloc[i + 1])

            # Chỉ lấy ATR hợp lệ 
            atr_value = df["atr"].iloc[i]
            if pd.isna(atr_value) or atr_value == 0:
                continue
            atr_threshold = atr_value * atr_multiplier

            is_sin_move_before = move_before > atr_threshold
            is_sin_move_after = move_after > atr_threshold

            if is_sin_move_before and is_sin_move_after:
                # Xhas data nh hu ng of cneeds n m nh
                is_bullish_before = df["close"].iloc[i - 1] > df["open"].iloc[i - 1]
                is_bullish_after = df["close"].iloc[i + 1] > df["open"].iloc[i + 1]

                # --- levelonfiguration  O CHI U ---
                # Drop-Base-Rally -> Vng C u  o chi u (Demand Reversal)
                if not is_bullish_before and is_bullish_after:
                    zones.append(
                        {
                            "type": "demand_reversal",
                            "high": df["high"].iloc[i],
                            "low": df["low"].iloc[i],
                            "index": df.index[i],
                        }
                    )
                # Rally-Base-Drop -> Vng cung  o chi u (Supply Reversal)
                elif is_bullish_before and not is_bullish_after:
                    zones.append(
                        {
                            "type": "supply_reversal",
                            "high": df["high"].iloc[i],
                            "low": df["low"].iloc[i],
                            "index": df.index[i],
                        }
                    )

                # --- levelonfiguration TI P DI N ---
                # Rally-Base-Rally -> Vng Cu Tip din (Demand continuation)
                elif is_bullish_before and is_bullish_after:
                    zones.append(
                        {
                            "type": "demand_continuation",
                            "high": df["high"].iloc[i],
                            "low": df["low"].iloc[i],
                            "index": df.index[i],
                        }
                    )
                # Drop-Base-Drop -> Vng cung Ti p di n (Supply Continuation)
                elif not is_bullish_before and not is_bullish_after:
                    zones.append(
                        {
                            "type": "supply_continuation",
                            "high": df["high"].iloc[i],
                            "low": df["low"].iloc[i],
                            "index": df.index[i],
                        }
                    )
        return zones

    def create_supply_demand_features(self, df, zones):
        """
        Create all supply/demand features.
        PHIN B N T I UU HA HI U NANG equal allh vector ha.
        """
        # --- Part 1: Calculate "in_zone" (keep original loop logic complexity) ---
        in_sr = np.zeros(len(df))
        in_dr = np.zeros(len(df))
        in_sc = np.zeros(len(df))
        in_dc = np.zeros(len(df))

        close_prices_np = df["close"].to_numpy()
        df_index_np = df.index

        for i in range(len(df)):
            current_price = close_prices_np[i]
            current_time = df_index_np[i]
            for zone in [z for z in zones if z["index"] < current_time]:
                if zone["low"] <= current_price <= zone["high"]:
                    if zone["type"] == "supply_reversal":
                        in_sr[i] = 1
                    elif zone["type"] == "demand_reversal":
                        in_dr[i] = 1
                    elif zone["type"] == "supply_continuation":
                        in_sc[i] = 1
                    elif zone["type"] == "demand_continuation":
                        in_dc[i] = 1
                    break

        df["in_supply_reversal"] = in_sr
        df["in_demand_reversal"] = in_dr
        df["in_supply_continuation"] = in_sc
        df["in_demand_continuation"] = in_dc

        # --- Ph n 2: T i uu ha tnh ton "distance_to..." equal vector ha ---

        # Separate all zones by type
        supply_rev_zones = [z for z in zones if z["type"] == "supply_reversal"]
        demand_rev_zones = [z for z in zones if z["type"] == "demand_reversal"]
        supply_cont_zones = [z for z in zones if z["type"] == "supply_continuation"]
        demand_cont_zones = [z for z in zones if z["type"] == "demand_continuation"]

        # Function to create "distance" features for all zones
        def create_zone_map(df_index, zone_list, price_key):
            zone_map = pd.Series(np.nan, index=df_index)
            for zone in zone_list:
                zone_map.loc[zone["index"]] = zone[price_key]
            return (
                zone_map.ffill()
            )  # Forward fill to know the nearest zone in the past 

        # Create distance features for all zone types
        sr_map = create_zone_map(df.index, supply_rev_zones, "low")
        dr_map = create_zone_map(df.index, demand_rev_zones, "high")
        sc_map = create_zone_map(df.index, supply_cont_zones, "low")
        dc_map = create_zone_map(df.index, demand_cont_zones, "high")

        # Calculate distances in batch (vectorized)
        dist_sr = (sr_map - df["close"]) / df["close"]
        dist_dr = (df["close"] - dr_map) / df["close"]
        dist_sc = (sc_map - df["close"]) / df["close"]
        dist_dc = (df["close"] - dc_map) / df["close"]

        # Solve all distance calculations (supply zones must be above, demand zones must be below)
        df["distance_to_supply_reversal"] = np.where(dist_sr > 0, dist_sr, np.nan)
        df["distance_to_demand_reversal"] = np.where(dist_dr > 0, dist_dr, np.nan)
        df["distance_to_supply_continuation"] = np.where(dist_sc > 0, dist_sc, np.nan)
        df["distance_to_demand_continuation"] = np.where(dist_dc > 0, dist_dc, np.nan)

        # --- Ph n 3: D n d p cu cng ---
        # use bfill to fill all NaN values at the beginning (before the first zone appears)
        cols_to_fill = [
            "distance_to_supply_reversal",
            "distance_to_demand_reversal",
            "distance_to_supply_continuation",
            "distance_to_demand_continuation",
        ]
        df[cols_to_fill] = df[cols_to_fill].bfill()

        return df

    def create_market_structure_signals(self, df):
        """
        To tn hiu da trn cu trc th trung.
        PHIN B N M I: Using tnh ton phn kRSI dufromi uu ha.
        """
        # 1. Signal from Bollinger Bands (original)
        bb = BollingerBands(df["close"], window=20, window_dev=2)
        df["bb_upper"] = bb.bollinger_hband()
        df["bb_lower"] = bb.bollinger_lband()
        df["bb_exhaustion_sell"] = ((df["close"].shift(1) > df["bb_upper"].shift(1)) & (df["close"] < df["bb_upper"])).astype(int)
        df["bb_exhaustion_buy"] = ((df["close"].shift(1) < df["bb_lower"].shift(1)) & (df["close"] > df["bb_lower"])).astype(int)

        # 2. Tín hiệu Phân kỳ RSI (RSI Divergence) - GỌI Function MỚI
        if 'rsi_14' not in df.columns:
            df["rsi_14"] = RSIIndicator(df["close"], window=14).rsi()

        # <<< THAY THTON BVNG L P l ng nhau equal 1 used this >>>
        df = calculate_rsi_divergence_vectorized(df)

        # D n d p all from from model if c
        cols_to_drop = ['price_peak', 'price_trough', 'rsi_peak', 'rsi_trough', 'rsi_14']
        df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True, errors='ignore')

        return df
    # <<< TFunction Function this VO in L P AdvancedFeatureEngineer >>>
    def create_market_regime_feature(self, df, adx_threshold=DEFAULT_ADX_THRESHOLD, ema_period=DEFAULT_EMA_PERIOD):
        """
        T o feature sdx l data nh tempty thi thtru ng.
        1: Uptrend, -1: Downtrend, 0: Sideways
        """
        logging.info("   [Features] Starting market state feature creation...")
        # Calculate necessary indicators
        adx_indicator = ADXIndicator(df['high'], df['low'], df['close'], window=14)
        df['adx_regime'] = adx_indicator.adx()
        df['ema_regime'] = EMAIndicator(df['close'], window=ema_period).ema_indicator()

        # Xhas data nh tempty thi
        conditions = [
            (df['adx_regime'] > adx_threshold) & (df['close'] > df['ema_regime']), # Uptrend
            (df['adx_regime'] > adx_threshold) & (df['close'] < df['ema_regime'])  # Downtrend
        ]
        choices = [1, -1]
        df['market_regime'] = np.select(conditions, choices, default=0) # Sideways l mc dnh data nh

        # D n d p all from from model
        df.drop(columns=['adx_regime', 'ema_regime'], inplace=True)
        logging.info("   [Features] Market state feature creation completed.")
        return df

class EnhancedEnsembleModel:
    """Enhanced ensemble with advanced CV, CPCV, and explainability"""

    def __init__(self):
        self.validation_threshold = 0.02  # Ultra-strict: from test results
        self.early_stopping_patience = 3   # Very early stopping
        self.regularization_strength = 0.15  # Very sin regularization from test results
        self.dropout_rate = 0.7  # Very high dropout from test results
        self.batch_normalization = True
        self.data_augmentation = True
        self.cross_validation_folds = 10  # More folds from test results
        self.out_of_sample_testing = True
        self.max_depth_limit = 4  # Very shallow trees from test results
        self.min_samples_split_limit = 50  # Very high minimum from test results
        self.min_samples_leaf_limit = 25   # Very high minimum from test results
        self.max_features_limit = 0.3  # Very few features from test results
        self.cpu_threshold = 80  # CPU optimization from test results
        self.memory_threshold = 85  # Memory optimization from test results
        self.alert_limit = 5  # Reduced alert limit from test results
        self.overfitting_detection = True  # Enable overfitting detection
        self.validation_requirements = "STRICT"  # Strict validation
    def enhanced_time_series_split(self, X, y, n_splits=5, gap=0, max_train_size=None):
        """Enhanced time series split with gap and purging"""
        n_samples = len(X)
        indices = np.arange(n_samples)

        # Calculate split sizes
        test_size = n_samples // (n_splits + 1)
        splits = []

        for i in range(n_splits):
            # Calculate test indices
            test_start = (i + 1) * test_size
            test_end = test_start + test_size

            if test_end > n_samples:
                test_end = n_samples

            test_indices = indices[test_start:test_end]

            # Calculate train indices with gap
            train_end = test_start - gap
            train_start = 0

            if max_train_size and train_end - train_start > max_train_size:
                train_start = train_end - max_train_size

            if train_start >= 0 and train_end > train_start:
                train_indices = indices[train_start:train_end]
                splits.append((train_indices, test_indices))

        return splits

    def combinatorial_purged_cv(self, X, y, n_splits=5, n_test_splits=2, gap=24):
        """Combinatorial Purged Cross-Validation for financial data"""
        n_samples = len(X)
        test_size = n_samples // n_splits

        # Generate all possible test set combinations
        test_starts = [i * test_size for i in range(n_splits)]
        test_combinations = []

        from itertools import combinations
        for combo in combinations(range(n_splits), n_test_splits):
            test_indices = []
            for i in combo:
                start_idx = test_starts[i]
                end_idx = min(start_idx + test_size, n_samples)
                test_indices.extend(range(start_idx, end_idx))

            # Remove overlapping periodfixnd apply gap
            test_indices = sorted(set(test_indices))

            # Create train indices with purging
            train_indices = []
            for i in range(n_samples):
                # Check if index is far enough from any test index
                min_distance = min([abs(i - t) for t in test_indices]) if test_indices else gap + 1
                if min_distance > gap:
                    train_indices.append(i)

            if len(train_indices) > 0 and len(test_indices) > 0:
                test_combinations.append((train_indices, test_indices))

        return test_combinations

    def get_model_explanation(self, X_sample, feature_names=None, top_k=5):
        """Gefrom modelodel explanation for a prediction"""
        explanations = {}

        # Get base model predictionfixnd feature importance
        for name, model in self.models.items():
            if hasattr(model, 'predict_proba'):
                prediction = model.predict_proba(X_sample.reshape(1, -1))[0, 1]
            else:
                prediction = model.predict(X_sample.reshape(1, -1))[0]

            # Get feature importance for this prediction
            if hasattr(model, 'feature_importances_'):
                if feature_names is None:
                    feature_names = [f"feature_{i}" for i in range(len(X_sample))]

                feature_importance = dict(zip(
                    feature_names,
                    model.feature_importances_
                ))

                # Get top contributing features
                top_features = sorted(
                    feature_importance.items(),
                    key=lambda x: abs(x[1]),
                    reverse=True
                )[:top_k]

                explanations[name] = {
                    'prediction': prediction,
                    'top_features': top_features,
                    'confidence': prediction if prediction > 0.5 else 1 - prediction
                }

        return explanations

class OrderSafetyManager:
    """Advanced order safety with multiple validation layers"""

    def __init__(self):
        self.validation_threshold = 0.02  # Ultra-strict: from test results
        self.early_stopping_patience = 3   # Very early stopping
        self.regularization_strength = 0.15  # Very sin regularization from test results
        self.dropout_rate = 0.7  # Very high dropout from test results
        self.batch_normalization = True
        self.data_augmentation = True
        self.cross_validation_folds = 10  # More folds from test results
        self.out_of_sample_testing = True
        self.max_depth_limit = 4  # Very shallow trees from test results
        self.min_samples_split_limit = 50  # Very high minimum from test results
        self.min_samples_leaf_limit = 25   # Very high minimum from test results
        self.max_features_limit = 0.3  # Very few features from test results
        self.cpu_threshold = 80  # CPU optimization from test results
        self.memory_threshold = 85  # Memory optimization from test results
        self.alert_limit = 5  # Reduced alert limit from test results
        self.overfitting_detection = True  # Enable overfitting detection
        self.validation_requirements = "STRICT"  # Strict validation
    def validate_order_size(self, symbol, proposed_size, account_balance, max_risk_per_trade=0.01):
        """Validate order size against risk limits"""
        max_position_value = account_balance * max_risk_per_trade

        # Get current price (simplified - in production, fetch real price)
        current_price = 1.0  # Placeholder
        proposed_value = abs(proposed_size) * current_price

        if proposed_value > max_position_value:
            return False, f"Position value {proposed_value} exceeds max risk {max_position_value}"

        return True, "Order size validated"

    def validate_order_frequency(self, symbol, cooldown_minutes=5):
        """Validate order frequency to preventovertrading"""
        current_time = datetime.now()
        recent_orders = [
            order for order in self.order_history
            if order['symbol'] == symbol and
               (current_time - order['timestamp']).total_seconds() < cooldown_minutes * 60
        ]

        if len(recent_orders) > 0:
            return False, f"Recentorder for {symbol} within {cooldown_minutes} minutes"

        return True, "Order frequency validated"

    def validate_market_conditions(self, symbol):
        """Validate market conditions for safe trading"""
        # Check if market is open
        if not is_market_open(symbol):
            return False, f"Market closed for {symbol}"

        # Check weekend conditions
        if is_weekend() and not is_crypto_symbol(symbol):
            return False, f"Weekend trading not allowed for {symbol}"

        return True, "Market conditions validated"

    async def comprehensive_order_validation(self, symbol, size, direction, account_balance, existing_positions=None, returns_history=None):
        """Comprehensive order validation with all safety checks"""
        validation_result = {
            'approved': True,
            'violations': [],
            'safety_score': 1.0,
            'recommendations': []
        }

        # 1. Size validation
        size_ok, size_msg = self.validate_order_size(symbol, size, account_balance)
        if not size_ok:
            validation_result['approved'] = False
            validation_result['violations'].append(f"Size: {size_msg}")

        # 2. Frequency validation
        freq_ok, freq_msg = self.validate_order_frequency(symbol)
        if not freq_ok:
            validation_result['approved'] = False
            validation_result['violations'].append(f"Frequency: {freq_msg}")

        # 3. Market conditions
        market_ok, market_msg = self.validate_market_conditions(symbol)
        if not market_ok:
            validation_result['approved'] = False
            validation_result['violations'].append(f"Market: {market_msg}")

        # 4. Risk management validation
        if existing_positions and returns_history:
            risk_validation = await self.risk_manager.validate_trade(
                symbol, size, existing_positions, returns_history
            )
            if not risk_validation['approved']:
                validation_result['approved'] = False
                validation_result['violations'].extend([
                    f"Risk: {v}" for v in risk_validation['violations']
                ])

        # 5. Calculate safety score
        total_checks = 4
        passed_checks = sum([
            size_ok, freq_ok, market_ok,
            risk_validation.get('approved', True) if 'risk_validation' in locals() else True
        ])
        validation_result['safety_score'] = passed_checks / total_checks

        # 6. Log validation result
        if not validation_result['approved']:
            violation_msg = f"Order validation failed for {symbol} {direction}:\n"
            for violation in validation_result['violations']:
                violation_msg += f"- {violation}\n"

            await self.observability.send_discord_alert(violation_msg, "ERROR")

            # Store violation
            self.safety_violations.append({
                'symbol': symbol,
                'violations': validation_result['violations'],
                'timestamp': datetime.now()
            })

        return validation_result

    def record_order(self, symbol, size, direction, price=None):
        """Record order for tracking and analysis"""
        order_record = {
            'symbol': symbol,
            'size': size,
            'direction': direction,
            'price': price,
            'timestamp': datetime.now()
        }

        self.order_history.append(order_record)
        logging.info(f"Order recorded: {symbol} {direction} {size}")

# Original class preserved for compatibility
class LSTMModel:
    def __init__(self, sequence_length=60, features_dim=40):
        self.sequence_length = sequence_length
        self.features_dim = features_dim
        self.model = None
        self.scaler = StandardScaler()

    def build_model(self):
        """Build LSTM model with enhanced anti-overfitting mechanisms"""
        input_layer = Input(shape=(self.sequence_length, self.features_dim))

        # Enhanced LSTM layers with siner regularization
        lstm1 = LSTM(64, return_sequences=True,
                    dropout=ML_CONFIG.get("DROPOUT_RATE", 0.35),
                    recurrent_dropout=0.2,
                    activation='tanh',
                    recurrent_activation='sigmoid',
                    kernel_regularizer=l2(ML_CONFIG.get("L2_REGULARIZATION", 0.001)))(input_layer)

        # Batch normalization for stability
        if ML_CONFIG.get("BATCH_NORMALIZATION", True):
            lstm1 = BatchNormalization()(lstm1)

        lstm2 = LSTM(32, return_sequences=True,
                    dropout=ML_CONFIG.get("DROPOUT_RATE", 0.35),
                    recurrent_dropout=0.2,
                    activation='tanh',
                    recurrent_activation='sigmoid',
                    kernel_regularizer=l2(ML_CONFIG.get("L2_REGULARIZATION", 0.001)))(lstm1)

        if ML_CONFIG.get("BATCH_NORMALIZATION", True):
            lstm2 = BatchNormalization()(lstm2)

        # Attention mechanism with regularization
        if ML_CONFIG.get("ATTENTION_MECHANISM", True):
            attention_out = Attention()([lstm2, lstm2])
        else:
            attention_out = lstm2

        lstm3 = LSTM(16, return_sequences=False,
                    dropout=ML_CONFIG.get("DROPOUT_RATE", 0.35),
                    recurrent_dropout=0.2,
                    activation='tanh',
                    recurrent_activation='sigmoid',
                    kernel_regularizer=l2(ML_CONFIG.get("L2_REGULARIZATION", 0.001)))(attention_out)

        # Dense layers with enhanced regularization
        dense1 = Dense(8, activation="relu",
                      kernel_regularizer=l2(ML_CONFIG.get("L2_REGULARIZATION", 0.001)))(lstm3)
        dropout1 = Dropout(ML_CONFIG.get("DROPOUT_RATE", 0.35))(dense1)

        # Additional dense layer for better representation
        dense2 = Dense(4, activation="relu",
                      kernel_regularizer=l2(ML_CONFIG.get("L2_REGULARIZATION", 0.001)))(dropout1)
        dropout2 = Dropout(ML_CONFIG.get("DROPOUT_RATE", 0.35))(dense2)

        output = Dense(1, activation="sigmoid")(dropout2)

        self.model = Model(inputs=input_layer, outputs=output)

        # Enhanced optimizer with gradient clipping
        optimizer = Adam(
            learning_rate=0.001,
            decay=1e-6,
            clipnorm=1.0 if ML_CONFIG.get("GRADIENT_CLIPPING", True) else None
        )

        self.model.compile(
            optimizer=optimizer,
            loss="binary_crossentropy",
            metrics=["accuracy", "precision", "recall"],
        )
        return self.model

    def prepare_sequences(self, X, y=None):
        """Prepare sequences for LSTM"""
        if X.empty or (y is not None and y.empty):
            logging.warning("   [LSTM PreSeq] Input X or y is empty.")
            return None, None if y is not None else None

        try:
            X_scaled = self.scaler.fit_transform(X)
        except ValueError as e:  # c th xy ra if X ch c 1 sample khi fit_transform
            print(
                f"   [LSTM PreSeq] Error scaling X (len {len(X)}): {e}. Using X as is for now."
            )
            if len(X.shape) == 1:  # if X l 1D array
                X_scaled = (
                    X.to_numpy().reshape(-1, 1)
                    if isinstance(X, pd.Series)
                    else np.array(X).reshape(-1, 1)
                )
            else:
                X_scaled = X.to_numpy() if isinstance(X, pd.DataFrame) else np.array(X)
            # Hoc transform nu scaler d fit before vi d liu
            # try:
            #    X_scaled = self.scaler.transform(X)
            # except NotFittedError:
            #    print("    Scaler not fitted, and cannot fit with current X. Returning None.")
            #    return None, None

        sequences = []
        targets = [] if y is not None else None

        max_length = len(X_scaled)
        if y is not None:
            max_length = min(len(X_scaled), len(y))

        # condition for sequence creation: max_length > self.sequence_length
        if max_length <= self.sequence_length:
            # print(f"   [LSTM PreSeq] Not enough data (max_length {max_length}) for sequence_length {self.sequence_length}. Need max_length > sequence_length.")
            return np.array([]), np.array([]) if y is not None else np.array([])

        for i in range(self.sequence_length, max_length):
            sequences.append(X_scaled[i - self.sequence_length : i])
            if y is not None:
                # Quan tempty: y l y_train_ensemble, n l pd.Series
                # needs d m b o index of y tuong  ng with allh l y X_scaled
                # if X l DataFrame v y l Series, X.iloc[i] v y.iloc[i] l ok
                targets.append(y.iloc[i])

        sequences = np.array(sequences)
        if targets is not None:
            targets = np.array(targets)
            if len(sequences) == 0:  # Double check
                return np.array([]), np.array([])
            return sequences, targets

        if len(sequences) == 0:  # Double check
            return np.array([])
        return sequences

    # TM V THAY THFunction this in L P LSTMModel

    def train(self, X, y):
        X_seq, y_seq = self.prepare_sequences(X, y)
        if X_seq is None or len(X_seq) < 10: # needs ti thiu 10 sequence d training
            print(f"   [LSTM] Not enough sequence data for training. Skipping.")
            self.model = None
            return None

        if self.model is None:
            self.build_model()

        # <<< C I money: L Y SPLIT old I CNG TTSCV THAY V SPLIT  U TIN >>>
        n_lstm_splits = 5
        if len(X_seq) < n_lstm_splits:
            n_lstm_splits = max(2, len(X_seq) -1) #  m b o from nh t 1 split

        tscv_lstm = TimeSeriesSplit(n_splits=n_lstm_splits)

        # L y ra splitcu cng
        all_splits = list(tscv_lstm.split(X_seq))
        train_idx_lstm, val_idx_lstm = all_splits[-1]

        X_train_lstm, X_val_lstm = X_seq[train_idx_lstm], X_seq[val_idx_lstm]
        y_train_lstm, y_val_lstm = y_seq[train_idx_lstm], y_seq[val_idx_lstm]
        # <<< K T THC C I money >>>

        callbacks = [
            EarlyStopping(
                monitor="val_accuracy",
                patience=ML_CONFIG.get("EARLY_STOPPING_PATIENCE", 15),
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor="val_accuracy",
                patience=5,
                factor=ML_CONFIG.get("LEARNING_RATE_DECAY", 0.95),
                min_lr=1e-7,
                verbose=1
            ),
        ]

        logging.info(f"   [LSTM] Starting training with {len(X_train_lstm)} train samplefixnd {len(X_val_lstm)} validation samples...")
        history = self.model.fit(
            X_train_lstm, y_train_lstm,
            validation_data=(X_val_lstm, y_val_lstm),
            epochs=200,
            batch_size=ML_CONFIG.get("BATCH_SIZE", 64),
            callbacks=callbacks,
            verbose=1,
            shuffle=False  # Important for time series data
        )
        return history

    # <<< THAY THTON BFunction predict_proba in L P EnsembleModel >>>

# ###  T Function this VO in CLASS LSTMModel ###
    # ### THAY THHON TON Function predict_proba old of N ###
    def predict_proba(self, X):
        """
        Prepare data sequence frominput and predict probability.
        REFACTORED: Added comprehensive error handling.
        """
        try:
            # used self.model (used with class LSTMModel)
            if self.model is None:
                logging.warning("LSTMModel.predict_proba: Model not yet trained")
                return 0.5  # probability trung tnh

            # Chu n bsequence data tDataFrame d u vo
            X_seq = self.prepare_sequences(X, y=None)

            # Check if unable to create sequences (due to insufficient data)
            if X_seq is None or len(X_seq) == 0:
                logging.warning("LSTMModel.predict_proba: Cannot create data sequence")
                return 0.5  # probability trung tnh

            # Equal equal model LSTM d training
            # Return from model ng contains probability cho allc sequence
            predictions = self.model.predict(X_seq, verbose=0)
            
            # processing k t quEqual
            if predictions is None or len(predictions) == 0:
                logging.warning("LSTMModel.predict_proba: Model trvk t quempty")
                return 0.5
            
            # Tr v probability cung (t array hoc scalar)
            if isinstance(predictions, np.ndarray):
                if len(predictions.shape) > 1:
                    # if l 2D array, l y ph n tcu cng
                    return float(predictions[-1, -1])
                else:
                    # if l 1D array, l y ph n tcu cng
                    return float(predictions[-1])
            else:
                # if l scalar
                return float(predictions)
                
        except Exception as e:
            logging.error(f"LSTMModel.predict_proba: Li nghim trng - {e}")
            return 0.5  # probability trung tnh an ton

# L p this not thay d i
class EnsembleModel:
    def __init__(self):
        self.validation_threshold = 0.02  # Ultra-strict: from test results
        self.early_stopping_patience = 3   # Very early stopping
        self.regularization_strength = 0.15  # Very sin regularization from test results
        self.dropout_rate = 0.7  # Very high dropout from test results
        self.batch_normalization = True
        self.data_augmentation = True
        self.cross_validation_folds = 10  # More folds from test results
        self.out_of_sample_testing = True
        self.max_depth_limit = 4  # Very shallow trees from test results
        self.min_samples_split_limit = 50  # Very high minimum from test results
        self.min_samples_leaf_limit = 25   # Very high minimum from test results
        self.max_features_limit = 0.3  # Very few features from test results
        self.cpu_threshold = 80  # CPU optimization from test results
        self.memory_threshold = 85  # Memory optimization from test results
        self.alert_limit = 5  # Reduced alert limit from test results
        self.overfitting_detection = True  # Enable overfitting detection
        self.validation_requirements = "STRICT"  # Strict validation
        self.model_weights = {}  # Dictionary dluu trweights of modelsodels
    def _objective(self, trial, X, y, model_name):
        """
        Function m fromiu dOptuna ti uu ha.
        PHIN B N NNG C P: Bsung Regularization (reg_alpha, reg_lambda) dch ng overfitting.
        """
        try:
            # Debug logging
            logging.info(f"      -> _objective called for {model_name} with X.shape={X.shape}, y.shape={y.shape}")

            # Tch ring numeric v categorical columns
            numeric_columns = X.select_dtypes(include=[np.number]).columns
            categorical_columns = X.select_dtypes(include=['object', 'category']).columns

            # ChUsing numeric columns cho tree-based models
            X_numeric = X[numeric_columns] if len(numeric_columns) > 0 else X

            logging.info(f"      -> Numeric columns: {len(numeric_columns)}, Categorical: {len(categorical_columns)}")

            if model_name == "xgboost" or model_name == "xgb":
                params = {
                    "objective": "binary:logistic", "eval_metric": "logloss",
                    "n_estimators": trial.suggest_int("n_estimators", 300, 1000, step=50),
                    "max_depth": trial.suggest_int("max_depth", 5, 12),
                    "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2),
                    "subsample": trial.suggest_float("subsample", 0.6, 0.9),  # Gi m dch ng overfitting
                    "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 0.9),  # Gi m dch ng overfitting
                    "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.6, 0.9),  # Gi m dch ng overfitting
                    "gamma": trial.suggest_float("gamma", 0.0, 5.0),  # Tang dch ng overfitting
                    "min_child_weight": trial.suggest_int("min_child_weight", 5, 25),  # Tang dch ng overfitting
                    # === ENHANCED REGULARIZATION - CH NG OVERFITTING ===
                    "reg_alpha": trial.suggest_float("reg_alpha", 0.1, 5.0),  # Tang L1 regularization
                    "reg_lambda": trial.suggest_float("reg_lambda", 0.1, 5.0),  # Tang L2 regularization
                    "max_delta_step": trial.suggest_int("max_delta_step", 0, 3),  # Reduce to more reasonable
                    "scale_pos_weight": trial.suggest_float("scale_pos_weight", 0.8, 1.2),  # Class balancing
                    "max_leaves": trial.suggest_int("max_leaves", 0, 50),  # Gi i h n leaves
                    # ===================================================
                    "random_state": 42, "n_jobs": -1
                }
                # TFunction early stopping cho XGBoost
                model = xgb.XGBClassifier(**params)
                # Sd Using in training with early_stopping_rounds

            elif model_name == "lightgbm" or model_name == "lgb":
                params = {
                    "objective": "binary", "metric": "binary_logloss",
                    "n_estimators": trial.suggest_int("n_estimators", 300, 1000, step=50),
                    "max_depth": trial.suggest_int("max_depth", 5, 12),
                    "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2),
                    "num_leaves": trial.suggest_int("num_leaves", 20, 100),  # Gi m dch ng overfitting
                    "subsample": trial.suggest_float("subsample", 0.6, 0.9),  # Gi m dch ng overfitting
                    "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 0.9),  # Gi m dch ng overfitting
                    "min_child_samples": trial.suggest_int("min_child_samples", 10, 50),  # Tang dch ng overfitting
                    "min_child_weight": trial.suggest_float("min_child_weight", 0.1, 10.0),  # Tang dch ng overfitting
                    # === ENHANCED REGULARIZATION - CH NG OVERFITTING ===
                    "reg_alpha": trial.suggest_float("reg_alpha", 0.1, 5.0),  # Tang L1 regularization
                    "reg_lambda": trial.suggest_float("reg_lambda", 0.1, 5.0),  # Tang L2 regularization
                    "feature_fraction": trial.suggest_float("feature_fraction", 0.6, 0.9),  # Gi m dch ng overfitting
                    "bagging_fraction": trial.suggest_float("bagging_fraction", 0.6, 0.9),  # Gi m dch ng overfitting
                    "bagging_freq": trial.suggest_int("bagging_freq", 1, 5),  # Gi m dch ng overfitting
                    "max_bin": trial.suggest_int("max_bin", 200, 500),  # Gi i h n bins
                    "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 10, 50),  # Tang dch ng overfitting
                    # ===================================================
                    "random_state": 42, "verbose": -1, "n_jobs": -1
                }
                # TFunction early stopping cho LightGBM
                model = lgb.LGBMClassifier(**params)
                # Sd Using in training with early_stopping_rounds

            elif model_name == "rf" or model_name == "random_forest":
                bootstrap = trial.suggest_categorical("bootstrap", [True, False])
                params = {
                    "n_estimators": trial.suggest_int("n_estimators", 200, 600, step=50),  # Gi m dch ng overfitting
                    "max_depth": trial.suggest_int("max_depth", 5, 12),  # Gi m dch ng overfitting
                    "min_samples_split": trial.suggest_int("min_samples_split", 10, 50),  # Tang dch ng overfitting
                    "min_samples_leaf": trial.suggest_int("min_samples_leaf", 5, 25),  # Tang dch ng overfitting
                    "max_features": trial.suggest_categorical("max_features", ["sqrt", "log2", 0.3, 0.5, 0.7]),  # Gi m dch ng overfitting
                    "bootstrap": bootstrap,
                    "max_leaf_nodes": trial.suggest_int("max_leaf_nodes", 10, 100),  # Gi i h n leaves
                    "min_impurity_decrease": trial.suggest_float("min_impurity_decrease", 0.0, 0.01),  # Gi mimpurity
                    "random_state": 42, "n_jobs": -1
                }
                # ChAdd modelax_samples if bootstrap=True
                if bootstrap:
                    params["max_samples"] = trial.suggest_float("max_samples", 0.6, 0.9)  # Gi m dch ng overfitting
                model = RandomForestClassifier(**params)

            elif model_name == "logistic_regression":
                params = {
                    "penalty": trial.suggest_categorical("penalty", ["l1", "l2", "elasticnet"]),
                    "C": trial.suggest_float("C", 0.01, 10.0, log=True),  # Gi m range dch ng overfitting
                    "fit_intercept": trial.suggest_categorical("fit_intercept", [True, False]),
                    "class_weight": trial.suggest_categorical("class_weight", [None, "balanced"]),
                    "max_iter": trial.suggest_int("max_iter", 1000, 3000),  # Gi m dch ng overfitting
                    "tol": trial.suggest_float("tol", 1e-5, 1e-3, log=True)  # Tolerance
                }
                if params.get("penalty") == "l1":
                    params["solver"] = "liblinear"
                elif params.get("penalty") == "elasticnet":
                    params["solver"] = "saga"
                    params["l1_ratio"] = trial.suggest_float("l1_ratio", 0.0, 1.0)
                else:
                    params["solver"] = "lbfgs"
                model = LogisticRegression(**params, random_state=42)

            elif model_name == "knn":
                params = {
                    "n_neighbors": trial.suggest_int("n_neighbors", 5, 50),
                    "weights": trial.suggest_categorical("weights", ["uniform", "distance"]),
                    "metric": trial.suggest_categorical("metric", ["minkowski", "euclidean", "manhattan", "chebyshev"]),
                    "algoritFunction": trial.suggest_categorical("algoritFunction", ["auto", "ball_tree", "kd_tree", "brute"]),
                    "leaf_size": trial.suggest_int("leaf_size", 10, 50),
                    "p": trial.suggest_int("p", 1, 3),  # Power parameter for Minkowski metric
                    "n_jobs": -1
                }
                model = KNeighborsClassifier(**params)
            else:
                logging.warning(f"      -> Unknown model name '{model_name}', returning 0.0")
                return 0.0

            # Using X_numeric cho t t cmodels dtrnh l i string conversion
            # ChSolve encoded features v numeric features
            X_to_use = X_numeric

            # TFunction encoded features if c
            encoded_cols = [col for col in X.columns if col.endswith('_encoded') or
                           col.startswith('asset_class_') or col.startswith('volatility_profile_') or
                           col.startswith('volatility_regime_') or col.startswith('prefers_') or
                           col.startswith('asset_') or col.endswith('_interaction')]

            if encoded_cols:
                encoded_features = X[encoded_cols]
                X_to_use = pd.concat([X_to_use, encoded_features], axis=1)

            #  m b o DataFrame c c u trhas datang cho XGBoost v LightGBM
            X_to_use = X_to_use.copy()  # T o copy dtrnh SettingWithCopyWarning
            X_to_use.columns = X_to_use.columns.astype(str)  #  m b o column names l string
            X_to_use = X_to_use.loc[:, ~X_to_use.columns.duplicated()]  # type bduplicate columns

            # Check has datadata not
            if len(X_to_use.columns) == 0:
                logging.warning(f"      -> No columnfixvailable for {model_name}")
                return 0.0

            logging.info(f"      -> Training {model_name} with {X_to_use.shape[1]} features")

            tscv = TimeSeriesSplit(n_splits=3)
            scores = safe_cross_val_score(model, X_to_use, y, cv=tscv, scoring="f1", n_jobs=-1)

            mean_score = np.mean(scores)
            logging.info(f"      -> {model_name} CV score: {mean_score:.4f}")

            return mean_score

        except Exception as e:
            logging.error(f"      -> Error in _objective for {model_name}: {e}")
            return 0.0

    def evaluate_model_with_purged_cv(self, model, X, y, n_splits=5, embargo=5):
        """
        Enhanced Purgedross-Validation with better anti-overfitting measures
        """
        X_idx = np.arange(len(X))

        # Using PurgedGroupTimeSeriesSplit thay v TimeSeriesSplit thng thu ng
        pgts = PurgedGroupTimeSeriesSplit(n_splits=n_splits, gap=embargo)
        oof_pred = np.full(len(X), np.nan)
        fold_metrics = []

        # TFunction noise injection dregularization
        noise_level = ML_CONFIG.get("NOISE_INJECTION", 0.01)

        for fold_id, (tr, te) in enumerate(pgts.split(X_idx), 1):
            if len(tr) == 0: continue

            X_tr, y_tr = X.iloc[tr], y.iloc[tr]
            X_te, y_te = X.iloc[te], y.iloc[te]

            # TFunction noise injection cho training data dch ng overfitting
            if noise_level > 0:
                noise = np.random.normal(0, noise_level, X_tr.shape)
                X_tr_noisy = X_tr + noise
            else:
                X_tr_noisy = X_tr

            # Clone model dtrnh side effects
            mdl = clone(model)

            # Fit with early stopping if model htr 
            try:
                if hasattr(mdl, 'fit') and hasattr(mdl, 'predict_proba'):
                    # Check type model d p early stopping suitable
                    model_name = mdl.__class__.__name__

                    if model_name in ['XGBClassifier', 'LGBMClassifier']:
                        #  i with XGBoost v LightGBM, early stopping d handle khneedshau
                        try:
                            mdl.fit(X_tr_noisy, y_tr,
                                   eval_set=[(X_te, y_te)],
                                   verbose=False)
                        except TypeError:
                            # Fallback if not htreval_set
                            mdl.fit(X_tr_noisy, y_tr)
                    else:
                        # modelsodel khc
                        mdl.fit(X_tr_noisy, y_tr)

                    # Predict with uncertainty quantification
                    if hasattr(mdl, "predict_proba"):
                        p = mdl.predict_proba(X_te)[:, 1]
                        y_hat = (p >= 0.5).astype(int)
                    else:
                        y_hat = mdl.predict(X_te)
                        p = y_hat.astype(float)

                    # Calculate metrics with class balancing
                    f1 = f1_score(y_te, y_hat, average='weighted')
                    accuracy = accuracy_score(y_te, y_hat)

                    # TFunction precision v recall ddnh gi ton di n
                    try:
                        from sklearn.metrics import precision_score, recall_score
                        precision = precision_score(y_te, y_hat, average='weighted', zero_division=0)
                        recall = recall_score(y_te, y_hat, average='weighted', zero_division=0)
                    except (NameError, ImportError):
                        # Fallback if import not successful
                        precision = 0.0
                        recall = 0.0

                    fold_metrics.append((f1, accuracy, precision, recall))
                    oof_pred[te] = p

            except Exception as e:
                logging.error(f"Error in model training fold {fold_id}: {e}")
                # Use default values if training fails
                fold_metrics.append((0.0, 0.0, 0.0, 0.0))
                oof_pred[te] = np.zeros(len(te))

        if not fold_metrics:
            return {"mean_f1": 0.0, "mean_accuracy": 0.0, "std_f1": 0.0, "std_accuracy": 0.0}

        f1s = [m[0] for m in fold_metrics]
        accuracies = [m[1] for m in fold_metrics]
        precisions = [m[2] for m in fold_metrics]
        recalls = [m[3] for m in fold_metrics]

        results = {
            "mean_f1": float(np.mean(f1s)),
            "mean_accuracy": float(np.mean(accuracies)),
            "mean_precision": float(np.mean(precisions)),
            "mean_recall": float(np.mean(recalls)),
            "std_f1": float(np.std(f1s)),
            "std_accuracy": float(np.std(accuracies)),
            "n_folds": len(fold_metrics),
            "oof_proba": oof_pred,
            "X_train_index_for_oof": X.index
        }
        print(f"   [PurgedCV] F1={results['mean_f1'] or 0.0:.3f}{results['std_f1'] or 0.0:.3f} | "
              f"Accuracy={results['mean_accuracy'] or 0.0:.3f}{results['std_accuracy'] or 0.0:.3f} | embargo={embargo}")
        return results


    # <<< THAY THTON BFunction this in L P EnsembleModel >>>

    def train_ensemble(self, X, y):
        print("🚀 [Ensemble Training] Starting ensemble model training...")
        print(f"   - Input shape: X={X.shape}, y={y.shape}")
        print(f"   - Target classes: {y.value_counts().to_dict()}")

        # Check for potential overfitting indicators
        class_balance = y.value_counts(normalize=True)
        if class_balance.min() < 0.1:  # Less than 10% minority class
            print(f" [Overfitting Warning] Class imbalance detected: {class_balance.to_dict()}")
        
        if len(X) < 1000:
            print(f" [Overfitting Warning] Small dataset size: {len(X)} samples")

        base_model_names = [m for m in ML_CONFIG["ENSEMBLE_MODELS"] if m != "lstm"]
        print(f"   - Base models: {base_model_names}")

        self.models = {}
        self.cv_results = {}
        self.base_model_feature_importance = {}

        # Tch ring numeric v categorical columns
        numeric_columns = X.select_dtypes(include=[np.number]).columns
        categorical_columns = X.select_dtypes(include=['object', 'category']).columns
        X_numeric = X[numeric_columns] if len(numeric_columns) > 0 else X

        print(f"   - Numeric columns: {len(numeric_columns)}")
        print(f"   - Categorical columns: {len(categorical_columns)}")

        # Apply feature selection to preventoverfitting
        if len(numeric_columns) > 50:
            print(" [Anti-Overfitting] Applying feature selection...")
            selector = SelectKBest(k=min(50, len(numeric_columns)))
            X_selected = selector.fit_transform(X_numeric, y)
            selected_features = numeric_columns[selector.get_support()]
            X_numeric = pd.DataFrame(X_selected, columns=selected_features, index=X.index)
            print(f"   - Selected features: {len(selected_features)}")

        logging.info("   [Stacking] Starting training of base models (Level 0)...")
        print("🚀 [Ensemble Training] Level 0: Starting base models training...")
        tscv = TimeSeriesSplit(n_splits=ML_CONFIG["CV_N_SPLITS"])
        oof_predictions = {}

        # Chu n bdata cho t t cmodels
        numeric_columns = X_numeric.columns
        X_numeric = X_numeric

        # TFunction encoded features if c
        encoded_cols = [col for col in X.columns if col.endswith('_encoded') or
                       col.startswith('asset_class_') or col.startswith('volatility_profile_') or
                       col.startswith('volatility_regime_') or col.startswith('prefers_') or
                       col.startswith('asset_') or col.endswith('_interaction')]

        if encoded_cols:
            encoded_features = X[encoded_cols]
            X_to_use = pd.concat([X_numeric, encoded_features], axis=1)
        else:
            X_to_use = X_numeric

        #  m b o DataFrame c c u trhas datang cho XGBoost
        X_to_use = X_to_use.copy()  # T o copy dtrnh SettingWithCopyWarning
        X_to_use.columns = X_to_use.columns.astype(str)  #  m b o column names l string
        X_to_use = X_to_use.loc[:, ~X_to_use.columns.duplicated()]  # type bduplicate columns

        for name in base_model_names:
            logging.info(f"      -> Training and getting OOF predictions for {name}...")
            model = self._get_optimized_model(name, X, y)
            oof_preds_for_model = np.full(len(X), np.nan)

            for train_idx, val_idx in tscv.split(X_to_use):
                X_train, y_train = X_to_use.iloc[train_idx], y.iloc[train_idx]
                X_val = X_to_use.iloc[val_idx]
                model_clone = clone(model)
                model_clone.fit(X_train, y_train)
                oof_preds_for_model[val_idx] = model_clone.predict_proba(X_val)[:, 1]

            oof_predictions[name] = oof_preds_for_model
            model.fit(X_to_use, y)
            self.models[name] = model
            self.cv_results[name] = self.evaluate_model_with_purged_cv(model, X_to_use, y)

            if hasattr(model, 'feature_importances_'):
                # Using used columns cho feature importance
                feature_columns = X_to_use.columns
                importances = pd.Series(model.feature_importances_, index=feature_columns)
                self.base_model_feature_importance[name] = importances.nlargest(5).to_dict()

        # === Level 1: Meta-Model tOOF ===
        meta_features_df = pd.DataFrame(oof_predictions, index=X.index).dropna()
        y_for_meta = y.reindex(meta_features_df.index)

        logging.info("\n   [Stacking] Starting Meta-Model training and evaluation (Level 1)...")
        meta_model_for_scoring = LogisticRegression()
        meta_score = np.mean(
            safe_cross_val_score(meta_model_for_scoring, meta_features_df, y_for_meta, cv=tscv, scoring="f1")
        )
        logging.info(f"   [Stacking] Meta-Model F1-Score (cross-validated): {meta_score:.4f}")

        # === Enhanced Stacking (Level 1) ===
        logging.info("   [Enhanced Stacking] Starting Level 1: Meta-learner training...")
        print("🚀 [Ensemble Training] Level 1: Starting meta-learners training...")
        self._train_stacking_model(X, y, oof_predictions)

        # === Level 2: Calibration (Isotonic/Platt) gitr t tth i gian ===
        logging.info("   [Stacking] Starting Level 2: Probability calibration...")
        print("🚀 [Ensemble Training] Level 2: Starting calibration training...")

        # Chia theo th i gian: 80% train meta, 20% calibrate
        split_idx = int(len(meta_features_df) * 0.8)
        if split_idx < 1 or split_idx >= len(meta_features_df):
            # Fallback an ton if data qu t
            split_idx = max(1, len(meta_features_df) - 1)

        X_meta_train = meta_features_df.iloc[:split_idx]
        y_meta_train = y_for_meta.iloc[:split_idx]
        X_meta_cal   = meta_features_df.iloc[split_idx:]
        y_meta_cal   = y_for_meta.iloc[split_idx:]

        # Fifrom modeleta-estimator trn ph n "train" th i gian before with regularization
        base_meta = LogisticRegression(max_iter=1000, C=0.1, penalty='l2')  # Add regularization
        base_meta.fit(X_meta_train, y_meta_train)

        # Hi u ch nh probability trn ph n "calibration" th i gian sau
        logging.info("   [Stacking] Starting probability calibration (Level 2: Calibration)...")
        try:
            # Use Platt scaling instead of isotonic for better generalization
            calibrated = create_calibrated_classifier(base_meta, method='sigmoid', cv='prefit')
            calibrated.fit(X_meta_cal, y_meta_cal)  # <== BU C QUAN Tempty: th c straining Level 2
            self.meta_model = calibrated
            logging.info("   [Stacking] Calibration (sigmoid, prefit) completed.")
            print(" [Ensemble Training] Level 2 completed: Calibration (sigmoid, prefit)")
        except Exception as e:
            # Fallback: used k-foldalibration if prefit th t b i (data t/kh)
            logging.warning(f"   [Stacking]  Calibration prefit error: {e}. Using cv=3 for auto fit + calibrate.")
            calibrated = create_calibrated_classifier(LogisticRegression(max_iter=1000, C=0.1),
                                        method='sigmoid', cv=3)
            calibrated.fit(meta_features_df, y_for_meta)
            self.meta_model = calibrated
            logging.info("   [Stacking] Calibration (isotonic, cv=3) completed.")
            print(" [Ensemble Training] Level 2 completed: Calibration (isotonic, cv=3)")

        logging.info("Stacking ensemble training (Level 0 + Level 1 + Level 2) completed!")
        print(" [Ensemble Training] Hon thnh training ensemble model!")
        print(" [Tm t t Training Results]:")
        print(f"    Level 0 (Base Models): {list(self.models.keys())}")
        print(f"    Level 1 (Meta Learners): {list(self.meta_learners.keys()) if hasattr(self, 'meta_learners') else 'None'}")
        print(f"    Level 2 (Calibration): {'Completed' if hasattr(self, 'meta_model') else 'Failed'}")
        print(f"    CV Results: {list(self.cv_results.keys())}")

    def _train_stacking_model(self, X, y, oof_predictions):
        """Enhanced stacking with multiple meta-learners"""
        if not ML_CONFIG.get("MODEL_STACKING_ENABLED", True):
            return

        logging.info("   [Enhanced Stacking] Training advanced meta-learners...")

        # Prepare meta-features
        meta_features = np.column_stack(list(oof_predictions.values()))
        meta_features_df = pd.DataFrame(meta_features, columns=list(oof_predictions.keys()))

        # Train multiple meta-learners
        meta_learners = {}

        # 1. XGBoosfrom modeleta-learner
        try:
            xgb_meta = xgb.XGBClassifier(
                n_estimators=100,
                max_depth=4,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1
            )
            xgb_meta.fit(meta_features_df, y)
            meta_learners['xgb_meta'] = xgb_meta
        except Exception as e:
            logging.warning(f"XGBoosfrom modeleta-learner failed: {e}")

        # 2. LightGBM meta-learner
        try:
            lgb_meta = lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=4,
                learning_rate=0.1,
                random_state=42,
                verbose=-1,
                n_jobs=-1
            )
            lgb_meta.fit(meta_features_df, y)
            meta_learners['lgb_meta'] = lgb_meta
        except Exception as e:
            logging.warning(f"LightGBM meta-learner failed: {e}")

        # 3. Logistic Regression meta-learner
        try:
            # Handle NaN values before training
            meta_features_clean = meta_features_df.fillna(0.0)

            lr_meta = LogisticRegression(
                C=1.0,
                random_state=42,
                max_iter=1000
            )
            lr_meta.fit(meta_features_clean, y)
            meta_learners['lr_meta'] = lr_meta
        except Exception as e:
            logging.warning(f"Logistic Regression meta-learner failed: {e}")

        # Store meta-learners
        self.meta_learners = meta_learners
        logging.info(f"   [Enhanced Stacking] Trained {len(meta_learners)} meta-learners")
        print(f" [Ensemble Training] Level 1 completed: {len(meta_learners)} meta-learners trained")

    # <<< TFunction Function M I this VO in L P EnsembleModel >>>
    def get_base_model_feature_influence(self):
        """
        Summary  nh hu ng of all feature tallc model base .
        """
        if not self.base_model_feature_importance:
            return "No feature influence data available."

        # Summary importance tallc model base 
        combined_influence = {}
        for model_name, features in self.base_model_feature_importance.items():
            for feature, importance in features.items():
                if feature not in combined_influence:
                    combined_influence[feature] = 0.0
                combined_influence[feature] += importance # add to importance score

        # S p x p v l y top 5 feature c  nh hu ng nh t trn ton bmodelsodel base 
        sorted_influence = sorted(combined_influence.items(), key=lambda item: item[1], reverse=True)
        top_5_text = ", ".join([f"{feat.replace('_', ' ')} ({val:.2f})" for feat, val in sorted_influence[:5]])
        return top_5_text

    # <<< TFunction Function HELPER this VO L P EnsembleModel >>>
    def _get_optimized_model(self, name, X, y):
        """Helper function to run Optuna and return model with best parameters."""
        logging.info(f"      -> Optimizing for {name}...")

        # Enhanced studuc configuration with fallback for missing integrations
        sampler = None
        pruner = None

        # Check v Using TPESampler if c
        if OPTUNA_CONFIG.get("SAMPLER") == "TPE" and OPTIONAL_PACKAGES.get('optuna_integrations', False):
            try:
                from optuna.samplers import TPESampler
                sampler = TPESampler(seed=42)
            except ImportError:
                logging.warning("TPESampler not available, using default sampler")

        # Check v Using MedianPruner if c
        if OPTUNA_CONFIG.get("PRUNING_ENABLED", True) and OPTIONAL_PACKAGES.get('optuna_integrations', False):
            try:
                from optuna.pruners import MedianPruner
                pruner = MedianPruner()
            except ImportError:
                logging.warning("MedianPruner not available, using default pruner")

        # To study vi SQLite storage d luu tr
        storage_url = OPTUNA_CONFIG.get("STORAGE_URL", "sqlite:///optuna_study.db")
        study_name = f"{OPTUNA_CONFIG.get('STUDY_NAME', 'trading_bot')}_{name}"
        
        # Using OptunaStudyManager
        study_manager = OptunaStudyManager(storage_url)
        study = study_manager.create_or_load_study(
            study_name=study_name,
            direction="maximize",
            sampler=sampler,
            pruner=pruner
        )
        try:
            study.optimize(
                lambda trial: self._objective(trial, X, y, name),
                n_trials=OPTUNA_CONFIG["N_TRIALS"],
                timeout=OPTUNA_CONFIG["TIMEOUT_SEC"],
            )
            best_params = study.best_params
        except Exception as e: # <<< TFunction EXCEPT
            print(f"   [Optuna Warning] Li in qu trnh ti uu ha cho {name}: {e}. Using tham s mc dnh data nh.")
            best_params = {} # Using dict empty dmodel used tham s mc dnh data nh

        if name == "xgboost" or name == "xgb":
            # Ensure random_state is not duplicated
            if "random_state" in best_params:
                best_params.pop("random_state")
            return xgb.XGBClassifier(**best_params, random_state=42)
        elif name == "lightgbm" or name == "lgb":
            # Ensure random_state and verbose are not duplicated
            if "random_state" in best_params:
                best_params.pop("random_state")
            if "verbose" in best_params:
                best_params.pop("verbose")
            return lgb.LGBMClassifier(**best_params, random_state=42, verbose=-1)
        elif name == "rf" or name == "random_forest":
            # Ensure random_state is not duplicated
            if "random_state" in best_params:
                best_params.pop("random_state")
            return RandomForestClassifier(**best_params, random_state=42)
        elif name == "logistic_regression":
            if best_params.get("penalty") == "l1": best_params["solver"] = "liblinear"
            else: best_params["solver"] = "lbfgs"
            # Ensure max_iter is set if not already in best_params
            if "max_iter" not in best_params:
                best_params["max_iter"] = 1000
            # Ensure random_state is not duplicated
            if "random_state" in best_params:
                best_params.pop("random_state")
            return LogisticRegression(**best_params, random_state=42)
        elif name == "knn":
            # Ensure n_jobs is not duplicated
            if "n_jobs" in best_params:
                best_params.pop("n_jobs")
            return KNeighborsClassifier(**best_params, n_jobs=-1)
        else:
            # Fallback: return a default RandomForestClassifier if name is not recognized
            logging.warning(f"Unknown model name '{name}', using RandomForestClassifier as fallback")
            return RandomForestClassifier(random_state=42)

# ###  T Function this VO in CLASS EnsembleModel ###
    # ### THAY THHON TON Function predict_proba old of N ###
    def predict_proba(self, X, feature_columns=None):
        """
        Equal probability cu cng equal allh l y trung bnh fromempty s .
        REFACTORED: TFunction error handling ton di n v processing edge cases.
        """
        try:
            # --- BU C 1: processing data d u vo ---
            # Check if X empty Or not must l DataFrame
            if not isinstance(X, pd.DataFrame) or X.empty:
                logging.warning("EnsembleModel.predict_proba: data đầu vào not hợp lệ ")
                return 0.5

            # l+m sߦch data m t l n duy nh t t i duc
            X_clean = X.copy()
            X_clean.replace([np.inf, -np.inf], np.nan, inplace=True)
            # Using ffill/bfill dprocessing cneedsaN cn st l i, sau d di n equal 0
            X_clean.fillna(method='ffill', inplace=True)
            X_clean.fillna(method='bfill', inplace=True)
            X_clean.fillna(0, inplace=True)

            # --- BU C 2: L Y DON TmodelsODEL base---
            base_predictions = {}
            for name, model in self.models.items():
                if model is None: 
                    continue
                try:
                    # Chneeds used data cu cng cho Equal live
                    X_pred = X_clean.tail(1)
                    
                    # Equal with model con
                    if hasattr(model, 'predict_proba'):
                        pred_proba = None
                        try:
                            # KH C PH C L I:  m b o feature ordering nh t qun
                            if feature_columns is not None:
                                # T o DataFrame with used thtfrom nhu khi training
                                X_clean_ordered = pd.DataFrame(index=X_pred.index)
                                
                                # TFunction t t cfeatures theo used tht 
                                for col in feature_columns:
                                    if col in X_pred.columns:
                                        X_clean_ordered[col] = X_pred[col]
                                    else:
                                        # TFunction gi trmc dnh data nh cho missing features
                                        X_clean_ordered[col] = 0.0
                                
                                #  m b o not needsaN values
                                X_clean_ordered = X_clean_ordered.fillna(0.0)
                                
                                # Chuy n d i threceiveumpy array dtrnh feature name issues
                                X_array = X_clean_ordered.values
                                pred_proba = model.predict_proba(X_array)
                            else:
                                # Fallback: Using reindex dd m b o thtfrom
                                if hasattr(model, 'feature_names_in_'):
                                    # Using feature names from model d dufromrain
                                    X_reindexed = X_pred.reindex(columns=model.feature_names_in_, fill_value=0.0)
                                    X_array = X_reindexed.values
                                    pred_proba = model.predict_proba(X_array)
                                else:
                                    # Fallback cu cng
                                    X_array = X_pred.values
                                    pred_proba = model.predict_proba(X_array)
                        except Exception as feature_error:
                            # Chlog warning m t l n dtrnh spam
                            if not hasattr(self, '_feature_error_logged'):
                                logging.warning(f"Feature ordering failed for {name}: {feature_error}")
                                self._feature_error_logged = True
                            try:
                                # Fallback 1: Using X_clean g c
                                pred_proba = model.predict_proba(X_pred)
                            except Exception as fallback_error:
                                logging.error(f"Fallback prediction failed for {name}: {fallback_error}")
                                pred_proba = None
                        
                        if pred_proba is not None:
                            # processing k t quEqual
                            if hasattr(pred_proba, 'shape') and len(pred_proba.shape) > 1:
                                pred_proba = pred_proba[0, 1] if pred_proba.shape[1] > 1 else pred_proba[0, 0]
                            else:
                                pred_proba = float(pred_proba[0]) if hasattr(pred_proba, '__len__') else float(pred_proba)
                            
                            base_predictions[name] = pred_proba
                        else:
                            # Chlog warning m t l n dtrnh spam
                            if not hasattr(self, '_ensemble_warning_logged'):
                                logging.warning(f"EnsembleModel.predict_proba: Model {name} trߦ v+ None")
                                self._ensemble_warning_logged = True
                    else:
                        # Fallback cho model not c predict_proba
                        pred = model.predict(X_pred)
                        base_predictions[name] = float(pred[0]) if hasattr(pred, '__len__') else float(pred)
                        
                except Exception as e:
                    logging.error(f"EnsembleModel.predict_proba: Li with model {name}: {e}")
                    continue

            # --- BU C 3: TNH TON TRUNG BNH fromempty S---
            if not base_predictions:
                logging.warning("EnsembleModel.predict_proba: No valid predictions from this model")
                return 0.5

            # Calculate weighted average
            weights = {'rf': 0.3, 'xgb': 0.4, 'lgb': 0.3}  # Default weights
            weighted_sum = 0.0
            total_weight = 0.0
            
            for name, pred in base_predictions.items():
                weight = weights.get(name, 0.33)  # Tempty smc dnh data nh
                weighted_sum += pred * weight
                total_weight += weight
            
            final_prediction = weighted_sum / total_weight if total_weight > 0 else 0.5
            
            #  m b o k t quin kho ng [0, 1]
            final_prediction = max(0.0, min(1.0, final_prediction))
            
            return final_prediction
            
        except Exception as e:
            logging.error(f"EnsembleModel.predict_proba: Li nghim trng - {e}")
            return 0.5

    def predict_proba_on_df(self, X, feature_columns=None):
        """
        Equal probability cho TON BDataFrame d u vo (used cho training).
        Return from model ng numpy c cng ddi with X.
        REFACTORED: TFunction error handling ton di n v feature ordering.
        """
        try:
            if not isinstance(X, pd.DataFrame) or X.empty:
                logging.warning("EnsembleModel.predict_proba_on_df: data đầu vào not hợp lệ ")
                return np.array([0.5] * len(X))

            # 1. l+m sߦch data
            X_clean = X.copy()
            X_clean.replace([np.inf, -np.inf], np.nan, inplace=True)
            X_clean.fillna(method='ffill', inplace=True)
            X_clean.fillna(method='bfill', inplace=True)
            X_clean.fillna(0, inplace=True)

            # 2. Equal with total model con
            predictions = []
            for name, model in self.models.items():
                if model is None:
                    continue
                try:
                    if hasattr(model, 'predict_proba'):
                        pred_proba = None
                        try:
                            # KH C PH C L I:  m b o feature ordering nh t qun
                            if feature_columns is not None:
                                # T o DataFrame with used thtfrom nhu khi training
                                X_clean_ordered = pd.DataFrame(index=X_clean.index)
                                
                                # TFunction t t cfeatures theo used tht 
                                for col in feature_columns:
                                    if col in X_clean.columns:
                                        X_clean_ordered[col] = X_clean[col]
                                    else:
                                        # TFunction gi trmc dnh data nh cho missing features
                                        X_clean_ordered[col] = 0.0
                                
                                #  m b o not needsaN values
                                X_clean_ordered = X_clean_ordered.fillna(0.0)
                                
                                # Chuy n d i threceiveumpy array dtrnh feature name issues
                                X_array = X_clean_ordered.values
                                pred_proba = model.predict_proba(X_array)
                            else:
                                # Fallback: Using reindex dd m b o thtfrom
                                if hasattr(model, 'feature_names_in_'):
                                    # Using feature names from model d dufromrain
                                    X_reindexed = X_clean.reindex(columns=model.feature_names_in_, fill_value=0.0)
                                    X_array = X_reindexed.values
                                    pred_proba = model.predict_proba(X_array)
                                else:
                                    # Fallback cu cng
                                    X_array = X_clean.values
                                    pred_proba = model.predict_proba(X_array)
                        except Exception as feature_error:
                            # Chlog warning m t l n dtrnh spam
                            if not hasattr(self, '_feature_error_logged_df'):
                                logging.warning(f"Feature ordering failed for {name}: {feature_error}")
                                self._feature_error_logged_df = True
                            try:
                                # Fallback 1: Using X_clean g c
                                pred_proba = model.predict_proba(X_clean)
                            except Exception as fallback_error:
                                logging.error(f"Fallback prediction failed for {name}: {fallback_error}")
                                pred_proba = None
                        
                        if pred_proba is not None:
                            # processing k t quEqual
                            if hasattr(pred_proba, 'shape') and len(pred_proba.shape) > 1:
                                if pred_proba.shape[1] > 1:
                                    pred_proba = pred_proba[:, 1]  # L y probability class 1
                                else:
                                    pred_proba = pred_proba[:, 0]  # L y probability class 0
                            else:
                                pred_proba = pred_proba.flatten()
                            
                            predictions.append(pred_proba)
                        else:
                            # Chlog warning m t l n dtrnh spam
                            if not hasattr(self, '_ensemble_warning_logged_df'):
                                logging.warning(f"EnsembleModel.predict_proba_on_df: Model {name} trߦ v+ None")
                                self._ensemble_warning_logged_df = True
                            predictions.append(np.full(len(X_clean), 0.5))
                    else:
                        # Fallback cho model not c predict_proba
                        pred = model.predict(X_clean)
                        predictions.append(pred.flatten())
                        
                except Exception as e:
                    logging.error(f"EnsembleModel.predict_proba_on_df: Li with model {name}: {e}")
                    predictions.append(np.full(len(X_clean), 0.5))
                    continue

            # 3. Calculate weighted average
            if not predictions:
                logging.warning("EnsembleModel.predict_proba_on_df: No valid predictions from this model")
                return np.full(len(X_clean), 0.5)

            # Calculate weighted average
            weights = {'rf': 0.3, 'xgb': 0.4, 'lgb': 0.3}  # Default weights
            weighted_sum = np.zeros(len(X_clean))
            total_weight = 0.0
            
            for i, pred in enumerate(predictions):
                model_name = list(self.models.keys())[i] if i < len(self.models) else f"model_{i}"
                weight = weights.get(model_name, 0.33)  # Tempty smc dnh data nh
                weighted_sum += pred * weight
                total_weight += weight
            
            final_predictions = weighted_sum / total_weight if total_weight > 0 else np.full(len(X_clean), 0.5)
            
            #  m b o k t quin kho ng [0, 1]
            final_predictions = np.clip(final_predictions, 0.0, 1.0)
            
            return final_predictions
            
        except Exception as e:
            logging.error(f"EnsembleModel.predict_proba_on_df: Li nghim trng - {e}")
            return np.full(len(X) if hasattr(X, '__len__') else 1, 0.5)

class EnhancedDataManager:
    def __init__(self, news_manager=None):
        # Store news_manager reference
        self.news_manager = news_manager
        
        # Initialize feature engineer
        self.feature_engineer = AdvancedFeatureEngineer()
        
        # Performance optimization: Cache for feature creation
        self._feature_cache = {}
        self._cache_ttl = 300  # 5 minutes cache TTL
        self._max_cache_size = 50  # Maximum number of cached features
        
        # <<< TreceiveANG M I: Qu n l symbols needs retrain >>>
        self.stale_symbols = set()  # Symbols with stale data
        self.retrain_reasons = {}   # L do needs retrain

        # Initialize asset class configurations
        self.asset_class_configs = self._initialize_asset_class_configs()

        # Data freshness settings
        self.data_freshness_thresholds = {
            "H1": 300,  # 5 minutes
            "H4": 900,  # 15 minutes
            "D1": 3600,  # 1 hour
            "W1": 14400  # 4 hours
        }
    
    def _clean_feature_cache(self):
        """Clean oldache entries to prevenfrom modelemory leaks"""
        import time
        current_time = time.time()
        keys_to_remove = []
        
        for key in self._feature_cache.keys():
            # Extract timestamp frolevelache key
            try:
                timestamp = int(key.split('_')[-1]) * self._cache_ttl
                if current_time - timestamp > self._cache_ttl * 2:  # Keep 2 TTL periods
                    keys_to_remove.append(key)
            except:
                keys_to_remove.append(key)  # Remove malformed keys
        
        for key in keys_to_remove:
            del self._feature_cache[key]
        
        # Also limit cache size
        if len(self._feature_cache) > self._max_cache_size:
            # Remove oldest entries
            oldest_keys = sorted(self._feature_cache.keys())[:-self._max_cache_size]
            for key in oldest_keys:
                del self._feature_cache[key]
    def _initialize_asset_class_configs(self):
        """Initialize asset class specific configurations"""
        return {
            "equity_index": {
                "min_candles": 100,
                "max_candles": 5000,
                "session_filter": True,
                "gap_handling": "forward_fill",
                "volatility_adjustment": True,
                "news_sensitivity": "high"
            },
            "commodity": {
                "min_candles": 50,  # Reduced from 200 to allow analysis with less data
                "max_candles": 10000,
                "session_filter": False,
                "gap_handling": "interpolate",
                "volatility_adjustment": True,
                "news_sensitivity": "very_high"
            },
            "forex": {
                "min_candles": 50,  # Reduced from 150 to allow analysis with less data
                "max_candles": 3000,
                "session_filter": True,
                "gap_handling": "forward_fill",
                "volatility_adjustment": False,
                "news_sensitivity": "medium"
            },
            "cryptocurrency": {
                "min_candles": 50,
                "max_candles": 2000,
                "session_filter": False,
                "gap_handling": "none",
                "volatility_adjustment": True,
                "news_sensitivity": "very_high"
            }
        }

    def _get_symbol_config(self, symbol):
        """Get configuration for specific symbol"""
        metadata = SYMBOL_METADATA.get(symbol, {})
        asset_class = metadata.get("asset_class", "equity_index")
        return self.asset_class_configs.get(asset_class, self.asset_class_configs["equity_index"])

    def _get_optimal_candle_count(self, symbol):
        """Getoptimal candle count based on symbol characteristics"""
        config = self._get_symbol_config(symbol)
        metadata = SYMBOL_METADATA.get(symbol, {})

        # Base count frolevelonfig
        base_count = config["max_candles"]

        # Adjust based on volatility profile
        volatility_profile = metadata.get("volatility_profile", "medium")
        if volatility_profile == "very_high":
            base_count = min(base_count, 1500)  # Less history for very volatile assets
        elif volatility_profile == "high":
            base_count = min(base_count, 2500)
        elif volatility_profile == "low":
            base_count = min(base_count, 5000)  # More history for stable assets

        return base_count

    def fetch_multi_timeframe_data(self, symbol, count=None, timeframes_to_use=None):
        """Enhanced data fetching with asset class optimization"""
        all_data = {}

        # Getoptimal candle count for symbol
        if count is None:
            count = self._get_optimal_candle_count(symbol)

        # Get symbol-specific configuration
        config = self._get_symbol_config(symbol)
        metadata = SYMBOL_METADATA.get(symbol, {})

        # Ch n primary theo symbol; if not fromh used default
        if timeframes_to_use is None:
            primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, PRIMARY_TIMEFRAME_DEFAULT)
            timeframes_to_use = TIMEFRAME_SET_BY_PRIMARY.get(primary_tf, ["H4", "D1", "W1"])

        # Map granularity chu n OANDA (Monthly used "M", not must "M1")
        granularity_map = {
            "M15": "M15", "M30": "M30",
            "H1": "H1",   "H4": "H4",
            "D1": "D",    "W1": "W",
            "MN1": "M"    # if sau this b n mu n used monthly, d t TF l "MN1"
        }

        for timeframe in timeframes_to_use:
            if timeframe not in granularity_map:
                continue
            try:
                # Get instrumenfrom modelapping with enhanced symbol support
                instrument = self._get_oanda_instrument(symbol)

                granularity = granularity_map[timeframe]
                url = f"{OANDA_URL}/instruments/{instrument}/candles"
                headers = {"Authorization": f"Bearer {OANDA_API_KEY}", "Content-Type": "application/json"}
                params = {"count": count, "granularity": granularity, "price": "M"}

                print(f"   --> Fetching: {instrument} | TF: {timeframe} | Count: {count}")
                response = requests.get(url, headers=headers, params=params, timeout=30)
                if response.status_code != 200:
                    print(f"API {response.status_code} {instrument} {timeframe}: {response.text}")
                    continue

                candles_raw = response.json().get("candles", [])
                print(f"   --> Received {len(candles_raw)} candles ({instrument} {timeframe}).")
                if not candles_raw:
                    continue

                candles = [
                    {
                        "time": pd.to_datetime(c["time"]),
                        "open": float(c["mid"]["o"]),
                        "high": float(c["mid"]["h"]),
                        "low":  float(c["mid"]["l"]),
                        "close":float(c["mid"]["c"]),
                        "volume": c.get("volume", 1000),
                    }
                    for c in candles_raw
                    if c.get("complete", True)
                ]

                # Asset class specific validation - reduced minimum requirement for better flexibility
                min_required = max(10, config["min_candles"] // 5)  # Reduce minimum requirement significantly
                if len(candles) < min_required:
                    print(f" Only {len(candles)} complete candles for {instrument} {timeframe}. Required: {min_required}. Skip.")
                    continue

                df = pd.DataFrame(candles).set_index("time")
                df.index = df.index.tz_convert(pytz.timezone("Asia/Bangkok"))

                # Apply asset class specific preprocessing
                df = self._apply_asset_class_preprocessing(df, symbol, config)

                all_data[timeframe] = df
            except Exception as e:
                print(f"Unexpected error {symbol} {timeframe}: {e}")
        return all_data

    def _get_oanda_instrument(self, symbol):
        """Enhanced instrumenfrom modelapping with all supported symbols"""
        instrument_map = {
            # Commodities
            "XAUUSD": "XAU_USD",
            "XAGUSD": "XAG_USD",

            # US Indices
            "SPX500": "SPX500_USD",
            "NAS100": "NAS100_USD",
            "US30": "US30_USD",

            # European Indices
            "DE40": "DE30_EUR",
            "UK100": "UK100_GBP",
            "FR40": "FR40_EUR",

            # Asian Indices
            "JP225": "JP225_USD",
            "HK50": "HK33_HKD",
            "AU200": "AU200_AUD",

            # Energy
            "USOIL": "WTICO_USD",
            "WTICO_USD": "WTICO_USD",
            "BCO_USD": "BCO_USD",
        }

        if symbol in instrument_map:
            return instrument_map[symbol]
        else:
            # Default logic for forex pairs
            return f"{symbol[:3]}_{symbol[3:]}"

    def _apply_asset_class_preprocessing(self, df, symbol, config):
        """Apply asset class specific preprocessing"""
        metadata = SYMBOL_METADATA.get(symbol, {})

        # Handle gaps based on asset class
        gap_handling = config["gap_handling"]
        if gap_handling == "forward_fill":
            df = df.fillna(method="ffill")
        elif gap_handling == "interpolate":
            df = df.interpolate(method="linear")
        # For "none", no gap handling (crypto)

        # Add asset class specific features
        df["asset_class"] = metadata.get("asset_class", "equity_index")
        df["volatility_profile"] = metadata.get("volatility_profile", "medium")
        df["pip_value"] = metadata.get("pip_value", 1.0)

        # Session-based filtering for applicable assets
        if config["session_filter"] and metadata.get("trading_hours"):
            df = self._apply_session_filter(df, metadata["trading_hours"])

        return df

    def _apply_session_filter(self, df, trading_hours):
        """Filter data to trading hours only"""
        try:
            # Convert to trading timezone
            timezone = trading_hours.get("timezone", "UTC")
            df_tz = df.index.tz_convert(timezone)

            start_hour = trading_hours.get("start", 0)
            end_hour = trading_hours.get("end", 23)

            # Create boolean mask for trading hours
            if start_hour <= end_hour:
                mask = (df_tz.hour >= start_hour) & (df_tz.hour < end_hour)
            else:
                # Handle overnight sessions (e.g., forex)
                mask = (df_tz.hour >= start_hour) | (df_tz.hour < end_hour)

            return df[mask]
        except Exception as e:
            logging.warning(f"Session filtering failed: {e}")
            return df

    # <<< TreceiveANG M I: all Function htrauto-retrain >>>
    def _mark_symbol_for_retrain(self, symbol, reason):
        """Mark symbol for retrain due to stale data"""
        logger = logging.getLogger('AutoRetrain')
        self.stale_symbols.add(symbol)
        self.retrain_reasons[symbol] = reason
        logger.warning(f"Marking {symbol} for retrain: {reason}")
        logging.warning(f"🔄 [Auto-Retrain] Marking {symbol} for retrain: {reason}")
        
        # Send Discord notification about retrain requirement
        try:
            message = f"🔄 **SYMBOL RETRAIN REQUIRED**\n"
            message += f"**Symbol:** {symbol}\n"
            message += f"**Reason:** {reason}\n"
            message += f"**Action:** Bot will retrain model automatically\n"
            
            retrain_data = {
                "Symbol": symbol,
                "Reason": reason,
                "Status": "Queued for retraining",
                "Next Action": "Automatic model retraining"
            }
            
            # Get bot instance to send Discord alert
            if hasattr(self, 'bot_instance') and self.bot_instance:
                self.bot_instance.send_discord_alert(message, "SYSTEM", "NORMAL", retrain_data)
        except Exception as e:
            logger.error(f"Failed to send retrain notification: {e}")

    def get_stale_symbols(self):
        """L y danh sch symbols needs retrain"""
        return list(self.stale_symbols)

    def clear_stale_symbols(self):
        """Xa danh sch symbols needs retrain sau khi d retrain"""
        self.stale_symbols.clear()
        self.retrain_reasons.clear()
        logging.info("[Auto-Retrain]  xa danh sch symbols needs retrain")
    
    def clear_cache(self):
        """Clear all feature cache to prepare for retraining"""
        self._feature_cache.clear()
        logging.info(" [Cache] Cleared all feature cache")

    def check_and_trigger_auto_retrain(self, bot_instance):
        """Check v activate auto-retrain if c symbols with stale data"""
        stale_symbols = self.get_stale_symbols()
        if stale_symbols:
            logging.warning(f" [Auto-Retrain] activate retrain cho {len(stale_symbols)} symbols: {stale_symbols}")
            try:
                # Trigger retrain
                bot_instance.load_or_train_models(force_retrain_symbols=stale_symbols)
                logging.info("[Auto-Retrain] Hon thnh retrain cho stale symbols")
            except Exception as e:
                logging.error(f"[Auto-Retrain] Error during retrain: {e}")
        return stale_symbols

    # <<< TFunction Function M I this VO CLASS >>>

    def get_latest_data(self, symbol):
        """L y data mi nh t cho symbol"""
        try:
            # L y data tprimary timeframe
            primary_tf = get_primary_timeframe(symbol)
            multi_tf_data = self.fetch_multi_timeframe_data(symbol, 100, [primary_tf])
            
            if multi_tf_data and primary_tf in multi_tf_data:
                return multi_tf_data[primary_tf].iloc[-1:]  # Trvcandle mi nh t
            return None
        except Exception as e:
            logging.error(f"Error getting latest data for {symbol}: {e}")
            return None

    def get_current_price(self, symbol):
        try:
            granularity_map = {
            "M15": "M15",
            "M30": "M30",
            "H1": "H1",
            "H4": "H4",
            "D1": "D",
            "W1": "W",
            "M1": "M",
            }

            if symbol == "XAUUSD":
                instrument = "XAU_USD"
            elif symbol == "XAGUSD":
                instrument = "XAG_USD"
            elif symbol == "BTCUSD":
                instrument = "BTC_USD"
            elif symbol == "ETHUSD":
                instrument = "ETH_USD"
            elif symbol == "SPX500":
                instrument = "SPX500_USD"
            elif symbol == "NAS100":
                instrument = "NAS100_USD"
            elif symbol == "US30":
                instrument = "US30_USD"
            elif symbol == "UK100":
                instrument = "UK100_GBP"
            elif symbol == "DE40":
                instrument = "DE30_EUR"
            elif symbol == "JP225":
                instrument = "JP225_USD"
            elif symbol == "HK50":
                instrument = "HK33_HKD"
            elif symbol == "AU200":
                instrument = "AU200_AUD"
            elif symbol == "WTICO_USD":
                instrument = "WTICO_USD"
            elif symbol == "BCO_USD":
                instrument = "BCO_USD"
            else: # Logic mc dnh data nh cOrc c p money tForex
                instrument = f"{symbol[:3]}_{symbol[3:]}"

            granularity = granularity_map.get(PRIMARY_TIMEFRAME)
            if not granularity:
                print(f"Primary timeframe {PRIMARY_TIMEFRAME} is not valid.")
                return None

            url = f"{OANDA_URL}/instruments/{instrument}/candles"
            headers = {"Authorization": f"Bearer {OANDA_API_KEY}"}
            params = {"count": 1, "price": "M", "granularity": granularity}
            response = requests.get(url, headers=headers, params=params, timeout=10)

            if response.status_code != 200:
                print(
                    f"Li API {response.status_code} khi l y gi nhanh cho {symbol}: {response.text}"
                )
                return None

            candles = response.json().get("candles", [])
            if candles:
                return float(candles[0]["mid"]["c"])
            else:
                print(f" not receive dn n no khi l y gi nhanh cho {symbol}.")
                return None
        except Exception as e:
            print(f"Li not x l data nh khi l y gi nhanh cho {symbol}: {e}")
            return None

    # <<< fix L I D T I M 2/2 >>>
    # Function this has d fix dqu n l vi has data i tn from 'rsi' v ghp data
    # m t allh chnh xc, d m b o not cn l i trng l p.

    # <<< fix L I D T I M 2/2 >>>
    # Function this has d fix dqu n l vi has data i tn from 'rsi' v ghp data
    # m t allh chnh xc, d m b o not cn l i trng l p.

    def create_enhanced_features(self, symbol):
            """
            T o features nng cao tmulti-timeframe data.
            """
            primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, PRIMARY_TIMEFRAME_DEFAULT)
            timeframes_to_use = TIMEFRAME_SET_BY_PRIMARY.get(primary_tf)

            multi_tf_data = self.fetch_multi_timeframe_data(symbol, 5000, timeframes_to_use)
            logging.info(f"[Sanity] weekend={is_weekend()} sym={symbol} crypto={is_crypto_symbol(symbol)} primary_tf={primary_tf}")
            # This call is now correct.
            if not data_freshness_manager.validate_with_recovery(multi_tf_data, primary_tf, symbol):
                print(f"data for {symbol} is too old. Skipping feature creation.")
                # <<< TreceiveANG M I: nhn d liu symbol needs retrain >>>
                self._mark_symbol_for_retrain(symbol, "stale_data")
                return None

            # <<< K T THfromHAY  I >>>
            # ========================
            if primary_tf not in multi_tf_data:
                print(f"⚠️ No data found for primary timeframe {primary_tf} of {symbol}")
                return None

            # 2. Start with primary timeframe ANdREATE ALL FEATURES FOR IT
            df_enhanced = multi_tf_data[primary_tf].copy()
            df_enhanced.name = symbol

            df_enhanced = self.feature_engineer.create_all_features(df_enhanced)

            if 'rsi' in df_enhanced.columns:
                df_enhanced.rename(columns={'rsi': f'rsi_{primary_tf}'}, inplace=True)

            for htf in timeframes_to_use:
                if htf != primary_tf and htf in multi_tf_data:
                    df_htf = multi_tf_data[htf].copy()

                    ema20_htf = EMAIndicator(df_htf["close"], 20).ema_indicator()
                    ema50_htf = EMAIndicator(df_htf["close"], 50).ema_indicator()
                    rsi_htf = RSIIndicator(df_htf["close"]).rsi()

                    htf_features = pd.DataFrame({
                        f"ema20_{htf}": ema20_htf,
                        f"ema50_{htf}": ema50_htf,
                        f"rsi_{htf}": rsi_htf,
                        f"trend_{htf}": (ema20_htf > ema50_htf).astype(int),
                    }, index=df_htf.index)

                    htf_resampled = htf_features.reindex(df_enhanced.index, method="ffill").bfill()
                    df_enhanced = df_enhanced.join(htf_resampled)

            df_enhanced.replace([np.inf, -np.inf], np.nan, inplace=True)
            df_enhanced.fillna(method='ffill', inplace=True)
            df_enhanced.fillna(method='bfill', inplace=True)

            # processing fillna an ton cOrategorical columns
            categorical_columns = []
            for col in df_enhanced.columns:
                if df_enhanced[col].dtype.name == 'category':
                    categorical_columns.append(col)

            # Fillna cOrc from not must categorical
            non_categorical_cols = [col for col in df_enhanced.columns if col not in categorical_columns]
            if non_categorical_cols:
                df_enhanced[non_categorical_cols] = df_enhanced[non_categorical_cols].fillna(0)

            # processing ring cOrategorical columns
            for col in categorical_columns:
                # L y categories current
                current_categories = df_enhanced[col].cat.categories
                # Add 0 to categories if not present
                if 0 not in current_categories:
                    df_enhanced[col] = df_enhanced[col].cat.add_categories([0])
                # Fillna with mode Or gi trd u tin in categories
                if df_enhanced[col].isna().any():
                    mode_value = df_enhanced[col].mode()
                    if len(mode_value) > 0:
                        df_enhanced[col] = df_enhanced[col].fillna(mode_value[0])
                    else:
                        df_enhanced[col] = df_enhanced[col].fillna(current_categories[0])
            # <<< TFunction ENCODING COrATEGORICAL COLUMNS >>>
            df_enhanced = self._encode_categorical_features(df_enhanced, symbol)

            # <<< KH C PH C L I: TFunction NEWS SENTIMENT FEATURES >>>
            #  m b o news features dufrom o in qu trnh Equal th i gian th c
            try:
                print(f" [Features] Checking news_manager for {symbol}...")
                print(f"   - hasattr(self, 'news_manager'): {hasattr(self, 'news_manager')}")
                print(f"   - news_manager is not None: {self.news_manager is not None if hasattr(self, 'news_manager') else 'N/A'}")
                print(f"   - news_manager type: {type(self.news_manager) if hasattr(self, 'news_manager') and self.news_manager is not None else 'N/A'}")
                
                if hasattr(self, 'news_manager') and self.news_manager is not None:
                    print(f" [Features] Adding news sentiment features for {symbol}")
                    print(f"   - news_manager type: {type(self.news_manager)}")
                    print(f"   - news_manager has_add_news_sentiment_features: {hasattr(self.news_manager, 'add_news_sentiment_features')}")
                    
                    # Test call to add_news_sentiment_features
                    df_enhanced = self.news_manager.add_news_sentiment_features(df_enhanced, symbol)
                    print(f"[Features] News sentiment feature added for {symbol}")
                else:
                    # T o all from news features mc dnh data receiveu not needsews_manager
                    print(f" [Features] News manager not available for {symbol}, using default features")
                    print(f"   - hasattr(self, 'news_manager'): {hasattr(self, 'news_manager')}")
                    print(f"   - news_manager is not None: {self.news_manager is not None if hasattr(self, 'news_manager') else 'N/A'}")
                    print(f"   - Reason: {'news_manager is None' if hasattr(self, 'news_manager') and self.news_manager is None else 'news_manager attribute does not exist'}")
                    news_columns = ['news_sentiment_score', 'news_sentiment_volume', 'news_quality_score', 
                                   'news_timing_score', 'news_sentiment_trend', 'news_impact_score']
                    for col in news_columns:
                        if col not in df_enhanced.columns:
                            df_enhanced[col] = 0.0
                    logging.info(f"News manager not available, using default news features for {symbol}")
            except Exception as e:
                logging.error(f"Error adding news features for {symbol}: {e}")
                print(f"[Features] Error adding news features for {symbol}: {e}")
                import traceback
                traceback.print_exc()
                # T o all from news features mc dnh data nh khi c l i
                news_columns = ['news_sentiment_score', 'news_sentiment_volume', 'news_quality_score', 
                               'news_timing_score', 'news_sentiment_trend', 'news_impact_score']
                for col in news_columns:
                    if col not in df_enhanced.columns:
                        df_enhanced[col] = 0.0

            # <<< ADD ECONOMIC EVENT FEATURES >>>
            # Add economic features that are required by the model
            try:
                print(f" [Features] Adding economic event features for {symbol}...")
                if hasattr(self, 'news_manager') and self.news_manager is not None:
                    df_enhanced = self.news_manager.add_economic_event_features(df_enhanced, symbol)
                    print(f"[Features] Economic event features added for {symbol}")
                else:
                    # Add basic economic features if news_manager is not available
                    print(f" [Features] News manager not available, adding basic economic features for {symbol}")
                    self._add_basic_economic_features_direct(df_enhanced, symbol)
            except Exception as e:
                logging.error(f"Error adding economic features for {symbol}: {e}")
                print(f"[Features] Error adding economic features for {symbol}: {e}")
                # Add basic economic features as fallback
                self._add_basic_economic_features_direct(df_enhanced, symbol)

            return df_enhanced

    def _encode_categorical_features(self, df, symbol):
        """
        Encode categorical features dtree-based models fromhUsing.
        T o cnumeric encoding v one-hot encoding.
        """
        # L y metadata cho symbol
        metadata = SYMBOL_METADATA.get(symbol, {})

        # 1. Encode asset_class
        if 'asset_class' in df.columns and 'asset_class_encoded' not in df.columns:
            asset_class_mapping = {
                'equity_index': 1,
                'commodity': 2,
                'forex': 3,
                'cryptocurrency': 4
            }
            df['asset_class_encoded'] = df['asset_class'].map(asset_class_mapping).fillna(0)

            # One-hot encoding cho asset_class
            asset_classes = ['equity_index', 'commodity', 'forex', 'cryptocurrency']
            for ac in asset_classes:
                col_name = f'asset_class_{ac}'
                if col_name not in df.columns:
                    df[col_name] = (df['asset_class'] == ac).astype(int)

        # 2. Encode volatility_profile
        if 'volatility_profile' in df.columns and 'volatility_profile_encoded' not in df.columns:
            volatility_mapping = {
                'very_low': 1,
                'low': 2,
                'medium': 3,
                'high': 4,
                'very_high': 5
            }
            df['volatility_profile_encoded'] = df['volatility_profile'].map(volatility_mapping).fillna(3)

            # One-hot encoding cho volatility_profile
            volatility_profiles = ['very_low', 'low', 'medium', 'high', 'very_high']
            for vp in volatility_profiles:
                col_name = f'volatility_profile_{vp}'
                if col_name not in df.columns:
                    df[col_name] = (df['volatility_profile'] == vp).astype(int)

        # 3. Encode volatility_regime (d l numerifrommarket_regime)
        if 'volatility_regime' in df.columns and 'volatility_regime_encoded' not in df.columns:
            # volatility_regime thu ng l categorical tmarket_regime
            if df['volatility_regime'].dtype.name == 'category':
                regime_mapping = {
                    'low_volatility': 1,
                    'medium_volatility': 2,
                    'high_volatility': 3,
                    'extreme_volatility': 4
                }
                df['volatility_regime_encoded'] = df['volatility_regime'].map(regime_mapping).fillna(2)

                # One-hot encoding cho volatility_regime
                regimes = ['low_volatility', 'medium_volatility', 'high_volatility', 'extreme_volatility']
                for reg in regimes:
                    col_name = f'volatility_regime_{reg}'
                    if col_name not in df.columns:
                        df[col_name] = (df['volatility_regime'] == reg).astype(int)
            else:
                # if d l numeric, chneeds d m b o not needsaN
                df['volatility_regime_encoded'] = df['volatility_regime'].fillna(0)

        # 4. T o interaction features gi a categorical v numeric
        if 'asset_class_encoded' in df.columns and 'volatility_profile_encoded' in df.columns:
            df['asset_volatility_interaction'] = df['asset_class_encoded'] * df['volatility_profile_encoded']

        # 5. T o session-based features tasset class
        if 'asset_class' in df.columns:
            # Asian session preference
            df['prefers_asian_session'] = df['asset_class'].isin(['equity_index']).astype(int)
            # European session preference
            df['prefers_european_session'] = df['asset_class'].isin(['equity_index']).astype(int)
            # American session preference
            df['prefers_american_session'] = df['asset_class'].isin(['equity_index', 'commodity']).astype(int)
            # 24h trading preference
            df['prefers_24h_trading'] = df['asset_class'].isin(['cryptocurrency', 'commodity']).astype(int)

        # 6. T o risk-adjusted features d a trn asset class
        if 'asset_class' in df.columns:
            # Risk multiplier d a trn asset class
            risk_multipliers = {
                'equity_index': 1.0,
                'forex': 1.2,
                'commodity': 1.5,
                'cryptocurrency': 2.0
            }
            df['asset_risk_multiplier'] = df['asset_class'].map(risk_multipliers).fillna(1.0)

            # Volatility adjustment factor
            volatility_factors = {
                'equity_index': 1.0,
                'forex': 0.8,
                'commodity': 1.3,
                'cryptocurrency': 1.8
            }
            df['asset_volatility_factor'] = df['asset_class'].map(volatility_factors).fillna(1.0)

        # 7. type ball categorical columns g c sau khi d to encoded versions
        categorical_cols_to_remove = ['asset_class', 'volatility_profile', 'volatility_regime']
        for col in categorical_cols_to_remove:
            if col in df.columns:
                df.drop(columns=[col], inplace=True)

        # 8. Log thng tin encoding
        remaining_categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        encoded_cols = [col for col in df.columns if col.endswith('_encoded') or col.startswith('asset_class_') or col.startswith('volatility_profile_') or col.startswith('volatility_regime_')]

        if len(remaining_categorical_cols) > 0:
            print(f"   [Encoding] Cn l i {len(remaining_categorical_cols)} from categorical: {list(remaining_categorical_cols)}")
        print(f"   [Encoding] Total {len(encoded_cols)} features encoded: {encoded_cols[:5]}...")

        return df

    def _add_basic_economic_features_direct(self, df, symbol):
        """Add basic economic event features directly to DataFrame"""
        try:
            # Add basic economic calendar features based on common patterns
            df["is_near_high_impact_event"] = 0
            
            # Add day-of-week effects (common economic events happen on specific days)
            df["is_friday"] = (df.index.dayofweek == 4).astype(int)
            df["is_monday"] = (df.index.dayofweek == 0).astype(int)
            
            # Add month-end effects (common for economic data releases)
            df["is_month_end"] = (df.index.day >= 25).astype(int)
            
            # Add quarterly effects (Q1, Q2, Q3, Q4)
            df["quarter"] = df.index.quarter
            df["is_q1"] = (df.index.quarter == 1).astype(int)
            df["is_q2"] = (df.index.quarter == 2).astype(int)
            df["is_q3"] = (df.index.quarter == 3).astype(int)
            df["is_q4"] = (df.index.quarter == 4).astype(int)
            
            # Add holiday proximity effects (simplified)
            df["is_holiday_proximity"] = 0  # Placeholder for holiday effects
            
            # Ensure all required features exist
            required_features = ['is_q1', 'is_q2', 'is_q3', 'is_q4', 'is_month_end', 'is_holiday_proximity']
            for feature in required_features:
                if feature not in df.columns:
                    df[feature] = 0
                    
            print(f"   [Features] Basic economic features added for {symbol}")
            return df
            
        except Exception as e:
            logging.error(f"Error adding basic economic features for {symbol}: {e}")
            # Fallback: ensure critical features exist with default values
            critical_features = ['is_q1', 'is_q2', 'is_q3', 'is_q4', 'is_month_end']
            for feature in critical_features:
                if feature not in df.columns:
                    df[feature] = 0
            return df

    # TFunction Function M I this VO in L P EnhancedDataManager

    async def fetch_multi_timeframe_data_async(self, session, symbol, count=2500, timeframes_to_use=None):
        all_data = {}
        primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, PRIMARY_TIMEFRAME_DEFAULT)
        timeframes_to_use = TIMEFRAME_SET_BY_PRIMARY.get(primary_tf, ["H4", "D1", "W1"])
        granularity_map = {"H1": "H1", "H4": "H4", "D1": "D", "W1": "W"}

        for timeframe in timeframes_to_use:
            if timeframe not in granularity_map: continue
            try:
                # (Logic get instrumentoriginal as function of b n)
                instrument = f"{symbol[:3]}_{symbol[3:]}" # V Equal gi n ha

                granularity = granularity_map[timeframe]
                url = f"{OANDA_URL}/instruments/{instrument}/candles"
                headers = {"Authorization": f"Bearer {OANDA_API_KEY}"}
                params = {"count": count, "granularity": granularity, "price": "M"}

                async with session.get(url, headers=headers, params=params, timeout=30) as response:
                    if response.status != 200:
                        print(f"API Async {response.status} {instrument} {timeframe}")
                        continue

                    data = await response.json()
                    candles_raw = data.get("candles", [])

                    candles = [
                        {"time": pd.to_datetime(c["time"]), "open": float(c["mid"]["o"]), "high": float(c["mid"]["h"]), "low": float(c["mid"]["l"]), "close": float(c["mid"]["c"]), "volume": c.get("volume", 1000)}
                        for c in candles_raw if c.get("complete", True)
                    ]

                    if candles:
                        df = pd.DataFrame(candles).set_index("time")
                        df.index = df.index.tz_convert(pytz.timezone("Asia/Bangkok"))
                        all_data[timeframe] = df
            except Exception as e:
                print(f"Li Async Fetch {symbol} {timeframe}: {e}")

        return symbol, all_data
    # all Function luu/t i m hnh not thay d i
    # <<< THAY THTON B2 Function this >>>
    # TFunction Function M I this VO in L P EnhancedTradingBot

    async def run_async_data_gathering(self):
        """
        T o v ch y cfromc vl y data v to features m t allh bt du b .
        """
        print("🔄 Starting asynchronous data collection cycle...")
        full_data_cache = {}
        async with aiohttp.ClientSession() as session:
            # T o danh sch cfromc vneeds th c hi n
            tasks = [self.data_manager.fetch_multi_timeframe_data_async(session, symbol) for symbol in SYMBOLS]

            # Ch y allfromc vsong song
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # processing k t qu 
            for result in results:
                if isinstance(result, Exception):
                    print(f" Li in m t tc v : {result}")
                    continue

                symbol, multi_tf_data = result
                if multi_tf_data:
                    primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, PRIMARY_TIMEFRAME_DEFAULT)
                    if primary_tf in multi_tf_data:
                        # (Logifrom o feature tmulti_tf_data of b n sd g i duc)
                        # V d :
                        # df_enhanced = self.data_manager.create_enhanced_features_from_data(multi_tf_data)
                        # full_data_cache[symbol] = df_enhanced
                        print(f" processing xong data cho {symbol}")
                    else:
                        print(f"data cho {symbol} not c khung th i gian chnh.")
                else:
                    print(f"not l y d dli u cho {symbol}")

        print("Data cycle starting.")
        return full_data_cache
def save_model_with_metadata(symbol, model_data, model_type="ensemble"):
    """
    Luu model v metadata.
    PHIN B N M I: B sung cv_mean_accuracy vo metadata.
    """
    if model_data is None or "ensemble" not in model_data:
        print(f"⚠️ No model_data or ensemble to save for {symbol} ({model_type}). Skipping.")
        return

    ensemble_model = model_data["ensemble"]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    filename = os.path.join(MODEL_DIR, f"{model_type}_model_{symbol}_{timestamp}.pkl")
    metadata_file = filename.replace(".pkl", ".json")

    try:
        joblib.dump(model_data, filename)

        first_model_name = list(ensemble_model.cv_results.keys())[0] if ensemble_model.cv_results else "unknown"
        cv_info = ensemble_model.cv_results.get(first_model_name, {})

        metadata = {
            "symbol": symbol,
            "timestamp": timestamp,
            "feature_columns": model_data.get("feature_columns", []),
            "cv_mean_f1": cv_info.get("mean_f1") or 0.0,
            "cv_std_f1": cv_info.get("std_f1") or 0.0,
            # === TFunction used this ===
            "cv_mean_accuracy": cv_info.get("mean_accuracy") or cv_info.get("mean_accuracy") or 0.0,
            # ======================
            "model_weights": getattr(ensemble_model, 'model_weights', {}),  # Fallback if not c model_weights
            "model_type": model_type,
            "model_file": filename,
        }

        with open(metadata_file, "w") as f:
            json.dump(metadata, f, indent=2)

        print(f"Model for {symbol} ({model_type}) saved at {filename}")
        cv_f1 = metadata['cv_mean_f1'] or 0.0
        cv_std = metadata['cv_std_f1'] or 0.0
        cv_accuracy = metadata['cv_mean_accuracy'] or 0.0
        print(f"   - CV F1-Score: {cv_f1:.3f} +/- {cv_std:.3f}")
        print(f"   - CV Accuracy: {cv_accuracy:.3f}")
    except Exception as e:
        print(f"Error saving model for {symbol} ({model_type}): {e}")

# <<< TFunction Function M I this VO FILE BOT >>>




# EnhancedTradingBot class

def load_latest_model(symbol, model_type="ensemble"):
    if not os.path.exists(MODEL_DIR): return None

    pattern = f"{model_type}_model_{symbol}"
    files = [f for f in os.listdir(MODEL_DIR) if f.startswith(pattern) and f.endswith(".pkl")]
    if not files: return None

    files.sort(reverse=True)
    latest_pkl_file = os.path.join(MODEL_DIR, files[0])
    metadata_file = latest_pkl_file.replace(".pkl", ".json")

    try:
        model_data = joblib.load(latest_pkl_file)

        # <<< C I money: LOGIC Check TNH TUONG THCH M I >>>
        is_valid = False
        ensemble_model = model_data.get("ensemble")
        # D u hi u of model Stacking (Bot LLMs) l fromhu fromnh 'meta_model'
        if ensemble_model and hasattr(ensemble_model, 'meta_model'):
            is_valid = True

        if is_valid:
            logging.info(f" Loaded_compatible model ({model_type}) for {symbol} from: {latest_pkl_file}")
            return model_data
        else:
            logging.warning(f"⚠️ Detected incompatible model (missing 'meta_model') for {symbol}.")
            logging.info(f"   -> Auto-deleting file: {latest_pkl_file}")
            try:
                os.remove(latest_pkl_file)
                if os.path.exists(metadata_file):
                    os.remove(metadata_file)
            except OSError as e:
                print(f"   Error deleting old file: {e}")
            return None # trߦ v+ None dactivate training l i

    except Exception as e:
        logging.error(f"Error loading or checking model at {latest_pkl_file}: {e}")
        return None

def save_open_positions(
    open_positions, filename=f"open_positions_{PRIMARY_TIMEFRAME.lower()}.json"
):
    try:
        positions_to_save = {
            symbol: {
                **pos,
                "opened_at": (
                    pos["opened_at"].isoformat()
                    if isinstance(pos.get("opened_at"), datetime)
                    else pos.get("opened_at")
                ),
            }
            for symbol, pos in open_positions.items()
        }
        with open(filename, "w") as f:
            json.dump(positions_to_save, f, indent=2)
    except Exception as e:
        print(f"Error saving positions: {e}")


def load_open_positions(filename=f"open_positions_{PRIMARY_TIMEFRAME.lower()}.json"):
    try:
        with open(filename, "r") as f:
            data = json.load(f)

        loaded_positions = {}
        for sym, pos in data.items():
            # Tused gn symbol vo bn in dictionary con
            pos["symbol"] = sym

            # processing th i gian with timezone managemenfrom model i
            pos["opened_at"] = (
                datetime.fromisoformat(pos["opened_at"]).astimezone(
                    pytz.timezone(TIMEZONE_CONFIG["DEFAULT_TIMEZONE"])
                )
                if "opened_at" in pos and isinstance(pos["opened_at"], str)
                else None
            )

            loaded_positions[sym] = pos

        print(f"Loaded {len(loaded_positions)} positions")
        return loaded_positions
    except (FileNotFoundError, json.JSONDecodeError):
        print(" No existing positions file, starting fresh")
        return {}
    except Exception as e:
        print(f"Error loading positions: {e}")
        return {}


# <<< C I money 1: THAY THTON BL P TRADINGENVIRONMENT equal L P this >>>
# TM V THAY THTON BL P this

class PortfolioEnvironment(gym.Env):
    """
    Mi tru ng RL qu n l portfolio.
    IMPORTANT: Reward function based on Sharpe Ratio using returns.
    """
    metadata = {'render_modes': ['human', 'rgb_array']}

    # PortfolioEnvironmenfrom modelethods

    def __init__(self, dict_df_features, dict_feature_columns, symbols, initial_balance=10000, render_mode=None):
        super(PortfolioEnvironment, self).__init__()
        
        self.render_mode = render_mode

        self.initial_balance = initial_balance
        self.feature_columns_map = dict_feature_columns
        self.dfs = {}
        self.features = {}
        valid_symbols_for_env = []

        # --- Step 1: Prepare data from (original) ---
        for symbol in symbols:
            df = dict_df_features.get(symbol)
            feature_cols = self.feature_columns_map.get(symbol)
            if df is None or df.empty or not feature_cols: continue

            # Ch+ lߦy all from c s n in dataframe
            available_cols = [col for col in feature_cols if col in df.columns]
            features_df = df[available_cols].dropna()

            if not features_df.empty:
                self.features[symbol] = features_df.to_numpy()
                self.dfs[symbol] = df.loc[features_df.index]
                valid_symbols_for_env.append(symbol)

        self.symbols = valid_symbols_for_env
        self.n_symbols = len(self.symbols)

        if self.n_symbols == 0:
            raise ValueError("Cannot create Env: No valid symbols.")

        self.max_steps = min(len(self.features[s]) for s in self.symbols)

        # --- Step 2: <<< IMPROVEMENT: Calculate Observation Space Size BASED ON ACTUAL DATA >>> ---
        # L y kch thu c of sfeature thtru ng tchnh array numpy d dufrom o
        single_symbol_market_features_dim = self.features[self.symbols[0]].shape[1]

        # public tFunction 3 state of vth(vth , pnl, th i gian)
        single_symbol_obs_size = single_symbol_market_features_dim + 3

        # public tFunction 4 state ton c c
        total_obs_size = self.n_symbols * single_symbol_obs_size + 4

        print(f"   [Environment] Kch thuc Observation Space duc tnh ton: {total_obs_size}")
        print(f"   [Environment] Market features per symbol: {single_symbol_market_features_dim}")
        print(f"   [Environment] Total symbols: {self.n_symbols}")

        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(total_obs_size,), dtype=np.float32)

        # Store dimensions for validation
        self.expected_market_dim = single_symbol_market_features_dim
        self.expected_total_dim = total_obs_size
        self.action_space = spaces.MultiDiscrete([3] * self.n_symbols)
        # --- K T THC C I money ---

        self.returns_history = deque(maprocessingen=30)
        self.reset()

    def _get_observation(self):
        # Function this Using logineedsng cao tBot LLMs with shape validation
        all_symbol_obs, num_open_positions = [], 0
        all_atr_normalized, trending_symbols_count = [], 0

        for i, symbol in enumerate(self.symbols):
            current_candle_data = self.dfs[symbol].iloc[self.current_step]
            market_obs = self.features[symbol][self.current_step]

            # Validate v pad market_obs if needs
            if len(market_obs) != self.expected_market_dim:
                if len(market_obs) < self.expected_market_dim:
                    # Pad with zeros
                    padding = np.zeros(self.expected_market_dim - len(market_obs))
                    market_obs = np.append(market_obs, padding)
                else:
                    # Truncate
                    market_obs = market_obs[:self.expected_market_dim]

            all_atr_normalized.append(current_candle_data.get('atr_normalized', 0))
            if current_candle_data.get('market_regime', 0) != 0: trending_symbols_count += 1

            pos_state = self.positions[i]
            pnl_state = self.unrealized_pnls[i] / self.balance if self.balance != 0 else 0.0
            time_state = self.time_in_trades[i] / 100.0
            symbol_obs = np.append(market_obs, [pos_state, pnl_state, time_state])
            all_symbol_obs.append(symbol_obs)
            if self.positions[i] != 0: num_open_positions += 1

        flat_obs = np.concatenate(all_symbol_obs).astype(np.float32)
        global_states = np.array([
            (self.balance / self.initial_balance) - 1.0,
            num_open_positions / self.n_symbols,
            np.mean(all_atr_normalized) if all_atr_normalized else 0,
            trending_symbols_count / self.n_symbols
        ]).astype(np.float32)

        final_obs = np.append(flat_obs, global_states)

        # Final validation
        if len(final_obs) != self.observation_space.shape[0]:
            logging.info(f"[RL Env] Adjusting observation shape: {len(final_obs)} -> {self.observation_space.shape[0]}")
            # Pad or truncate to match expected size
            if len(final_obs) < self.observation_space.shape[0]:
                padding = np.zeros(self.observation_space.shape[0] - len(final_obs))
                final_obs = np.append(final_obs, padding)
            else:
                final_obs = final_obs[:self.observation_space.shape[0]]

        return final_obs

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.balance = self.initial_balance
        self.current_step = 0
        self.positions = [0] * self.n_symbols
        self.entry_prices = [0] * self.n_symbols
        self.unrealized_pnls = [0] * self.n_symbols
        self.time_in_trades = [0] * self.n_symbols
        self.returns_history.clear() # <<< Add model I
        return self._get_observation(), {}

    # PortfolioEnvironmenfrom modelethods

    def step(self, action):
        previous_balance = self.balance

        # --- PH N LOGIC C P NH T Tempty THI BTHI U ---
        # (This part uses standard environment logic)
        total_unrealized_pnl = 0
        for i in range(self.n_symbols):
            if self.positions[i] != 0:
                self.time_in_trades[i] += 1
                price_now = self.dfs[self.symbols[i]]['close'].iloc[self.current_step]
                pnl_for_symbol = (price_now - self.entry_prices[i]) * self.positions[i]
                self.unrealized_pnls[i] = pnl_for_symbol
                total_unrealized_pnl += pnl_for_symbol

        realized_pnl_this_step = 0
        action_vector = np.asarray(action).flatten()
        for i, action_code in enumerate(action_vector):
            symbol = self.symbols[i]
            current_price = self.dfs[symbol]['close'].iloc[self.current_step]
            pos = self.positions[i]

            if action_code == 1 and pos == 0: # Ml nh MUA
                self.positions[i] = 1
                self.entry_prices[i] = current_price
                self.time_in_trades[i] = 0
            elif action_code == 2 and pos == 0: # Ml nh BN
                self.positions[i] = -1
                self.entry_prices[i] = current_price
                self.time_in_trades[i] = 0
            elif action_code == 0 and pos != 0: # ng l nh
                pnl = (current_price - self.entry_prices[i]) * self.positions[i]
                realized_pnl_this_step += pnl
                self.positions[i] = 0
                self.entry_prices[i] = 0
                self.unrealized_pnls[i] = 0
                self.time_in_trades[i] = 0

        self.balance += realized_pnl_this_step
        # --- K T THC PH N LOGIC C P NH T ---

        # Ph n tnh ton Function thu ng Sharpe Ratio d used
        step_return = (self.balance / previous_balance) - 1 if previous_balance > 0 else 0.0
        self.returns_history.append(step_return)
        if len(self.returns_history) > 1:
            risk = np.std(self.returns_history) + 1e-9
            reward = np.mean(self.returns_history) / risk
        else:
            reward = step_return
        if np.all(action_vector == 0): reward -= 0.001

        self.current_step += 1
        terminated = self.current_step >= self.max_steps - 1
        obs = self._get_observation()
        info = {'balance': self.balance}
        return obs, reward, terminated, False, info
    
    def render(self):
        """Render the environment."""
        if self.render_mode == "human":
            print(f"Step: {self.current_step}, Balance: {self.balance:.2f}")
        elif self.render_mode == "rgb_array":
            # Return a simple visualization as numpy array
            return np.zeros((100, 100, 3), dtype=np.uint8)
        return None
# --- NEW: RL AGENT CLASS ---
class RLAgent:
    def __init__(self, model_path=None):
        self.model = None
        self.reward_history = deque(maxlen=1000)
        self.action_history = deque(maxlen=100)
        self.state_memory = deque(maxlen=50)
        self.performance_metrics = {
            'total_rewards': 0,
            'win_rate': 0,
            'sharpe_ratio': 0,
            'max_drawdown': 0,
            'profit_factor': 0
        }
        
        if model_path and os.path.exists(model_path):
            self.model = PPO.load(model_path)
            print(f"✅ Enhanced RL Agent loaded from {model_path}")
        else:
            print("🤖 Enhanced RL Agent not loaded. Needs to be trained.")

    def train(self, env, total_timesteps=20000, save_path=None):
        """
        Enhanced training with improved reward functions and callbacks
        """
        if self.model:
            self.model.set_env(env)
        else:
            # Enhanced PPO configuration with better hyperparameters
            self.model = PPO(
                "MlpPolicy", 
                env, 
                verbose=1, 
                tensorboard_log=f"./ppo_tensorboard_{PRIMARY_TIMEFRAME.lower()}/",
                learning_rate=3e-4,
                n_steps=2048,
                batch_size=64,
                n_epochs=10,
                gamma=0.99,
                gae_lambda=0.95,
                clip_range=0.2,
                ent_coef=0.01,
                vf_coef=0.5,
                max_grad_norm=0.5
            )

        # Enhanced callbacks for better training
        callbacks = [
            StopTrainingOnMaxDrawdown(max_drawdown_threshold=0.25, verbose=1),
            EnhancedRewardCallback(self),
            PerformanceMonitorCallback(self)
        ]

        print("🤖 Starting Enhanced RL Agent training...")
        print(f"   - Total timesteps: {total_timesteps}")
        print(f"   - Callbacks: {len(callbacks)} active")
        
        # Train with enhanced callbacks
        self.model.learn(
            total_timesteps=total_timesteps, 
            progress_bar=True, 
            callback=callbacks
        )
        
        # Calculate final performance metrics
        self._calculate_final_metrics()
        
        logging.info("Enhanced RL Agent training completed.")
        print(f"✅ Training completed. Final metrics:")
        print(f"   - Win Rate: {self.performance_metrics['win_rate']:.2%}")
        print(f"   - Sharpe Ratio: {self.performance_metrics['sharpe_ratio']:.3f}")
        print(f"   - Max Drawdown: {self.performance_metrics['max_drawdown']:.2%}")

        if save_path:
            self.model.save(save_path)
            # Save performance metrics alongside model
            metrics_path = save_path + "_metrics.json"
            with open(metrics_path, 'w') as f:
                json.dump(self.performance_metrics, f, indent=2)
            print(f"✅ Enhanced RL Agent saved to {save_path}")
            print(f"✅ Performance metrics saved to {metrics_path}")

    def predict(self, observation, deterministic=True):
        """Enhanced prediction with state memory and action tracking"""
        if not self.model:
            return 0  # Default to HOLD if not trained

        # Store state for analysis
        self.state_memory.append(observation.copy() if hasattr(observation, 'copy') else observation)
        
        # Get prediction
        action, _states = self.model.predict(observation, deterministic=deterministic)
        action_value = action.item() if hasattr(action, 'item') else action
        
        # Track action history
        self.action_history.append(action_value)
        
        return action_value
    
    def update_reward(self, reward):
        """Update reward history and performance tracking"""
        self.reward_history.append(reward)
        self.performance_metrics['total_rewards'] += reward
    
    def _calculate_final_metrics(self):
        """Calculate comprehensive performance metrics"""
        if not self.reward_history:
            return
        
        rewards = list(self.reward_history)
        
        # Win rate
        positive_rewards = [r for r in rewards if r > 0]
        self.performance_metrics['win_rate'] = len(positive_rewards) / len(rewards) if rewards else 0
        
        # Sharpe ratio (simplified)
        if len(rewards) > 1:
            mean_reward = np.mean(rewards)
            std_reward = np.std(rewards)
            self.performance_metrics['sharpe_ratio'] = mean_reward / std_reward if std_reward > 0 else 0
        
        # Max drawdown (simplified)
        cumulative_rewards = np.cumsum(rewards)
        running_max = np.maximum.accumulate(cumulative_rewards)
        drawdown = (running_max - cumulative_rewards) / np.maximum(running_max, 1)
        self.performance_metrics['max_drawdown'] = np.max(drawdown) if len(drawdown) > 0 else 0
        
        # Profit factor
        total_profits = sum(positive_rewards)
        negative_rewards = [abs(r) for r in rewards if r < 0]
        total_losses = sum(negative_rewards)
        self.performance_metrics['profit_factor'] = total_profits / total_losses if total_losses > 0 else float('inf')
    
    def get_state_representation(self, market_data, position_data=None):
        """
        Create enhanced state representation for RL agent
        """
        try:
            state_features = []
            
            # Price-based features
            df = market_data.get('price_data')
            if df is not None and not df.empty:
                # Recent price changes
                recent_returns = df['close'].pct_change().fillna(0).tail(5).tolist()
                state_features.extend(recent_returns)
                
                # Technical indicators
                if 'rsi' in df.columns:
                    state_features.append(df['rsi'].iloc[-1] / 100.0)  # Normalize RSI
                else:
                    state_features.append(0.5)  # Default neutral
                
                if 'macd' in df.columns:
                    state_features.append(np.tanh(df['macd'].iloc[-1]))  # Normalize MACD
                else:
                    state_features.append(0.0)
                
                # Volume features
                if 'volume' in df.columns:
                    vol_ratio = df['volume'].iloc[-1] / df['volume'].rolling(20).mean().iloc[-1]
                    state_features.append(np.tanh(vol_ratio - 1))  # Normalize volume ratio
                else:
                    state_features.append(0.0)
                
                # Volatility
                volatility = df['close'].pct_change().rolling(10).std().iloc[-1]
                state_features.append(np.tanh(volatility * 100))  # Normalize volatility
            else:
                # Fallback features if no price data
                state_features.extend([0.0] * 9)  # 5 returns + 4 indicators
            
            # Position features
            if position_data:
                state_features.append(1.0 if position_data.get('has_position', False) else 0.0)
                state_features.append(position_data.get('profit_pct', 0.0))
                state_features.append(position_data.get('days_held', 0.0) / 30.0)  # Normalize days
            else:
                state_features.extend([0.0, 0.0, 0.0])
            
            # Market regime features
            market_sentiment = market_data.get('market_sentiment', 0.0)
            state_features.append(np.tanh(market_sentiment))
            
            # News sentiment if available
            news_sentiment = market_data.get('news_sentiment', {}).get('compound', 0.0)
            state_features.append(news_sentiment)
            
            # Ensure fixed size state (pad or truncate if necessary)
            target_size = 14
            if len(state_features) < target_size:
                state_features.extend([0.0] * (target_size - len(state_features)))
            elif len(state_features) > target_size:
                state_features = state_features[:target_size]
            
            return np.array(state_features, dtype=np.float32)
            
        except Exception as e:
            print(f"❌ [RL Agent] Error creating state representation: {e}")
            # Return default neutral state
            return np.zeros(14, dtype=np.float32)
    
    def calculate_enhanced_reward(self, action, market_data, position_data, price_change):
        """
        Calculate enhanced reward function with multiple factors
        """
        try:
            reward = 0.0
            
            # Base reward from price change
            if action == 1:  # BUY
                reward += price_change * 10  # Amplify reward
            elif action == 2:  # SELL
                reward -= price_change * 10  # Reward short positions
            # HOLD (action == 0) gets small penalty for inaction in trending markets
            
            # Risk-adjusted reward
            volatility = market_data.get('volatility', 0.01)
            if volatility > 0:
                reward = reward / (1 + volatility)  # Penalize high volatility trades
            
            # Position holding penalty/reward
            if position_data and position_data.get('has_position', False):
                days_held = position_data.get('days_held', 0)
                if days_held > 7:  # Penalize holding too long
                    reward -= 0.1 * (days_held - 7)
                elif days_held > 0:  # Small reward for patience
                    reward += 0.05
            
            # News sentiment alignment reward
            news_sentiment = market_data.get('news_sentiment', {}).get('compound', 0.0)
            if abs(news_sentiment) > 0.3:  # Strong news sentiment
                if (action == 1 and news_sentiment > 0) or (action == 2 and news_sentiment < 0):
                    reward += 0.2  # Reward trading with sentiment
                elif action != 0:  # Penalize trading against strong sentiment
                    reward -= 0.1
            
            # Technical alignment reward
            df = market_data.get('price_data')
            if df is not None and 'rsi' in df.columns:
                rsi = df['rsi'].iloc[-1]
                if action == 1 and rsi < 40:  # Buy when oversold
                    reward += 0.1
                elif action == 2 and rsi > 60:  # Sell when overbought
                    reward += 0.1
                elif action != 0 and (rsi < 20 or rsi > 80):  # Penalize extreme trades
                    reward -= 0.15
            
            # Drawdown penalty
            if position_data and position_data.get('profit_pct', 0) < -0.05:  # More than 5% loss
                reward -= 0.5
            
            # Profit target reward
            if position_data and position_data.get('profit_pct', 0) > 0.02:  # More than 2% profit
                reward += 0.3
            
            # Update reward history
            self.update_reward(reward)
            
            return reward
            
        except Exception as e:
            print(f"❌ [RL Agent] Error calculating reward: {e}")
            return 0.0


# === ENHANCED RL CALLBACKS ===
class EnhancedRewardCallback:
    """Callback to monitor and adjust rewards during training"""
    
    def __init__(self, rl_agent):
        self.rl_agent = rl_agent
        self.episode_rewards = []
        self.call_count = 0
    
    def __call__(self, locals_dict, globals_dict):
        """Called during training to monitor rewards"""
        self.call_count += 1
        
        # Monitor rewards every 1000 steps
        if self.call_count % 1000 == 0:
            recent_rewards = list(self.rl_agent.reward_history)[-100:] if self.rl_agent.reward_history else []
            if recent_rewards:
                avg_reward = np.mean(recent_rewards)
                print(f"🔄 [Reward Monitor] Step {self.call_count}: Avg reward (last 100): {avg_reward:.4f}")
        
        return True  # Continue training

class PerformanceMonitorCallback:
    """Callback to monitor performance metrics during training"""
    
    def __init__(self, rl_agent):
        self.rl_agent = rl_agent
        self.call_count = 0
        self.last_metrics_update = 0
    
    def __call__(self, locals_dict, globals_dict):
        """Called during training to monitor performance"""
        self.call_count += 1
        
        # Update metrics every 5000 steps
        if self.call_count - self.last_metrics_update >= 5000:
            self.rl_agent._calculate_final_metrics()
            metrics = self.rl_agent.performance_metrics
            
            print(f"📊 [Performance Monitor] Step {self.call_count}:")
            print(f"   - Win Rate: {metrics['win_rate']:.2%}")
            print(f"   - Sharpe Ratio: {metrics['sharpe_ratio']:.3f}")
            print(f"   - Max Drawdown: {metrics['max_drawdown']:.2%}")
            
            self.last_metrics_update = self.call_count
        
        return True  # Continue training


# === ENHANCED RL EVALUATION ===
class EnhancedRLEvaluation:
    """Enhanced RL evaluation with comprehensive risk-adjusted metrics"""

    def __init__(self):
        self.evaluation_metrics = {
            'total_reward': [],
            'sharpe_ratio': [],
            'calmar_ratio': [],
            'max_drawdown': [],
            'win_rate': [],
            'profit_factor': [],
            'sortino_ratio': [],
            'var_95': [],
            'cvar_95': []
        }
        self.episode_results = []

    def evaluate_rl_model(self, model, test_env, n_episodes=100):
        """Enhanced RL evaluation with comprehensive risk metrics"""
        results = {
            'episodes': [],
            'rewards': [],
            'returns': [],
            'drawdowns': [],
            'portfolio_values': []
        }

        logging.info(f" Starting enhanced RL evaluation with {n_episodes} episodes...")

        for episode in range(n_episodes):
            obs = test_env.reset()
            episode_reward = 0
            episode_returns = []
            episode_portfolio_values = []

            while True:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, done, info = test_env.step(action)
                episode_reward += reward
                episode_returns.append(reward)

                # Get portfolio value if available
                if hasattr(test_env, 'balance'):
                    episode_portfolio_values.append(test_env.balance)

                if done:
                    break

            results['episodes'].append(episode)
            results['rewards'].append(episode_reward)
            results['returns'].append(episode_returns)
            results['portfolio_values'].append(episode_portfolio_values)

            # Calculate episode-level metrics
            if episode_returns:
                episode_metrics = self._calculate_episode_metrics(episode_returns, episode_portfolio_values)
                self.episode_results.append(episode_metrics)

        # Calculate comprehensive risk metrics
        risk_metrics = self._calculate_comprehensive_metrics(results)

        # Log results
        logging.info(f" RL Evaluation Results:")
        logging.info(f"   - Total Reward: {risk_metrics['total_reward']:.4f}")
        logging.info(f"   - Sharpe Ratio: {risk_metrics['sharpe_ratio']:.4f}")
        logging.info(f"   - Calmar Ratio: {risk_metrics['calmar_ratio']:.4f}")
        logging.info(f"   - Max Drawdown: {risk_metrics['max_drawdown']:.4f}")
        logging.info(f"   - Win Rate: {risk_metrics['win_rate']:.4f}")
        logging.info(f"   - Profit Factor: {risk_metrics['profit_factor']:.4f}")

        return risk_metrics

    def _calculate_episode_metrics(self, returns, portfolio_values):
        """Calculate metrics for a single episode"""
        if not returns:
            return {}

        metrics = {
            'total_return': sum(returns),
            'mean_return': np.mean(returns),
            'std_return': np.std(returns),
            'max_return': np.max(returns),
            'min_return': np.min(returns),
            'win_rate': np.sum(np.array(returns) > 0) / len(returns)
        }

        if portfolio_values:
            metrics['max_drawdown'] = self._calculate_max_drawdown_from_values(portfolio_values)
            metrics['final_value'] = portfolio_values[-1]
            metrics['initial_value'] = portfolio_values[0]

        return metrics

    def _calculate_comprehensive_metrics(self, results):
        """Calculate comprehensive risk-adjusted metrics"""
        all_returns = np.concatenate(results['returns'])
        all_rewards = results['rewards']

        metrics = {
            'total_reward': np.mean(all_rewards),
            'sharpe_ratio': self._calculate_sharpe_ratio(all_returns),
            'calmar_ratio': self._calculate_calmar_ratio(all_returns),
            'max_drawdown': self._calculate_max_drawdown(all_returns),
            'win_rate': self._calculate_win_rate(all_returns),
            'profit_factor': self._calculate_profit_factor(all_returns),
            'sortino_ratio': self._calculate_sortino_ratio(all_returns),
            'var_95': self._calculate_var(all_returns, 0.05),
            'cvar_95': self._calculate_cvar(all_returns, 0.05)
        }

        return metrics

    def _calculate_sharpe_ratio(self, returns):
        """Calculate Sharpe ratio"""
        if len(returns) == 0 or np.std(returns) == 0:
            return 0
        return np.mean(returns) / np.std(returns) * np.sqrt(252)

    def _calculate_calmar_ratio(self, returns):
        """Calculate Calmar ratio"""
        max_dd = self._calculate_max_drawdown(returns)
        if max_dd == 0:
            return 0
        return np.mean(returns) * 252 / abs(max_dd)

    def _calculate_max_drawdown(self, returns):
        """Calculate maximum drawdown from returns"""
        if len(returns) == 0:
            return 0

        cumulative = np.cumprod(1 + returns)
        running_max = np.maximum.accumulate(cumulative)
        drawdown = (cumulative - running_max) / running_max
        return np.min(drawdown)

    def _calculate_max_drawdown_from_values(self, values):
        """Calculate maximum drawdown from portfolio values"""
        if len(values) == 0:
            return 0

        values = np.array(values)
        running_max = np.maximum.accumulate(values)
        drawdown = (values - running_max) / running_max
        return np.min(drawdown)

    def _calculate_win_rate(self, returns):
        """Calculate win rate"""
        if len(returns) == 0:
            return 0
        return np.sum(np.array(returns) > 0) / len(returns)

    def _calculate_profit_factor(self, returns):
        """Calculate profit factor"""
        profits = returns[returns > 0]
        losses = returns[returns < 0]

        if len(losses) == 0:
            return float('inf') if len(profits) > 0 else 0

        return np.sum(profits) / abs(np.sum(losses))

    def _calculate_sortino_ratio(self, returns):
        """Calculate Sortino ratio (downside deviation)"""
        if len(returns) == 0:
            return 0

        downside_returns = returns[returns < 0]
        if len(downside_returns) == 0:
            return float('inf') if np.mean(returns) > 0 else 0

        downside_std = np.std(downside_returns)
        if downside_std == 0:
            return 0

        return np.mean(returns) / downside_std * np.sqrt(252)

    def _calculate_var(self, returns, confidence_level):
        """Calculate Value at Risk"""
        if len(returns) == 0:
            return 0
        return np.percentile(returns, confidence_level * 100)

    def _calculate_cvar(self, returns, confidence_level):
        """Calculate Conditional Value at Risk (Expected Shortfall)"""
        if len(returns) == 0:
            return 0

        var = self._calculate_var(returns, confidence_level)
        return np.mean(returns[returns <= var])

    def get_evaluation_summary(self):
        """Get comprehensive evaluation summary"""
        if not self.episode_results:
            return {}

        summary = {
            'total_episodes': len(self.episode_results),
            'mean_total_return': np.mean([ep['total_return'] for ep in self.episode_results]),
            'std_total_return': np.std([ep['total_return'] for ep in self.episode_results]),
            'mean_win_rate': np.mean([ep['win_rate'] for ep in self.episode_results]),
            'mean_max_drawdown': np.mean([ep.get('max_drawdown', 0) for ep in self.episode_results]),
            'best_episode': max(self.episode_results, key=lambda x: x['total_return']),
            'worst_episode': min(self.episode_results, key=lambda x: x['total_return'])
        }

        return summary

# === ENHANCED SESSION MANAGER ===
class EnhancedSessionManager:
    """Enhanced session management with proper timezone handling and break periods"""

    def __init__(self):
        self.timezone_mapping = {
            "this": "America/New_York",
            "LON": "Europe/London",
            "TOK": "Asia/Tokyo",
            "SYD": "Australia/Sydney",
            "UTC": "UTC"
        }

        self.session_breaks = {
            "FOREX": {
                "lunch_break": {"start": "21:00", "end": "22:00", "timezone": "UTC"},
                "weekend": {"start": "21:00", "end": "21:00", "days": ["Saturday", "Sunday"]}
            },
            "CME": {
                "lunch_break": {"start": "21:00", "end": "22:00", "timezone": "UTC"},
                "maintenance": {"start": "17:00", "end": "18:00", "timezone": "UTC", "days": ["Sunday"]}
            },
            "COMEX": {
                "lunch_break": {"start": "21:00", "end": "22:00", "timezone": "UTC"},
                "maintenance": {"start": "17:00", "end": "18:00", "timezone": "UTC", "days": ["Sunday"]}
            },
            "CRYPTO": {
                "no_breaks": True
            }
        }

        self.session_schedules = {
            "FOREX": {
                "monday": {"start": "21:00", "end": "21:00", "timezone": "UTC"},
                "tuesday": {"start": "21:00", "end": "21:00", "timezone": "UTC"},
                "wednesday": {"start": "21:00", "end": "21:00", "timezone": "UTC"},
                "thursday": {"start": "21:00", "end": "21:00", "timezone": "UTC"},
                "friday": {"start": "21:00", "end": "21:00", "timezone": "UTC"}
            },
            "CME": {
                "monday": {"start": "22:00", "end": "21:00", "timezone": "UTC"},
                "tuesday": {"start": "22:00", "end": "21:00", "timezone": "UTC"},
                "wednesday": {"start": "22:00", "end": "21:00", "timezone": "UTC"},
                "thursday": {"start": "22:00", "end": "21:00", "timezone": "UTC"},
                "friday": {"start": "22:00", "end": "21:00", "timezone": "UTC"}
            }
        }

    def wait_for_optimal_entry_time(self, symbol, current_time=None):
        """Enhanced optimal entry time with proper timezone handling"""
        if current_time is None:
            current_time = datetime.now()

        # Get symbol metadata
        metadata = SYMBOL_METADATA.get(symbol, {})
        asset_class = metadata.get("asset_class", "forex")

        # Get session info
        session_info = self._get_session_info(symbol, asset_class)

        # Check if market is open
        if not self._is_market_open(symbol, current_time, session_info):
            next_open = self._get_next_market_open(symbol, current_time, session_info)
            wait_time = (next_open - current_time).total_seconds()

            logging.info(f"Market closed for {symbol}. Waiting {wait_time/3600:.1f} hours until {next_open}")
            return next_open

        # Check for breaks
        if self._is_in_break_period(symbol, current_time, session_info):
            next_resume = self._get_next_resume_time(symbol, current_time, session_info)
            wait_time = (next_resume - current_time).total_seconds()

            logging.info(f"Break period for {symbol}. Waiting {wait_time/60:.1f} minutes until {next_resume}")
            return next_resume

        return current_time

    def _get_session_info(self, symbol, asset_class):
        """Get session information for symbol"""
        if asset_class == "cryptocurrency":
            return self.session_breaks["CRYPTO"]
        elif asset_class in ["commodity", "equity_index"]:
            return self.session_breaks["CME"]
        else:
            return self.session_breaks["FOREX"]

    def _is_market_open(self, symbol, current_time, session_info):
        """Check if market is open"""
        if session_info.get("no_breaks"):
            return True

        # Check weekend
        if current_time.weekday() >= 5:  # Saturday = 5, Sunday = 6
            return False

        # Check break periods
        return not self._is_in_break_period(symbol, current_time, session_info)

    def _is_in_break_period(self, symbol, current_time, session_info):
        """Check if current time is in break period"""
        if session_info.get("no_breaks"):
            return False

        # Check lunch break
        lunch_break = session_info.get("lunch_break")
        if lunch_break and self._is_in_time_period(current_time, lunch_break):
            return True

        # Check maintenance break
        maintenance = session_info.get("maintenance")
        if maintenance and self._is_in_time_period(current_time, maintenance):
            return True

        return False

    def _is_in_time_period(self, current_time, period_config):
        """Check if current time is within a specififromime period"""
        try:
            start_time = datetime.strptime(period_config["start"], "%H:%M").time()
            end_time = datetime.strptime(period_config["end"], "%H:%M").time()
            current_time_only = current_time.time()

            # Handle overnight periods
            if start_time <= end_time:
                return start_time <= current_time_only <= end_time
            else:
                return current_time_only >= start_time or current_time_only <= end_time
        except Exception as e:
            logging.warning(f"Error checking time period: {e}")
            return False

    def _get_next_market_open(self, symbol, current_time, session_info):
        """Get nexfrom modelarketopen time"""
        if session_info.get("no_breaks"):
            return current_time

        # Find next weekday
        next_open = current_time
        while next_open.weekday() >= 5:  # Skip weekends
            next_open += timedelta(days=1)

        # Set to marketopen time
        market_open_time = datetime.strptime("22:00", "%H:%M").time()
        next_open = next_open.replace(hour=market_open_time.hour, minute=market_open_time.minute, second=0, microsecond=0)

        return next_open

    def _get_next_resume_time(self, symbol, current_time, session_info):
        """Get next resume time after break"""
        if session_info.get("no_breaks"):
            return current_time

        # Get lunch break end time
        lunch_break = session_info.get("lunch_break")
        if lunch_break:
            end_time = datetime.strptime(lunch_break["end"], "%H:%M").time()
            resume_time = current_time.replace(hour=end_time.hour, minute=end_time.minute, second=0, microsecond=0)

            # If resume time is tomorrow
            if resume_time <= current_time:
                resume_time += timedelta(days=1)

            return resume_time

        return current_time

    def get_session_status(self, symbol, current_time=None):
        """Get comprehensive session status for symbol"""
        if current_time is None:
            current_time = datetime.now()

        metadata = SYMBOL_METADATA.get(symbol, {})
        asset_class = metadata.get("asset_class", "forex")
        session_info = self._get_session_info(symbol, asset_class)

        status = {
            "symbol": symbol,
            "asset_class": asset_class,
            "current_time": current_time,
            "is_market_open": self._is_market_open(symbol, current_time, session_info),
            "is_in_break": self._is_in_break_period(symbol, current_time, session_info),
            "session_info": session_info
        }

        if not status["is_market_open"]:
            status["next_open"] = self._get_next_market_open(symbol, current_time, session_info)

        if status["is_in_break"]:
            status["next_resume"] = self._get_next_resume_time(symbol, current_time, session_info)

        return status

# ==============================================================================
# ENHANCED RL FLEXIBILITY CLASSES
# ==============================================================================

class MarketRegimeDetector:
    """Detecfrom modelarket regime (trending, ranging, volatile) for adaptive RL"""
    
    def __init__(self):
        self.regime_history = []
        self.current_regime = "ranging"
        self.regime_confidence = 0.5
        
    def detect_regime(self, symbol_data):
        """Detect fromarket regime based on price action"""
        try:
            if len(symbol_data) < 50:
                return "ranging", 0.5
                
            # Calculate regime indicators
            price_series = symbol_data['close'].tail(50)
            
            # Trend strength
            sma_20 = price_series.rolling(20).mean()
            trend_strength = abs((price_series.iloc[-1] - sma_20.iloc[-1]) / sma_20.iloc[-1])
            
            # Volatility
            volatility = price_series.pct_change().std() * 100
            
            # Range detection
            high_20 = price_series.rolling(20).max()
            low_20 = price_series.rolling(20).min()
            range_size = (high_20.iloc[-1] - low_20.iloc[-1]) / price_series.iloc[-1]
            
            # Determine regime
            if trend_strength > 0.02 and volatility < 2.0:
                regime = "trending"
                confidence = min(0.9, trend_strength * 10)
            elif volatility > 3.0:
                regime = "volatile"
                confidence = min(0.9, volatility / 5)
            else:
                regime = "ranging"
                confidence = min(0.9, range_size * 10)
                
            self.current_regime = regime
            self.regime_confidence = confidence
            
            return regime, confidence
            
        except Exception as e:
            logging.error(f"MarketRegimeDetector error: {e}")
            return "ranging", 0.5

class RLPerformanceTracker:
    """Track RL Agent performance for adaptive learning"""
    
    def __init__(self):
        self.symbol_performance = {}
        self.action_success_rates = {}
        self.confidence_calibration = {}
        
    def update_performance(self, symbol, action, confidence, outcome):
        """Update performance metrics"""
        if symbol not in self.symbol_performance:
            self.symbol_performance[symbol] = {
                'total_actions': 0,
                'successful_actions': 0,
                'confidence_history': [],
                'outcome_history': []
            }
            
        perf = self.symbol_performance[symbol]
        perf['total_actions'] += 1
        perf['confidence_history'].append(confidence)
        perf['outcome_history'].append(outcome)
        
        if outcome > 0:  # Positive outcome
            perf['successful_actions'] += 1
            
        # Update action success rates
        action_key = f"{symbol}_{action}"
        if action_key not in self.action_success_rates:
            self.action_success_rates[action_key] = {'total': 0, 'success': 0}
            
        self.action_success_rates[action_key]['total'] += 1
        if outcome > 0:
            self.action_success_rates[action_key]['success'] += 1
            
    def get_adaptive_threshold(self, symbol):
        """Get adaptive confidence threshold based on performance"""
        if symbol not in self.symbol_performance:
            return 0.52  # Default threshold
            
        perf = self.symbol_performance[symbol]
        if perf['total_actions'] < 10:
            return 0.52  # Not enough data
            
        success_rate = perf['successful_actions'] / perf['total_actions']
        
        # Adjust threshold based on success rate
        if success_rate > 0.7:
            return 0.48  # Lower threshold for high-performing symbols
        elif success_rate < 0.4:
            return 0.58  # Higher threshold for low-performing symbols
        else:
            return 0.52  # Default threshold

class DynamicActionSpace:
    """Dynamiofction space based on market conditions"""
    
    def __init__(self):
        self.base_actions = [0, 1, 2]  # HOLD, BUY, SELL
        self.extended_actions = [0, 1, 2, 3, 4]  # HOLD, BUY, SELL, Sin_BUY, Sin_SELL
        
    def get_action_space(self, symbol, market_regime, volatility):
        """Get appropriate action space based on conditions"""
        if market_regime == "volatile" and volatility > 3.0:
            return self.extended_actions  # More aggressive actions in volatile markets
        elif market_regime == "trending":
            return self.base_actions  # Standard actions in trending markets
        else:
            return self.base_actions  # Conservative actions in ranging markets
            
    def decode_action(self, action_code, symbol, market_regime):
        """Decode action code to trading signal"""
        action_map = {
            0: "HOLD",
            1: "BUY", 
            2: "SELL",
            3: "Sin_BUY",
            4: "Sin_SELL"
        }
        
        action_name = action_map.get(action_code, "HOLD")
        
        # Adjust confidence based on action strength
        if action_name in ["Sin_BUY", "Sin_SELL"]:
            confidence_multiplier = 1.2
        else:
            confidence_multiplier = 1.0
            
        return action_name, confidence_multiplier

class TaskSelector:
    """Select appropriate task-specific model based on market conditions"""
    
    def __init__(self):
        self.task_weights = {
            'trending': 0.3,
            'ranging': 0.3,
            'news_driven': 0.2,
            'volatile': 0.2
        }
        
    def select_task(self, symbol, market_regime, news_impact, volatility):
        """Select the most appropriate task for current conditions"""
        task_scores = {}
        
        # Trending task score
        if market_regime == "trending":
            task_scores['trending'] = 0.8
        else:
            task_scores['trending'] = 0.2
            
        # Ranging task score
        if market_regime == "ranging":
            task_scores['ranging'] = 0.8
        else:
            task_scores['ranging'] = 0.2
            
        # News-driven task score
        if news_impact > 0.6:
            task_scores['news_driven'] = 0.8
        else:
            task_scores['news_driven'] = 0.2
            
        # Volatile task score
        if volatility > 3.0:
            task_scores['volatile'] = 0.8
        else:
            task_scores['volatile'] = 0.2
            
        # Select task with highest score
        selected_task = max(task_scores, key=task_scores.get)
        confidence = task_scores[selected_task]
        
        return selected_task, confidence

class TransferLearningManager:
    """Manage knowledge transfer between symbols for RL Agent"""
    
    def __init__(self):
        self.symbol_similarity_matrix = {}
        self.knowledge_base = {}
        self.transfer_weights = {}
        
    def calculate_symbol_similarity(self, symbol1, symbol2, market_data):
        """Calculate similarity between two symbols"""
        try:
            # Get price data for both symbols
            data1 = market_data.get(symbol1, pd.DataFrame())
            data2 = market_data.get(symbol2, pd.DataFrame())
            
            if len(data1) < 50 or len(data2) < 50:
                return 0.5  # Default similarity
                
            # Calculate correlation
            price1 = data1['close'].pct_change().dropna()
            price2 = data2['close'].pct_change().dropna()
            
            # Align data
            min_len = min(len(price1), len(price2))
            price1 = price1.tail(min_len)
            price2 = price2.tail(min_len)
            
            correlation = price1.corr(price2)
            similarity = abs(correlation) if not pd.isna(correlation) else 0.5
            
            return similarity
            
        except Exception as e:
            logging.error(f"Error calculating similarity between {symbol1} and {symbol2}: {e}")
            return 0.5
    
    def build_similarity_matrix(self, symbols, market_data):
        """Build similarity matrix for all symbols"""
        self.symbol_similarity_matrix = {}
        
        for sym1 in symbols:
            self.symbol_similarity_matrix[sym1] = {}
            for sym2 in symbols:
                if sym1 != sym2:
                    similarity = self.calculate_symbol_similarity(sym1, sym2, market_data)
                    self.symbol_similarity_matrix[sym1][sym2] = similarity
                else:
                    self.symbol_similarity_matrix[sym1][sym2] = 1.0
    
    def get_transfer_weights(self, target_symbol, source_symbols):
        """Get transfer weights for knowledge transfer"""
        if target_symbol not in self.symbol_similarity_matrix:
            return {sym: 0.0 for sym in source_symbols}
            
        similarities = self.symbol_similarity_matrix[target_symbol]
        weights = {}
        
        for sym in source_symbols:
            if sym in similarities:
                weights[sym] = similarities[sym]
            else:
                weights[sym] = 0.0
                
        # Normalize weights
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v/total_weight for k, v in weights.items()}
        
        return weights
    
    def transfer_knowledge(self, target_symbol, source_symbols, source_performance):
        """Transfer knowledge from source symbols to target symbol"""
        transfer_weights = self.get_transfer_weights(target_symbol, source_symbols)
        
        # Calculate weighted performance
        weighted_performance = 0.0
        total_weight = 0.0
        
        for sym, weight in transfer_weights.items():
            if sym in source_performance and weight > 0:
                weighted_performance += source_performance[sym] * weight
                total_weight += weight
        
        if total_weight > 0:
            transferred_performance = weighted_performance / total_weight
        else:
            transferred_performance = 0.5  # Default performance
            
        return transferred_performance, transfer_weights
    
    def decide_tp_sl_levels(self, symbol, entry_price, direction, market_data):
        """
        Enhanced TP/SL decision making based on multi-factor analysis
        """
        try:
            print(f"🎯 [Master Agent] Deciding TP/SL levels for {symbol}")
            
            # Gather analysis from specialist agents
            analysis_results = {}
            
            # Get volatility analysis
            if 'volatility_predictor' in self.specialist_agents:
                volatility_analysis = self.specialist_agents['volatility_predictor'].analyze(market_data, symbol)
                analysis_results['volatility'] = volatility_analysis
            
            # Get risk analysis
            if 'risk_manager' in self.specialist_agents:
                risk_analysis = self.specialist_agents['risk_manager'].analyze(market_data, symbol)
                analysis_results['risk'] = risk_analysis
            
            # Get trend analysis
            if 'trend_analyzer' in self.specialist_agents:
                trend_analysis = self.specialist_agents['trend_analyzer'].analyze(market_data, symbol)
                analysis_results['trend'] = trend_analysis
            
            # Calculate ATR-based levels
            atr_levels = self._calculate_atr_based_levels(symbol, entry_price, market_data)
            
            # Calculate support/resistance levels
            sr_levels = self._calculate_support_resistance_levels(symbol, market_data)
            
            # Calculate volatility-adjusted levels
            volatility_levels = self._calculate_volatility_adjusted_levels(
                symbol, entry_price, direction, analysis_results.get('volatility')
            )
            
            # Apply intelligent decision logic
            tp_sl_decision = self._apply_tp_sl_decision_logic(
                symbol, entry_price, direction, atr_levels, sr_levels, 
                volatility_levels, analysis_results
            )
            
            # Store decision history
            self.tp_sl_history[symbol] = {
                'timestamp': datetime.now(),
                'entry_price': entry_price,
                'direction': direction,
                'decision': tp_sl_decision,
                'analysis_results': analysis_results
            }
            
            print(f"✅ [Master Agent] TP/SL Decision for {symbol}: TP={tp_sl_decision['take_profit']:.6f}, SL={tp_sl_decision['stop_loss']:.6f}")
            
            return tp_sl_decision
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in TP/SL decision: {e}")
            # Fallback to default levels
            return self._get_default_tp_sl_levels(entry_price, direction)
    
    def _calculate_atr_based_levels(self, symbol, entry_price, market_data):
        """Calculate TP/SL levels based on ATR"""
        try:
            # Get recent price data
            df = market_data.get('price_data')
            if df is None or len(df) < 20:
                return None
            
            # Calculate ATR
            from ta.volatility import AverageTrueRange
            atr_indicator = AverageTrueRange(df["high"], df["low"], df["close"], window=14)
            atr_value = atr_indicator.average_true_range().iloc[-1]
            
            return {
                'atr_value': atr_value,
                'tp_multiplier': 2.0,  # 2x ATR for TP
                'sl_multiplier': 1.0   # 1x ATR for SL
            }
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating ATR levels: {e}")
            return None
    
    def _calculate_support_resistance_levels(self, symbol, market_data):
        """Calculate support and resistance levels"""
        try:
            df = market_data.get('price_data')
            if df is None or len(df) < 50:
                return None
            
            # Find pivot points
            highs = df['high'].rolling(window=10, center=True).max()
            lows = df['low'].rolling(window=10, center=True).min()
            
            resistance_levels = df[df['high'] == highs]['high'].dropna().tail(5).tolist()
            support_levels = df[df['low'] == lows]['low'].dropna().tail(5).tolist()
            
            return {
                'resistance_levels': resistance_levels,
                'support_levels': support_levels
            }
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating S/R levels: {e}")
            return None
    
    def _calculate_volatility_adjusted_levels(self, symbol, entry_price, direction, volatility_analysis):
        """Calculate volatility-adjusted TP/SL levels"""
        try:
            if not volatility_analysis:
                return None
            
            # Extract volatility metrics
            volatility_score = volatility_analysis[1] if isinstance(volatility_analysis, tuple) else 0.5
            
            # Adjust multipliers based on volatility
            if volatility_score > 0.7:  # High volatility
                tp_multiplier = 3.0
                sl_multiplier = 1.5
            elif volatility_score > 0.5:  # Medium volatility
                tp_multiplier = 2.5
                sl_multiplier = 1.2
            else:  # Low volatility
                tp_multiplier = 2.0
                sl_multiplier = 1.0
            
            return {
                'volatility_score': volatility_score,
                'tp_multiplier': tp_multiplier,
                'sl_multiplier': sl_multiplier
            }
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating volatility levels: {e}")
            return None
    
    def _apply_tp_sl_decision_logic(self, symbol, entry_price, direction, atr_levels, 
                                   sr_levels, volatility_levels, analysis_results):
        """Apply intelligent decision logic for TP/SL"""
        try:
            # Base calculations
            base_tp_multiplier = 2.0
            base_sl_multiplier = 1.0
            
            # Adjust based on ATR
            if atr_levels:
                atr_value = atr_levels['atr_value']
                base_tp_distance = atr_value * atr_levels['tp_multiplier']
                base_sl_distance = atr_value * atr_levels['sl_multiplier']
            else:
                # Fallback to percentage-based
                base_tp_distance = entry_price * 0.02  # 2%
                base_sl_distance = entry_price * 0.01  # 1%
            
            # Adjust based on volatility
            if volatility_levels:
                vol_tp_multiplier = volatility_levels['tp_multiplier']
                vol_sl_multiplier = volatility_levels['sl_multiplier']
                base_tp_distance *= (vol_tp_multiplier / 2.0)
                base_sl_distance *= (vol_sl_multiplier / 1.0)
            
            # Adjust based on trend strength
            trend_adjustment = 1.0
            if analysis_results.get('trend'):
                trend_confidence = analysis_results['trend'][1] if isinstance(analysis_results['trend'], tuple) else 0.5
                if trend_confidence > 0.7:
                    trend_adjustment = 1.2  # Increase TP in strong trends
                elif trend_confidence < 0.3:
                    trend_adjustment = 0.8  # Decrease TP in weak trends
            
            base_tp_distance *= trend_adjustment
            
            # Calculate final levels
            if direction.upper() == 'BUY':
                take_profit = entry_price + base_tp_distance
                stop_loss = entry_price - base_sl_distance
            else:  # SELL
                take_profit = entry_price - base_tp_distance
                stop_loss = entry_price + base_sl_distance
            
            # Adjust based on support/resistance levels
            if sr_levels:
                if direction.upper() == 'BUY':
                    # Adjust TP to nearest resistance
                    nearby_resistance = [r for r in sr_levels['resistance_levels'] if r > entry_price]
                    if nearby_resistance:
                        closest_resistance = min(nearby_resistance)
                        if closest_resistance < take_profit:
                            take_profit = closest_resistance * 0.99  # Slightly below resistance
                    
                    # Adjust SL to nearest support
                    nearby_support = [s for s in sr_levels['support_levels'] if s < entry_price]
                    if nearby_support:
                        closest_support = max(nearby_support)
                        if closest_support > stop_loss:
                            stop_loss = closest_support * 1.01  # Slightly above support
                else:  # SELL
                    # Adjust TP to nearest support
                    nearby_support = [s for s in sr_levels['support_levels'] if s < entry_price]
                    if nearby_support:
                        closest_support = max(nearby_support)
                        if closest_support > take_profit:
                            take_profit = closest_support * 1.01  # Slightly above support
                    
                    # Adjust SL to nearest resistance
                    nearby_resistance = [r for r in sr_levels['resistance_levels'] if r > entry_price]
                    if nearby_resistance:
                        closest_resistance = min(nearby_resistance)
                        if closest_resistance < stop_loss:
                            stop_loss = closest_resistance * 0.99  # Slightly below resistance
            
            return {
                'take_profit': take_profit,
                'stop_loss': stop_loss,
                'tp_distance': abs(take_profit - entry_price),
                'sl_distance': abs(stop_loss - entry_price),
                'risk_reward_ratio': abs(take_profit - entry_price) / abs(stop_loss - entry_price),
                'reasoning': {
                    'atr_based': atr_levels is not None,
                    'volatility_adjusted': volatility_levels is not None,
                    'trend_adjusted': trend_adjustment != 1.0,
                    'sr_adjusted': sr_levels is not None
                }
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in TP/SL decision logic: {e}")
            return self._get_default_tp_sl_levels(entry_price, direction)
    
    def _get_default_tp_sl_levels(self, entry_price, direction):
        """Get default TP/SL levels as fallback"""
        if direction.upper() == 'BUY':
            return {
                'take_profit': entry_price * 1.02,  # 2% profit
                'stop_loss': entry_price * 0.99,    # 1% loss
                'tp_distance': entry_price * 0.02,
                'sl_distance': entry_price * 0.01,
                'risk_reward_ratio': 2.0,
                'reasoning': {'default_fallback': True}
            }
        else:
            return {
                'take_profit': entry_price * 0.98,  # 2% profit
                'stop_loss': entry_price * 1.01,    # 1% loss
                'tp_distance': entry_price * 0.02,
                'sl_distance': entry_price * 0.01,
                'risk_reward_ratio': 2.0,
                'reasoning': {'default_fallback': True}
            }
    
    def decide_trailing_stop_activation(self, symbol, current_price, entry_price, direction, market_data):
        """
        Decide when to activate trailing stop based on market conditions
        """
        try:
            print(f"🎯 [Master Agent] Analyzing trailing stop activation for {symbol}")
            
            # Calculate current profit
            if direction.upper() == 'BUY':
                profit_pct = (current_price - entry_price) / entry_price
            else:
                profit_pct = (entry_price - current_price) / entry_price
            
            # Get analysis from specialist agents
            analysis_results = {}
            
            if 'trend_analyzer' in self.specialist_agents:
                trend_analysis = self.specialist_agents['trend_analyzer'].analyze(market_data, symbol)
                analysis_results['trend'] = trend_analysis
            
            if 'volatility_predictor' in self.specialist_agents:
                volatility_analysis = self.specialist_agents['volatility_predictor'].analyze(market_data, symbol)
                analysis_results['volatility'] = volatility_analysis
            
            # Decision logic
            should_activate = False
            activation_reason = []
            
            # Rule 1: Activate if profit exceeds 1.5%
            if profit_pct > 0.015:
                should_activate = True
                activation_reason.append("Profit threshold reached (>1.5%)")
            
            # Rule 2: Strong trend continuation
            if analysis_results.get('trend'):
                trend_signal, trend_confidence = analysis_results['trend']
                if trend_confidence > 0.7 and profit_pct > 0.01:
                    should_activate = True
                    activation_reason.append(f"Strong trend continuation ({trend_confidence:.2%})")
            
            # Rule 3: High volatility environment - be more conservative
            if analysis_results.get('volatility'):
                volatility_score = analysis_results['volatility'][1] if isinstance(analysis_results['volatility'], tuple) else 0.5
                if volatility_score > 0.8 and profit_pct > 0.02:
                    should_activate = True
                    activation_reason.append("High volatility - protective trailing")
            
            # Store decision
            self.trailing_stop_decisions[symbol] = {
                'timestamp': datetime.now(),
                'should_activate': should_activate,
                'current_profit_pct': profit_pct,
                'reasons': activation_reason,
                'analysis_results': analysis_results
            }
            
            print(f"✅ [Master Agent] Trailing stop decision for {symbol}: {'ACTIVATE' if should_activate else 'HOLD'}")
            if activation_reason:
                print(f"   Reasons: {', '.join(activation_reason)}")
            
            return {
                'should_activate': should_activate,
                'reasons': activation_reason,
                'current_profit_pct': profit_pct,
                'recommended_distance': self._calculate_trailing_distance(symbol, current_price, volatility_analysis)
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in trailing stop decision: {e}")
            return {'should_activate': False, 'reasons': ['Error in analysis'], 'current_profit_pct': 0}
    
    def _calculate_trailing_distance(self, symbol, current_price, volatility_analysis):
        """Calculate optimal trailing stop distance"""
        try:
            base_distance = current_price * 0.01  # 1% base
            
            if volatility_analysis:
                volatility_score = volatility_analysis[1] if isinstance(volatility_analysis, tuple) else 0.5
                # Higher volatility = larger trailing distance
                volatility_multiplier = 1 + volatility_score
                base_distance *= volatility_multiplier
            
            return base_distance
            
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating trailing distance: {e}")
            return current_price * 0.01
    
    def analyze_optimal_entry_point(self, symbol, signal_direction, market_data):
        """
        Analyze and determine optimal entry points using multi-factor analysis
        """
        try:
            print(f"🎯 [Master Agent] Analyzing optimal entry point for {symbol}")
            
            # Get current price
            current_price = market_data.get('current_price')
            if not current_price:
                df = market_data.get('price_data')
                if df is not None and not df.empty:
                    current_price = df['close'].iloc[-1]
                else:
                    print(f"❌ [Master Agent] No price data available for {symbol}")
                    return None
            
            # Gather comprehensive analysis
            analysis_results = {}
            
            # Technical analysis
            technical_score = self._analyze_technical_entry_conditions(symbol, market_data)
            analysis_results['technical'] = technical_score
            
            # Volume analysis
            volume_score = self._analyze_volume_conditions(symbol, market_data)
            analysis_results['volume'] = volume_score
            
            # Market structure analysis
            structure_score = self._analyze_market_structure(symbol, market_data)
            analysis_results['structure'] = structure_score
            
            # Volatility timing analysis
            volatility_score = self._analyze_volatility_timing(symbol, market_data)
            analysis_results['volatility'] = volatility_score
            
            # News sentiment timing
            news_score = self._analyze_news_timing(symbol, market_data)
            analysis_results['news'] = news_score
            
            # Calculate composite entry score
            entry_analysis = self._calculate_entry_score(
                signal_direction, analysis_results, current_price
            )
            
            # Store analysis
            self.entry_point_analysis[symbol] = {
                'timestamp': datetime.now(),
                'signal_direction': signal_direction,
                'current_price': current_price,
                'analysis_results': analysis_results,
                'entry_decision': entry_analysis
            }
            
            print(f"✅ [Master Agent] Entry analysis for {symbol}: Score={entry_analysis['composite_score']:.2f}, Recommendation={entry_analysis['recommendation']}")
            
            return entry_analysis
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in entry point analysis: {e}")
            return {
                'recommendation': 'WAIT',
                'composite_score': 0.5,
                'reasoning': ['Error in analysis'],
                'optimal_price': current_price
            }
    
    def _analyze_technical_entry_conditions(self, symbol, market_data):
        """Analyze technical conditions for entry"""
        try:
            df = market_data.get('price_data')
            if df is None or len(df) < 50:
                return 0.5
            
            score = 0.0
            factors = 0
            
            # RSI conditions
            if 'rsi' in df.columns:
                rsi = df['rsi'].iloc[-1]
                if 30 <= rsi <= 70:  # Not overbought/oversold
                    score += 0.8
                elif 20 <= rsi <= 80:
                    score += 0.6
                else:
                    score += 0.3
                factors += 1
            
            # MACD conditions
            if 'macd' in df.columns and 'macd_signal' in df.columns:
                macd = df['macd'].iloc[-1]
                macd_signal = df['macd_signal'].iloc[-1]
                macd_prev = df['macd'].iloc[-2]
                macd_signal_prev = df['macd_signal'].iloc[-2]
                
                # Check for MACD crossover
                if (macd > macd_signal and macd_prev <= macd_signal_prev):  # Bullish crossover
                    score += 0.9
                elif (macd < macd_signal and macd_prev >= macd_signal_prev):  # Bearish crossover
                    score += 0.9
                elif macd > macd_signal:  # Above signal line
                    score += 0.6
                else:
                    score += 0.4
                factors += 1
            
            # Moving average alignment
            if 'ema_20' in df.columns and 'ema_50' in df.columns:
                ema20 = df['ema_20'].iloc[-1]
                ema50 = df['ema_50'].iloc[-1]
                current_price = df['close'].iloc[-1]
                
                if ema20 > ema50 and current_price > ema20:  # Bullish alignment
                    score += 0.8
                elif ema20 < ema50 and current_price < ema20:  # Bearish alignment
                    score += 0.8
                else:
                    score += 0.5
                factors += 1
            
            return score / max(factors, 1)
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in technical analysis: {e}")
            return 0.5
    
    def _analyze_volume_conditions(self, symbol, market_data):
        """Analyze volume conditions for entry timing"""
        try:
            df = market_data.get('price_data')
            if df is None or 'volume' not in df.columns or len(df) < 20:
                return 0.5
            
            current_volume = df['volume'].iloc[-1]
            avg_volume = df['volume'].rolling(20).mean().iloc[-1]
            
            volume_ratio = current_volume / avg_volume
            
            # Higher volume = better entry conditions
            if volume_ratio > 1.5:
                return 0.9
            elif volume_ratio > 1.2:
                return 0.8
            elif volume_ratio > 0.8:
                return 0.6
            else:
                return 0.4
                
        except Exception as e:
            print(f"❌ [Master Agent] Error in volume analysis: {e}")
            return 0.5
    
    def _analyze_market_structure(self, symbol, market_data):
        """Analyze market structure for entry timing"""
        try:
            df = market_data.get('price_data')
            if df is None or len(df) < 30:
                return 0.5
            
            # Look for pullbacks in trends
            recent_highs = df['high'].rolling(10).max()
            recent_lows = df['low'].rolling(10).min()
            current_price = df['close'].iloc[-1]
            
            # Check if price is near support/resistance
            resistance_distance = abs(current_price - recent_highs.iloc[-1]) / current_price
            support_distance = abs(current_price - recent_lows.iloc[-1]) / current_price
            
            # Closer to support/resistance = better entry
            min_distance = min(resistance_distance, support_distance)
            
            if min_distance < 0.01:  # Within 1%
                return 0.9
            elif min_distance < 0.02:  # Within 2%
                return 0.7
            elif min_distance < 0.05:  # Within 5%
                return 0.6
            else:
                return 0.4
                
        except Exception as e:
            print(f"❌ [Master Agent] Error in structure analysis: {e}")
            return 0.5
    
    def _analyze_volatility_timing(self, symbol, market_data):
        """Analyze volatility for entry timing"""
        try:
            df = market_data.get('price_data')
            if df is None or len(df) < 20:
                return 0.5
            
            # Calculate recent volatility
            returns = df['close'].pct_change().dropna()
            current_vol = returns.rolling(10).std().iloc[-1]
            avg_vol = returns.rolling(20).std().mean()
            
            vol_ratio = current_vol / avg_vol
            
            # Moderate volatility is better for entries
            if 0.8 <= vol_ratio <= 1.2:
                return 0.9
            elif 0.6 <= vol_ratio <= 1.5:
                return 0.7
            else:
                return 0.5
                
        except Exception as e:
            print(f"❌ [Master Agent] Error in volatility timing: {e}")
            return 0.5
    
    def _analyze_news_timing(self, symbol, market_data):
        """Analyze news sentiment timing"""
        try:
            # Check if news analysis is available
            news_sentiment = market_data.get('news_sentiment')
            if not news_sentiment:
                return 0.6  # Neutral when no news
            
            sentiment_score = news_sentiment.get('compound', 0)
            
            # Convert sentiment to entry timing score
            if abs(sentiment_score) > 0.5:  # Strong sentiment
                return 0.8
            elif abs(sentiment_score) > 0.2:  # Moderate sentiment
                return 0.7
            else:  # Neutral sentiment
                return 0.6
                
        except Exception as e:
            print(f"❌ [Master Agent] Error in news timing: {e}")
            return 0.6
    
    def _calculate_entry_score(self, signal_direction, analysis_results, current_price):
        """Calculate composite entry score and recommendation"""
        try:
            # Weighted scoring
            weights = {
                'technical': 0.3,
                'volume': 0.2,
                'structure': 0.25,
                'volatility': 0.15,
                'news': 0.1
            }
            
            composite_score = 0.0
            total_weight = 0.0
            
            for factor, score in analysis_results.items():
                if factor in weights:
                    composite_score += score * weights[factor]
                    total_weight += weights[factor]
            
            if total_weight > 0:
                composite_score = composite_score / total_weight
            else:
                composite_score = 0.5
            
            # Determine recommendation
            if composite_score >= 0.8:
                recommendation = 'ENTER_NOW'
                reasoning = ['Excellent entry conditions']
            elif composite_score >= 0.7:
                recommendation = 'ENTER_SOON'
                reasoning = ['Good entry conditions']
            elif composite_score >= 0.6:
                recommendation = 'WAIT_FOR_BETTER'
                reasoning = ['Moderate conditions - wait for improvement']
            else:
                recommendation = 'AVOID'
                reasoning = ['Poor entry conditions']
            
            # Add specific reasoning
            for factor, score in analysis_results.items():
                if score >= 0.8:
                    reasoning.append(f'Strong {factor} signal')
                elif score <= 0.4:
                    reasoning.append(f'Weak {factor} signal')
            
            return {
                'composite_score': composite_score,
                'recommendation': recommendation,
                'reasoning': reasoning,
                'optimal_price': current_price,
                'factor_scores': analysis_results
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating entry score: {e}")
            return {
                'composite_score': 0.5,
                'recommendation': 'WAIT',
                'reasoning': ['Error in calculation'],
                'optimal_price': current_price
            }
        
    def initialize_specialist_agents(self):
        """Khởi tạo all specialist agents"""
        print("🎯 [Master Agent Coordinator] Starting specialist agents initialization...")
        
        try:
            self.specialist_agents = {
                'trend_analyzer': TrendAnalysisAgent(),
                'news_analyzer': NewsAnalysisAgent(), 
                'risk_manager': RiskManagementAgent(),
                'sentiment_analyzer': SentimentAnalysisAgent(),
                'volatility_predictor': VolatilityPredictionAgent(),
                'portfolio_optimizer': PortfolioOptimizationAgent()
            }
            print(f" [Master Agent Coordinator] Initialized {len(self.specialist_agents)} specialist agents")
            
            # setup p communication matrix
            self._setup_communication_matrix()
            print(" [Master Agent Coordinator] Communication matrix setup completed")
            
        except Exception as e:
            print(f" [Master Agent Coordinator] Error initializing specialist agents: {e}")
            import traceback
            traceback.print_exc()
            # Fallback: to empty specialist agents
            self.specialist_agents = {}
    
    def coordinate_decision(self, task_type, market_data, symbol):
        """decision coordination for all specialist agents"""
        print(f"[Master Agent Coordinator] Starting coordinate_decision for {symbol}")
        logging.info(f"[Master Agent Coordinator] Starting coordinate_decision for {symbol}")
        
        try:
            # Decompose task
            subtasks = self.decompose_task(task_type, market_data)
            print(f" [Master Agent Coordinator] Decomposed tasks: {list(subtasks.keys())}")
            
            # Collect opinions from specialist agents
            agent_opinions = {}
            agent_confidences = {}
            
            for subtask_name, data in subtasks.items():
                # Map subtask names to agent names
                agent_mapping = {
                    'trend_analysis': 'trend_analyzer',
                    'news_analysis': 'news_analyzer', 
                    'risk_assessment': 'risk_manager',
                    'sentiment_analysis': 'sentiment_analyzer',
                    'volatility_prediction': 'volatility_predictor',
                    'portfolio_optimization': 'portfolio_optimizer'
                }
                
                agent_name = agent_mapping.get(subtask_name, subtask_name)
                if agent_name in self.specialist_agents:
                    print(f" [Master Agent Coordinator] Starting analysis with {agent_name}")
                    agent = self.specialist_agents[agent_name]
                    try:
                        opinion, confidence = agent.analyze(data, symbol)
                        agent_opinions[subtask_name] = opinion
                        agent_confidences[subtask_name] = confidence
                        print(f" [Master Agent Coordinator] {subtask_name}: {opinion} ({confidence:.2%})")
                    except Exception as e:
                        print(f"[Master Agent Coordinator] Li in {subtask_name}: {e}")
                        agent_opinions[subtask_name] = "HOLD"
                        agent_confidences[subtask_name] = 0.5
            
            # Apply consensus mechanism
            final_decision, final_confidence = self._apply_consensus_mechanism(
                agent_opinions, agent_confidences, symbol
            )
            
            print(f" [Master Agent Coordinator] Final decision cho {symbol}: {final_decision} ({final_confidence:.2%})")
            
            # Update agent performance
            self._update_agent_performance(agent_opinions, agent_confidences, symbol)
            
            return final_decision, final_confidence
            
        except Exception as e:
            print(f"[Master Agent Coordinator] Li in coordinate_decision: {e}")
            logging.error(f"Error in Master Agent coordination: {e}")
            return "HOLD", 0.5
    
    def decompose_task(self, task_type, market_data):
        """Chia nh task thnh subtasks cho specialist agents"""
        if task_type == 'trading_decision':
            return {
                'trend_analysis': market_data,
                'news_analysis': market_data,
                'risk_assessment': market_data,
                'sentiment_analysis': market_data,
                'volatility_prediction': market_data,
                'portfolio_optimization': market_data
            }
        elif task_type == 'position_management':
            return {
                'risk_assessment': market_data,
                'trend_analysis': market_data,
                'portfolio_optimization': market_data
            }
        else:
            return {'general_analysis': market_data}
    
    def _apply_consensus_mechanism(self, opinions, confidences, symbol):
        """Apply consensus mechanism to agent opinions"""
        try:
            if not opinions:
                return "HOLD", 0.5
            
            # Weight opinions by confidence
            weighted_votes = {}
            total_weight = 0
            
            for agent, opinion in opinions.items():
                confidence = confidences.get(agent, 0.5)
                if opinion not in weighted_votes:
                    weighted_votes[opinion] = 0
                weighted_votes[opinion] += confidence
                total_weight += confidence
            
            # Find decision with highest weighted score
            if weighted_votes:
                best_decision = max(weighted_votes, key=weighted_votes.get)
                final_confidence = weighted_votes[best_decision] / total_weight if total_weight > 0 else 0.5
                return best_decision, min(final_confidence, 0.95)
            else:
                return "HOLD", 0.5
                
        except Exception as e:
            print(f"[Master Agent Coordinator] Error in consensus mechanism: {e}")
            return "HOLD", 0.5
    
    def _update_agent_performance(self, opinions, confidences, symbol):
        """Update performance of agents"""
        try:
            if not hasattr(self, 'agent_performance'):
                self.agent_performance = {}
                
            for agent_name in opinions.keys():
                if agent_name not in self.agent_performance:
                    self.agent_performance[agent_name] = {}
                if symbol not in self.agent_performance[agent_name]:
                    self.agent_performance[agent_name][symbol] = 0.5
                    
        except Exception as e:
            print(f"[Master Agent Coordinator] Error updating agent performance: {e}")
        
    def _setup_communication_matrix(self):
        """setup p ma tr n giao ti p gi a cofgents"""
        agents = list(self.specialist_agents.keys())
        self.communication_matrix = {}
        
        for agent1 in agents:
            self.communication_matrix[agent1] = {}
            for agent2 in agents:
                if agent1 != agent2:
                    # Xhas data nh mc dnh datagiao ti p needs thi t
                    if self._should_communicate(agent1, agent2):
                        self.communication_matrix[agent1][agent2] = True
                    else:
                        self.communication_matrix[agent1][agent2] = False
                else:
                    self.communication_matrix[agent1][agent2] = False
    
    def _should_communicate(self, agent1, agent2):
        """Xhas data nh xem 2 agents c needs giao ti p not"""
        communication_pairs = [
            ('trend_analyzer', 'volatility_predictor'),
            ('news_analyzer', 'sentiment_analyzer'),
            ('risk_manager', 'portfolio_optimizer'),
            ('trend_analyzer', 'risk_manager'),
            ('news_analyzer', 'trend_analyzer'),
            ('sentiment_analyzer', 'portfolio_optimizer')
        ]
        
        return (agent1, agent2) in communication_pairs or (agent2, agent1) in communication_pairs

class TrendAnalysisAgent:
    """Specialist agent cho trend analysis"""
    
    def analyze(self, data, symbol):
        """analysis trenda symbol"""
        try:
            if len(data) < 20:
                return "HOLD", 0.5
            
            # Calculate trend indicators
            sma_20 = data['close'].rolling(20).mean()
            sma_50 = data['close'].rolling(50).mean() if len(data) >= 50 else sma_20
            
            current_price = data['close'].iloc[-1]
            sma_20_current = sma_20.iloc[-1]
            sma_50_current = sma_50.iloc[-1]
            
            # Trend strength
            trend_strength = abs(current_price - sma_20_current) / sma_20_current
            
            # Determine trend direction
            if current_price > sma_20_current > sma_50_current:
                decision = "BUY"
                confidence = min(0.9, trend_strength * 10)
            elif current_price < sma_20_current < sma_50_current:
                decision = "SELL"
                confidence = min(0.9, trend_strength * 10)
            else:
                decision = "HOLD"
                confidence = 0.5
            
            return decision, confidence
            
        except Exception as e:
            logging.error(f"TrendAnalysisAgent error: {e}")
            return "HOLD", 0.5

class NewsAnalysisAgent:
    """Specialist agent cho trend analysis"""
    
    def analyze(self, data, symbol):
        """analysis trenda symbol"""
        try:
            if len(data) < 20:
                return "HOLD", 0.5
            
            # Calculate trend indicators
            sma_20 = data['close'].rolling(20).mean()
            sma_50 = data['close'].rolling(50).mean() if len(data) >= 50 else sma_20
            
            current_price = data['close'].iloc[-1]
            sma_20_current = sma_20.iloc[-1]
            sma_50_current = sma_50.iloc[-1]
            
            # Trend strength
            trend_strength = abs(current_price - sma_20_current) / sma_20_current
            
            # Determine trend direction
            if current_price > sma_20_current > sma_50_current:
                decision = "BUY"
                confidence = min(0.9, trend_strength * 10)
            elif current_price < sma_20_current < sma_50_current:
                decision = "SELL"
                confidence = min(0.9, trend_strength * 10)
            else:
                decision = "HOLD"
                confidence = 0.5
            
            return decision, confidence
            
        except Exception as e:
            logging.error(f"TrendAnalysisAgent error: {e}")
            return "HOLD", 0.5

class NewsAnalysisAgent:
    """Specialist agent cho newfixnalysis"""
    
    def analyze(self, data, symbol):
        """analysis news impact"""
        try:
            # Simulate newfixnalysis (in real implementation, this would analyze actual news)
            news_impact = np.random.normal(0, 0.3)  # Random news impact
            
            if news_impact > 0.2:
                decision = "BUY"
                confidence = min(0.8, abs(news_impact))
            elif news_impact < -0.2:
                decision = "SELL"
                confidence = min(0.8, abs(news_impact))
            else:
                decision = "HOLD"
                confidence = 0.5
            
            return decision, confidence
            
        except Exception as e:
            logging.error(f"NewsAnalysisAgent error: {e}")
            return "HOLD", 0.5

class RiskManagementAgent:
    """Specialist agent cho risk management"""
    
    def analyze(self, data, symbol):
        """analysis risk v dua ra khuy n ngh """
        try:
            if len(data) < 10:
                return "HOLD", 0.5
            
            # Calculate volatility
            returns = data['close'].pct_change().dropna()
            volatility = returns.std() * 100
            
            # Risk assessment
            if volatility > 5.0:  # High volatility
                decision = "HOLD"  # Conservative approach
                confidence = 0.7
            elif volatility < 1.0:  # Low volatility
                decision = "BUY"  # Can take more risk
                confidence = 0.6
            else:
                decision = "HOLD"
                confidence = 0.5
            
            return decision, confidence
            
        except Exception as e:
            logging.error(f"RiskManagementAgent error: {e}")
            return "HOLD", 0.5

class SentimentAnalysisAgent:
    """Specialist agent cho sentiment analysis"""
    
    def analyze(self, data, symbol):
        """analysis sentiment"""
        try:
            # Simulate sentiment analysis
            sentiment_score = np.random.uniform(-1, 1)
            
            if sentiment_score > 0.3:
                decision = "BUY"
                confidence = min(0.8, sentiment_score)
            elif sentiment_score < -0.3:
                decision = "SELL"
                confidence = min(0.8, abs(sentiment_score))
            else:
                decision = "HOLD"
                confidence = 0.5
            
            return decision, confidence
            
        except Exception as e:
            logging.error(f"SentimentAnalysisAgent error: {e}")
            return "HOLD", 0.5

class VolatilityPredictionAgent:
    """Specialist agent cho volatility prediction"""
    
    def analyze(self, data, symbol):
        """Equal volatility v dua ra khuy n ngh """
        try:
            if len(data) < 20:
                return "HOLD", 0.5
            
            # Calculate current volatility
            returns = data['close'].pct_change().dropna()
            current_volatility = returns.tail(10).std() * 100
            
            # Predict future volatility trend
            volatility_trend = returns.tail(5).std() - returns.tail(10).std()
            
            if volatility_trend > 0:  # Increasing volatility
                decision = "HOLD"  # Be cautious
                confidence = 0.7
            else:  # Decreasing volatility
                decision = "BUY"  # Can take positions
                confidence = 0.6
            
            return decision, confidence
            
        except Exception as e:
            logging.error(f"VolatilityPredictionAgent error: {e}")
            return "HOLD", 0.5

class PortfolioOptimizationAgent:
    """Specialist agent cho portfolio optimization"""
    
    def analyze(self, data, symbol):
        """T i uu ha portfolio"""
        try:
            # Simulate portfolio optimization
            portfolio_score = np.random.uniform(0, 1)
            
            if portfolio_score > 0.6:
                decision = "BUY"
                confidence = portfolio_score
            elif portfolio_score < 0.4:
                decision = "SELL"
                confidence = 1 - portfolio_score
            else:
                decision = "HOLD"
                confidence = 0.5
            
            return decision, confidence
            
        except Exception as e:
            logging.error(f"PortfolioOptimizationAgent error: {e}")
            return "HOLD", 0.5

# === MASTER AGENT FOR TP/SL DECISIONS ===
class MasterAgent:
    """
    Master Agent for intelligent TP/SL and Trailing Stop decisions
    
    This agent combines multiple analysis methods to determine optimal:
    - Take Profit levels
    - Stop Loss levels  
    - Trailing Stop activation timing
    - Risk-Reward optimization
    """
    
    def __init__(self):
        print("🎯 [Master Agent] Initializing Master Agent for TP/SL decisions...")
        
        # Core decision tracking
        self.tp_sl_history = {}
        self.trailing_stop_decisions = {}
        self.performance_metrics = {}
        
        # Initialize specialist agents for analysis
        self.initialize_specialist_agents()
        
        # Decision parameters
        self.min_risk_reward_ratio = 1.5
        self.max_risk_reward_ratio = 5.0
        self.trailing_activation_profit_threshold = 0.015  # 1.5%
        
        # Learning parameters - weights for different factors
        self.decision_weights = {
            'atr_weight': 0.3,
            'volatility_weight': 0.25,
            'trend_weight': 0.2,
            'support_resistance_weight': 0.15,
            'news_sentiment_weight': 0.1
        }
        
        # Market condition thresholds
        self.volatility_thresholds = {
            'low': 0.3,
            'medium': 0.6,
            'high': 0.8
        }
        
        print("✅ [Master Agent] Master Agent initialized successfully")
    
    def coordinate_decision(self, task_type, market_data, symbol):
        """Decision coordination for all specialist agents"""
        print(f"[Master Agent Coordinator] Starting coordinate_decision for {symbol}")
        logging.info(f"[Master Agent Coordinator] Starting coordinate_decision for {symbol}")
        
        try:
            # Decompose task
            subtasks = self.decompose_task(task_type, market_data)
            print(f" [Master Agent Coordinator] Decomposed tasks: {list(subtasks.keys())}")
            
            # Collect opinions from specialist agents
            agent_opinions = {}
            agent_confidences = {}
            
            for subtask_name, data in subtasks.items():
                # Map subtask names to agent names
                agent_mapping = {
                    'trend_analysis': 'trend_analyzer',
                    'news_analysis': 'news_analyzer', 
                    'risk_assessment': 'risk_manager',
                    'sentiment_analysis': 'sentiment_analyzer',
                    'volatility_prediction': 'volatility_predictor',
                    'portfolio_optimization': 'portfolio_optimizer'
                }
                
                agent_name = agent_mapping.get(subtask_name, subtask_name)
                if agent_name in self.specialist_agents:
                    print(f" [Master Agent Coordinator] Starting analysis with {agent_name}")
                    agent = self.specialist_agents[agent_name]
                    try:
                        opinion, confidence = agent.analyze(data, symbol)
                        agent_opinions[subtask_name] = opinion
                        agent_confidences[subtask_name] = confidence
                        print(f" [Master Agent Coordinator] {subtask_name}: {opinion} ({confidence:.2%})")
                    except Exception as e:
                        print(f"[Master Agent Coordinator] Error in {subtask_name}: {e}")
                        agent_opinions[subtask_name] = "HOLD"
                        agent_confidences[subtask_name] = 0.5
            
            # Apply consensus mechanism
            final_decision, final_confidence = self._apply_consensus_mechanism(
                agent_opinions, agent_confidences, symbol
            )
            
            print(f" [Master Agent Coordinator] Final decision for {symbol}: {final_decision} ({final_confidence:.2%})")
            
            # Update agent performance
            self._update_agent_performance(agent_opinions, agent_confidences, symbol)
            
            return final_decision, final_confidence
            
        except Exception as e:
            print(f"[Master Agent Coordinator] Error in coordinate_decision: {e}")
            logging.error(f"Error in Master Agent coordination: {e}")
            return "HOLD", 0.5
    
    def decompose_task(self, task_type, market_data):
        """Decompose task into subtasks for specialist agents"""
        subtasks = {}
        
        if task_type == 'trading_decision':
            subtasks['trend_analysis'] = market_data
            subtasks['news_analysis'] = market_data
            subtasks['risk_assessment'] = market_data
            subtasks['sentiment_analysis'] = market_data
            subtasks['volatility_prediction'] = market_data
            subtasks['portfolio_optimization'] = market_data
            
        return subtasks
    
    def _apply_consensus_mechanism(self, opinions, confidences, symbol):
        """Apply consensus mechanism to agent opinions"""
        try:
            # Weight votes by confidence
            weighted_votes = {}
            total_weight = 0.0
            
            for agent, opinion in opinions.items():
                confidence = confidences.get(agent, 0.5)
                if opinion not in weighted_votes:
                    weighted_votes[opinion] = 0.0
                weighted_votes[opinion] += confidence
                total_weight += confidence
            
            # Find decision with highest weighted score
            if weighted_votes:
                best_decision = max(weighted_votes, key=weighted_votes.get)
                final_confidence = weighted_votes[best_decision] / total_weight if total_weight > 0 else 0.5
                return best_decision, min(final_confidence, 0.95)
            else:
                return "HOLD", 0.5
                
        except Exception as e:
            print(f"[Master Agent Coordinator] Error in consensus mechanism: {e}")
            return "HOLD", 0.5
    
    def _update_agent_performance(self, opinions, confidences, symbol):
        """Update performance of agents"""
        try:
            if not hasattr(self, 'agent_performance'):
                self.agent_performance = {}
                
            for agent_name in opinions.keys():
                if agent_name not in self.agent_performance:
                    self.agent_performance[agent_name] = {}
                if symbol not in self.agent_performance[agent_name]:
                    self.agent_performance[agent_name][symbol] = 0.5
                    
        except Exception as e:
            print(f"[Master Agent Coordinator] Error updating agent performance: {e}")
        
    def _setup_communication_matrix(self):
        """Setup communication matrix between agents"""
        agents = list(self.specialist_agents.keys())
        self.communication_matrix = {}
        
        for agent1 in agents:
            self.communication_matrix[agent1] = {}
            for agent2 in agents:
                if agent1 != agent2:
                    # Define communication strength based on agent types
                    comm_strength = self._calculate_communication_strength(agent1, agent2)
                    self.communication_matrix[agent1][agent2] = comm_strength
                else:
                    self.communication_matrix[agent1][agent2] = 1.0
    
    def _calculate_communication_strength(self, agent1, agent2):
        """Calculate communication strength between two agents"""
        # Define which agents should communicate more strongly
        communication_pairs = [
            ('trend_analyzer', 'risk_manager'),
            ('news_analyzer', 'trend_analyzer'),
            ('sentiment_analyzer', 'portfolio_optimizer')
        ]
        
        return (agent1, agent2) in communication_pairs or (agent2, agent1) in communication_pairs
    
    def initialize_specialist_agents(self):
        """Initialize all specialist agents"""
        print("🎯 [Master Agent] Starting specialist agents initialization...")
        
        try:
            self.specialist_agents = {
                'trend_analyzer': TrendAnalysisAgent(),
                'news_analyzer': NewsAnalysisAgent(), 
                'risk_manager': RiskManagementAgent(),
                'sentiment_analyzer': SentimentAnalysisAgent(),
                'volatility_predictor': VolatilityPredictionAgent(),
                'portfolio_optimizer': PortfolioOptimizationAgent()
            }
            print(f"✅ [Master Agent] Initialized {len(self.specialist_agents)} specialist agents")
            
            # setup communication matrix
            self._setup_communication_matrix()
            print("✅ [Master Agent] Communication matrix setup completed")
            
        except Exception as e:
            print(f"❌ [Master Agent] Error initializing specialist agents: {e}")
            import traceback
            traceback.print_exc()
            # Fallback: to empty specialist agents
            self.specialist_agents = {}
    
    def _setup_communication_matrix(self):
        """Setup communication matrix between agents"""
        agents = list(self.specialist_agents.keys())
        self.communication_matrix = {}
        
        for agent1 in agents:
            self.communication_matrix[agent1] = {}
            for agent2 in agents:
                if agent1 != agent2:
                    # Check if agents should communicate
                    if self._should_communicate(agent1, agent2):
                        self.communication_matrix[agent1][agent2] = True
                    else:
                        self.communication_matrix[agent1][agent2] = False
                else:
                    self.communication_matrix[agent1][agent2] = False
    
    def _should_communicate(self, agent1, agent2):
        """Check if two agents should communicate"""
        communication_pairs = [
            ('trend_analyzer', 'volatility_predictor'),
            ('news_analyzer', 'sentiment_analyzer'),
            ('risk_manager', 'portfolio_optimizer'),
            ('trend_analyzer', 'risk_manager'),
            ('news_analyzer', 'trend_analyzer'),
            ('sentiment_analyzer', 'portfolio_optimizer')
        ]
        
        return (agent1, agent2) in communication_pairs or (agent2, agent1) in communication_pairs

    def decide_tp_sl_levels(self, symbol, entry_price, direction, market_data):
        """
        Master decision function for TP/SL levels
        Combines multiple analysis methods for optimal risk management
        """
        try:
            print(f"🎯 [Master Agent] Deciding TP/SL levels for {symbol} - {direction} at {entry_price}")
            
            # Step 1: Gather market intelligence
            market_intelligence = self._gather_market_intelligence(symbol, market_data)
            
            # Step 2: Calculate base levels using multiple methods
            atr_levels = self._calculate_atr_based_levels(symbol, entry_price, market_data)
            volatility_levels = self._calculate_volatility_adjusted_levels(symbol, entry_price, direction, market_intelligence)
            trend_levels = self._calculate_trend_based_levels(symbol, entry_price, direction, market_intelligence)
            sr_levels = self._calculate_support_resistance_levels(symbol, market_data)
            
            # Step 3: Apply intelligent fusion algorithm
            final_levels = self._fuse_tp_sl_decisions(
                symbol, entry_price, direction,
                atr_levels, volatility_levels, trend_levels, sr_levels,
                market_intelligence
            )
            
            # Step 4: Validate and optimize risk-reward ratio
            optimized_levels = self._optimize_risk_reward_ratio(final_levels, entry_price, direction)
            
            # Step 5: Store decision for learning
            self._store_tp_sl_decision(symbol, entry_price, direction, optimized_levels, market_intelligence)
            
            print(f"✅ [Master Agent] Final TP/SL Decision for {symbol}:")
            print(f"   📈 Take Profit: {optimized_levels['take_profit']:.6f}")
            print(f"   📉 Stop Loss: {optimized_levels['stop_loss']:.6f}")
            print(f"   ⚖️ Risk/Reward Ratio: {optimized_levels['risk_reward_ratio']:.2f}")
            
            return optimized_levels
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in TP/SL decision for {symbol}: {e}")
            return self._get_fallback_tp_sl_levels(entry_price, direction)
    
    def decide_trade_entry(self, symbol, market_data, base_signal, base_confidence):
        """
        The final entry decision method, synthesizing multiple sources.
        Returns a dictionary with the decision and justification.
        """
        print(f"🎯 [Master Agent] Analyzing ENTRY for {base_signal} {symbol} (Confidence: {base_confidence:.2%})")
        
        agent_opinions = {}
        agent_confidences = {}
        
        # Run specialist agents
        for agent_name, agent in self.specialist_agents.items():
            try:
                opinion, confidence = agent.analyze(market_data['price_data'], symbol)
                agent_opinions[agent_name] = opinion
                agent_confidences[agent_name] = confidence
            except Exception as e:
                print(f"❌ Error running {agent_name}: {e}")
                agent_opinions[agent_name] = "HOLD"
                agent_confidences[agent_name] = 0.5
                
        # Synthesize opinions
        buy_votes = sum(conf for agent, op in agent_opinions.items() if op == "BUY" for conf in [agent_confidences[agent]])
        sell_votes = sum(conf for agent, op in agent_opinions.items() if op == "SELL" for conf in [agent_confidences[agent]])

        # Analyze market structure - need to access bot instance for this
        # We'll pass this as part of market_data or calculate it here
        pa_score = 0.0
        if 'pa_score' in market_data:
            pa_score = market_data['pa_score']
        
        # Final decision logic
        final_decision = "HOLD"
        justification = []

        if base_signal == "BUY":
            if buy_votes > sell_votes * 1.5 and pa_score > 0.1:
                final_decision = "APPROVE"
                justification.append(f"Strong consensus from specialist agents (Buy Score: {buy_votes:.2f}).")
                justification.append(f"Favorable Price Action (Score: {pa_score:.2f}).")
            else:
                final_decision = "REJECT"
                justification.append(f"Weak consensus (Buy Score: {buy_votes:.2f} vs Sell Score: {sell_votes:.2f}).")
                if pa_score <= 0.1:
                    justification.append(f"Unfavorable Price Action (Score: {pa_score:.2f}).")

        elif base_signal == "SELL":
            if sell_votes > buy_votes * 1.5 and pa_score < -0.1:
                final_decision = "APPROVE"
                justification.append(f"Strong consensus from specialist agents (Sell Score: {sell_votes:.2f}).")
                justification.append(f"Favorable Price Action (Score: {pa_score:.2f}).")
            else:
                final_decision = "REJECT"
                justification.append(f"Weak consensus (Sell Score: {sell_votes:.2f} vs Buy Score: {buy_votes:.2f}).")
                if pa_score >= -0.1:
                    justification.append(f"Unfavorable Price Action (Score: {pa_score:.2f}).")

        print(f"✅ [Master Agent] Decision: {final_decision}")
        print(f"📝 [Master Agent] Justification: {' '.join(justification)}")
        
        return {
            "decision": final_decision,
            "justification": " ".join(justification),
            "confidence": base_confidence
        }

    def decide_trailing_stop_activation(self, symbol, current_price, entry_price, direction, market_data):
        """
        Intelligent decision on when to activate trailing stops
        """
        try:
            print(f"🎯 [Master Agent] Analyzing trailing stop activation for {symbol}")
            
            # Calculate current profit
            if direction.upper() == 'BUY':
                profit_pct = (current_price - entry_price) / entry_price
                profit_pips = (current_price - entry_price) * 10000  # For forex
            else:
                profit_pct = (entry_price - current_price) / entry_price
                profit_pips = (entry_price - current_price) * 10000
            
            # Gather market intelligence
            market_intelligence = self._gather_market_intelligence(symbol, market_data)
            
            # Decision factors
            decision_factors = {
                'profit_threshold': profit_pct > self.trailing_activation_profit_threshold,
                'trend_strength': self._assess_trend_strength(market_intelligence),
                'volatility_level': self._assess_volatility_level(market_intelligence),
                'momentum_confirmation': self._check_momentum_confirmation(market_data, direction),
                'support_resistance': self._check_sr_proximity(current_price, market_data)
            }
            
            # Apply decision logic
            activation_decision = self._apply_trailing_stop_logic(decision_factors, profit_pct)
            
            # Calculate optimal trailing distance
            trailing_distance = self._calculate_optimal_trailing_distance(
                symbol, current_price, market_intelligence, profit_pct
            )
            
            # Store decision
            self.trailing_stop_decisions[symbol] = {
                'timestamp': datetime.now(),
                'should_activate': activation_decision['should_activate'],
                'current_profit_pct': profit_pct,
                'current_profit_pips': profit_pips,
                'reasons': activation_decision['reasons'],
                'trailing_distance': trailing_distance,
                'decision_factors': decision_factors
            }
            
            result = {
                'should_activate': activation_decision['should_activate'],
                'reasons': activation_decision['reasons'],
                'current_profit_pct': profit_pct,
                'current_profit_pips': profit_pips,
                'recommended_distance': trailing_distance,
                'confidence': activation_decision['confidence']
            }
            
            print(f"✅ [Master Agent] Trailing Stop Decision for {symbol}:")
            print(f"   🎯 Activate: {'YES' if result['should_activate'] else 'NO'}")
            print(f"   💰 Current Profit: {profit_pct:.2%} ({profit_pips:.1f} pips)")
            print(f"   📏 Recommended Distance: {trailing_distance:.5f}")
            print(f"   🔍 Reasons: {', '.join(result['reasons'])}")
            
            return result
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in trailing stop decision for {symbol}: {e}")
            return {
                'should_activate': False,
                'reasons': ['Error in analysis'],
                'current_profit_pct': 0,
                'recommended_distance': current_price * 0.01
            }
    
    def _gather_market_intelligence(self, symbol, market_data):
        """Gather comprehensive market intelligence from specialist agents"""
        intelligence = {}
        
        try:
            df = market_data.get('price_data')
            if df is None or len(df) < 20:
                return {'error': 'Insufficient data'}
            
            # Get analysis from specialist agents
            for agent_name, agent in self.specialist_agents.items():
                try:
                    analysis_result = agent.analyze(df, symbol)
                    intelligence[agent_name] = analysis_result
                except Exception as e:
                    print(f"⚠️ [Master Agent] Error in {agent_name}: {e}")
                    intelligence[agent_name] = ("HOLD", 0.5)
            
            # Calculate technical indicators
            intelligence['technical'] = self._calculate_technical_indicators(df)
            
            return intelligence
            
        except Exception as e:
            print(f"❌ [Master Agent] Error gathering market intelligence: {e}")
            return {'error': str(e)}
    
    def _calculate_technical_indicators(self, df):
        """Calculate key technical indicators for decision making"""
        try:
            current_price = df["close"].iloc[-1]
            
            # ATR for volatility
            from ta.volatility import AverageTrueRange
            atr = AverageTrueRange(df["high"], df["low"], df["close"], window=14)
            current_atr = atr.average_true_range().iloc[-1]
            
            # RSI for momentum
            from ta.momentum import RSIIndicator
            rsi = RSIIndicator(df["close"], window=14)
            current_rsi = rsi.rsi().iloc[-1]
            
            # Moving averages for trend
            sma_20 = df["close"].rolling(20).mean().iloc[-1]
            sma_50 = df["close"].rolling(50).mean().iloc[-1] if len(df) >= 50 else sma_20
            
            # Bollinger Bands for volatility context
            from ta.volatility import BollingerBands
            bb = BollingerBands(df["close"], window=20, window_dev=2)
            bb_upper = bb.bollinger_hband().iloc[-1]
            bb_lower = bb.bollinger_lband().iloc[-1]
            bb_position = (current_price - bb_lower) / (bb_upper - bb_lower)
            
            return {
                'atr': current_atr,
                'atr_pct': current_atr / current_price,
                'rsi': current_rsi,
                'bb_position': bb_position,
                'price_vs_sma20': current_price / sma_20,
                'price_vs_sma50': current_price / sma_50,
                'sma_trend': 1 if sma_20 > sma_50 else -1,
                'current_price': current_price
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating technical indicators: {e}")
            return {}
    
    def _calculate_atr_based_levels(self, symbol, entry_price, market_data):
        """Calculate TP/SL levels based on ATR"""
        try:
            df = market_data.get('price_data')
            if df is None or len(df) < 20:
                return None
            
            # Calculate ATR
            from ta.volatility import AverageTrueRange
            atr_indicator = AverageTrueRange(df["high"], df["low"], df["close"], window=14)
            atr_value = atr_indicator.average_true_range().iloc[-1]
            
            # Get symbol-specific multipliers from config
            symbol_config = ENTRY_TP_SL_CONFIG.get(symbol, {})
            
            tp_multiplier = symbol_config.get('atr_multiplier_tp', 3.0)
            sl_multiplier = symbol_config.get('atr_multiplier_sl', 2.0)
            
            return {
                'atr_value': atr_value,
                'tp_multiplier': tp_multiplier,
                'sl_multiplier': sl_multiplier,
                'tp_distance': atr_value * tp_multiplier,
                'sl_distance': atr_value * sl_multiplier
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating ATR levels: {e}")
            return None
    
    def _calculate_volatility_adjusted_levels(self, symbol, entry_price, direction, market_intelligence):
        """Calculate TP/SL levels adjusted for current volatility"""
        try:
            if 'technical' not in market_intelligence:
                return None
                
            technical = market_intelligence['technical']
            volatility_pct = technical.get('atr_pct', 0.01)
            bb_position = technical.get('bb_position', 0.5)
            
            # Adjust multipliers based on volatility
            if volatility_pct > 0.03:  # High volatility
                tp_mult = 4.0
                sl_mult = 1.5
            elif volatility_pct > 0.015:  # Medium volatility
                tp_mult = 3.0
                sl_mult = 2.0
            else:  # Low volatility
                tp_mult = 2.5
                sl_mult = 2.5
            
            # Adjust based on Bollinger Band position
            if bb_position > 0.8:  # Near upper band
                tp_mult *= 0.8  # Reduce TP
                sl_mult *= 1.2  # Increase SL
            elif bb_position < 0.2:  # Near lower band
                tp_mult *= 0.8
                sl_mult *= 1.2
            
            base_distance = entry_price * volatility_pct
            
            return {
                'tp_distance': base_distance * tp_mult,
                'sl_distance': base_distance * sl_mult,
                'volatility_score': volatility_pct,
                'bb_position': bb_position
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating volatility levels: {e}")
            return None
    
    def _calculate_trend_based_levels(self, symbol, entry_price, direction, market_intelligence):
        """Calculate TP/SL levels based on trend strength"""
        try:
            if 'technical' not in market_intelligence:
                return None
                
            technical = market_intelligence['technical']
            trend_strength = technical.get('sma_trend', 0)
            price_vs_sma20 = technical.get('price_vs_sma20', 1.0)
            
            # Get trend analysis from specialist agent
            trend_signal, trend_confidence = market_intelligence.get('trend_analyzer', ('HOLD', 0.5))
            
            # Adjust levels based on trend alignment
            if direction.upper() == 'BUY':
                if trend_strength > 0 and price_vs_sma20 > 1.01:  # Strong uptrend
                    tp_mult = 3.5 * trend_confidence
                    sl_mult = 1.5
                else:  # Weak or counter-trend
                    tp_mult = 2.0
                    sl_mult = 2.5
            else:  # SELL
                if trend_strength < 0 and price_vs_sma20 < 0.99:  # Strong downtrend
                    tp_mult = 3.5 * trend_confidence
                    sl_mult = 1.5
                else:  # Weak or counter-trend
                    tp_mult = 2.0
                    sl_mult = 2.5
            
            base_distance = entry_price * 0.01  # 1% base
            
            return {
                'tp_distance': base_distance * tp_mult,
                'sl_distance': base_distance * sl_mult,
                'trend_strength': trend_strength,
                'trend_confidence': trend_confidence
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating trend levels: {e}")
            return None
    
    def _calculate_support_resistance_levels(self, symbol, market_data):
        """Calculate support/resistance levels for TP/SL placement"""
        try:
            df = market_data.get('price_data')
            if df is None or len(df) < 50:
                return None
            
            current_price = df["close"].iloc[-1]
            
            # Calculate pivot points
            high = df["high"].iloc[-1]
            low = df["low"].iloc[-1]
            close = df["close"].iloc[-1]
            
            pivot = (high + low + close) / 3
            r1 = 2 * pivot - low
            s1 = 2 * pivot - high
            r2 = pivot + (high - low)
            s2 = pivot - (high - low)
            
            # Find recent highs and lows
            recent_highs = df["high"].rolling(20).max().dropna()
            recent_lows = df["low"].rolling(20).min().dropna()
            
            resistance_levels = [r1, r2] + recent_highs.tail(5).tolist()
            support_levels = [s1, s2] + recent_lows.tail(5).tolist()
            
            # Find closest levels
            resistance_above = [r for r in resistance_levels if r > current_price]
            support_below = [s for s in support_levels if s < current_price]
            
            nearest_resistance = min(resistance_above) if resistance_above else current_price * 1.02
            nearest_support = max(support_below) if support_below else current_price * 0.98
            
            return {
                'nearest_resistance': nearest_resistance,
                'nearest_support': nearest_support,
                'pivot': pivot,
                'resistance_distance': nearest_resistance - current_price,
                'support_distance': current_price - nearest_support
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating S/R levels: {e}")
            return None
    
    def _fuse_tp_sl_decisions(self, symbol, entry_price, direction, atr_levels, volatility_levels, trend_levels, sr_levels, market_intelligence):
        """Intelligent fusion of multiple TP/SL calculation methods"""
        try:
            tp_distances = []
            sl_distances = []
            weights = []
            
            # Collect TP/SL distances from different methods
            if atr_levels:
                tp_distances.append(atr_levels['tp_distance'])
                sl_distances.append(atr_levels['sl_distance'])
                weights.append(self.decision_weights['atr_weight'])
            
            if volatility_levels:
                tp_distances.append(volatility_levels['tp_distance'])
                sl_distances.append(volatility_levels['sl_distance'])
                weights.append(self.decision_weights['volatility_weight'])
            
            if trend_levels:
                tp_distances.append(trend_levels['tp_distance'])
                sl_distances.append(trend_levels['sl_distance'])
                weights.append(self.decision_weights['trend_weight'])
            
            if sr_levels:
                # Use S/R levels to cap TP/SL distances
                if direction.upper() == 'BUY':
                    max_tp_distance = sr_levels['resistance_distance'] * 0.8  # 80% to resistance
                    max_sl_distance = sr_levels['support_distance'] * 0.8   # 80% to support
                else:
                    max_tp_distance = sr_levels['support_distance'] * 0.8
                    max_sl_distance = sr_levels['resistance_distance'] * 0.8
                
                tp_distances.append(max_tp_distance)
                sl_distances.append(max_sl_distance)
                weights.append(self.decision_weights['support_resistance_weight'])
            
            # Calculate weighted averages
            if tp_distances and sl_distances:
                total_weight = sum(weights)
                if total_weight > 0:
                    weighted_tp_distance = sum(tp * w for tp, w in zip(tp_distances, weights)) / total_weight
                    weighted_sl_distance = sum(sl * w for sl, w in zip(sl_distances, weights)) / total_weight
                else:
                    weighted_tp_distance = sum(tp_distances) / len(tp_distances)
                    weighted_sl_distance = sum(sl_distances) / len(sl_distances)
            else:
                # Fallback to percentage-based
                weighted_tp_distance = entry_price * 0.02  # 2%
                weighted_sl_distance = entry_price * 0.01  # 1%
            
            # Calculate final TP/SL levels
            if direction.upper() == 'BUY':
                take_profit = entry_price + weighted_tp_distance
                stop_loss = entry_price - weighted_sl_distance
            else:
                take_profit = entry_price - weighted_tp_distance
                stop_loss = entry_price + weighted_sl_distance
            
            # Calculate risk-reward ratio
            risk_distance = abs(stop_loss - entry_price)
            reward_distance = abs(take_profit - entry_price)
            risk_reward_ratio = reward_distance / risk_distance if risk_distance > 0 else 2.0
            
            return {
                'take_profit': take_profit,
                'stop_loss': stop_loss,
                'tp_distance': weighted_tp_distance,
                'sl_distance': weighted_sl_distance,
                'risk_reward_ratio': risk_reward_ratio,
                'fusion_weights': dict(zip(['atr', 'volatility', 'trend', 'sr'], weights)),
                'reasoning': {
                    'method': 'weighted_fusion',
                    'inputs_used': len(tp_distances),
                    'total_weight': sum(weights)
                }
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in fusion algorithm: {e}")
            return self._get_fallback_tp_sl_levels(entry_price, direction)
    
    def _optimize_risk_reward_ratio(self, levels, entry_price, direction):
        """Optimize the risk-reward ratio within acceptable bounds"""
        try:
            current_rr = levels['risk_reward_ratio']
            
            # Check if adjustment is needed
            if self.min_risk_reward_ratio <= current_rr <= self.max_risk_reward_ratio:
                return levels  # No adjustment needed
            
            # Calculate base distances
            risk_distance = abs(levels['stop_loss'] - entry_price)
            
            # Adjust to meet minimum RR ratio
            if current_rr < self.min_risk_reward_ratio:
                target_reward_distance = risk_distance * self.min_risk_reward_ratio
                
                if direction.upper() == 'BUY':
                    optimized_tp = entry_price + target_reward_distance
                else:
                    optimized_tp = entry_price - target_reward_distance
                
                levels['take_profit'] = optimized_tp
                levels['tp_distance'] = target_reward_distance
                levels['risk_reward_ratio'] = self.min_risk_reward_ratio
                levels['reasoning']['optimization'] = f'Adjusted TP to meet min RR {self.min_risk_reward_ratio}'
            
            # Adjust to meet maximum RR ratio
            elif current_rr > self.max_risk_reward_ratio:
                target_reward_distance = risk_distance * self.max_risk_reward_ratio
                
                if direction.upper() == 'BUY':
                    optimized_tp = entry_price + target_reward_distance
                else:
                    optimized_tp = entry_price - target_reward_distance
                
                levels['take_profit'] = optimized_tp
                levels['tp_distance'] = target_reward_distance
                levels['risk_reward_ratio'] = self.max_risk_reward_ratio
                levels['reasoning']['optimization'] = f'Adjusted TP to meet max RR {self.max_risk_reward_ratio}'
            
            return levels
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in RR optimization: {e}")
            return levels
    
    def _get_fallback_tp_sl_levels(self, entry_price, direction):
        """Fallback TP/SL levels when calculations fail"""
        if direction.upper() == 'BUY':
            return {
                'take_profit': entry_price * 1.02,  # 2% profit
                'stop_loss': entry_price * 0.99,    # 1% loss
                'tp_distance': entry_price * 0.02,
                'sl_distance': entry_price * 0.01,
                'risk_reward_ratio': 2.0,
                'reasoning': {'fallback': True, 'method': 'percentage_based'}
            }
        else:
            return {
                'take_profit': entry_price * 0.98,  # 2% profit
                'stop_loss': entry_price * 1.01,    # 1% loss
                'tp_distance': entry_price * 0.02,
                'sl_distance': entry_price * 0.01,
                'risk_reward_ratio': 2.0,
                'reasoning': {'fallback': True, 'method': 'percentage_based'}
            }
    
    def _store_tp_sl_decision(self, symbol, entry_price, direction, levels, market_intelligence):
        """Store TP/SL decision for performance tracking and learning"""
        try:
            self.tp_sl_history[symbol] = {
                'timestamp': datetime.now(),
                'entry_price': entry_price,
                'direction': direction,
                'take_profit': levels['take_profit'],
                'stop_loss': levels['stop_loss'],
                'risk_reward_ratio': levels['risk_reward_ratio'],
                'market_intelligence': market_intelligence,
                'reasoning': levels.get('reasoning', {}),
                'status': 'active'
            }
        except Exception as e:
            print(f"❌ [Master Agent] Error storing decision: {e}")
    
    # === TRAILING STOP METHODS ===
    
    def _assess_trend_strength(self, market_intelligence):
        """Assess the strength of the current trend"""
        try:
            trend_signal, trend_confidence = market_intelligence.get('trend_analyzer', ('HOLD', 0.5))
            technical = market_intelligence.get('technical', {})
            
            # Combine trend signal confidence with technical indicators
            sma_trend = technical.get('sma_trend', 0)
            price_vs_sma20 = technical.get('price_vs_sma20', 1.0)
            
            # Calculate composite trend strength
            if trend_signal == 'BUY' and sma_trend > 0:
                strength = trend_confidence * min(price_vs_sma20, 1.1)  # Cap at 10% above SMA
            elif trend_signal == 'SELL' and sma_trend < 0:
                strength = trend_confidence * min(2 - price_vs_sma20, 1.1)  # Inverse for downtrend
            else:
                strength = trend_confidence * 0.5  # Reduce strength for conflicting signals
            
            return min(strength, 1.0)  # Cap at 1.0
            
        except Exception as e:
            print(f"❌ [Master Agent] Error assessing trend strength: {e}")
            return 0.5
    
    def _assess_volatility_level(self, market_intelligence):
        """Assess the current volatility level"""
        try:
            technical = market_intelligence.get('technical', {})
            atr_pct = technical.get('atr_pct', 0.01)
            bb_position = technical.get('bb_position', 0.5)
            
            # Normalize volatility score
            if atr_pct > 0.03:
                volatility_score = 1.0  # High volatility
            elif atr_pct > 0.015:
                volatility_score = 0.7  # Medium volatility
            else:
                volatility_score = 0.3  # Low volatility
            
            # Adjust based on Bollinger Band position (extreme positions indicate volatility)
            if bb_position > 0.8 or bb_position < 0.2:
                volatility_score = min(volatility_score + 0.2, 1.0)
            
            return volatility_score
            
        except Exception as e:
            print(f"❌ [Master Agent] Error assessing volatility: {e}")
            return 0.5
    
    def _check_momentum_confirmation(self, market_data, direction):
        """Check if momentum confirms the trade direction"""
        try:
            df = market_data.get('price_data')
            if df is None or len(df) < 14:
                return False
            
            # Calculate RSI for momentum
            from ta.momentum import RSIIndicator
            rsi = RSIIndicator(df["close"], window=14)
            current_rsi = rsi.rsi().iloc[-1]
            
            if direction.upper() == 'BUY':
                # For BUY positions, RSI should be above 50 but not overbought
                return 50 < current_rsi < 80
            else:
                # For SELL positions, RSI should be below 50 but not oversold
                return 20 < current_rsi < 50
            
        except Exception as e:
            print(f"❌ [Master Agent] Error checking momentum: {e}")
            return False
    
    def _check_sr_proximity(self, current_price, market_data):
        """Check proximity to support/resistance levels"""
        try:
            sr_levels = self._calculate_support_resistance_levels('', market_data)
            if not sr_levels:
                return True  # No S/R constraints
            
            nearest_resistance = sr_levels['nearest_resistance']
            nearest_support = sr_levels['nearest_support']
            
            # Check if price is too close to S/R levels (within 1%)
            resistance_distance_pct = abs(nearest_resistance - current_price) / current_price
            support_distance_pct = abs(nearest_support - current_price) / current_price
            
            # Return True if not too close to major S/R levels
            return resistance_distance_pct > 0.01 and support_distance_pct > 0.01
            
        except Exception as e:
            print(f"❌ [Master Agent] Error checking S/R proximity: {e}")
            return True
    
    def _apply_trailing_stop_logic(self, decision_factors, profit_pct):
        """Apply intelligent logic to decide on trailing stop activation"""
        try:
            reasons = []
            score = 0
            max_score = 5
            
            # Factor 1: Profit threshold (mandatory)
            if decision_factors['profit_threshold']:
                score += 2
                reasons.append(f"Profit threshold met ({profit_pct:.2%})")
            else:
                return {
                    'should_activate': False,
                    'reasons': [f"Insufficient profit ({profit_pct:.2%} < 1.5%)"],
                    'confidence': 0.1
                }
            
            # Factor 2: Trend strength
            trend_strength = decision_factors['trend_strength']
            if trend_strength > 0.7:
                score += 1
                reasons.append(f"Strong trend continuation ({trend_strength:.2f})")
            elif trend_strength > 0.5:
                score += 0.5
                reasons.append(f"Moderate trend strength ({trend_strength:.2f})")
            
            # Factor 3: Volatility level
            volatility_level = decision_factors['volatility_level']
            if volatility_level > 0.7:
                score += 0.5
                reasons.append(f"High volatility - protective trailing needed")
            elif volatility_level < 0.4:
                score += 0.5
                reasons.append(f"Low volatility - stable for trailing")
            
            # Factor 4: Momentum confirmation
            if decision_factors['momentum_confirmation']:
                score += 1
                reasons.append("Momentum confirms direction")
            
            # Factor 5: S/R proximity
            if decision_factors['support_resistance']:
                score += 0.5
                reasons.append("Clear of major S/R levels")
            
            # Decision logic
            confidence = score / max_score
            should_activate = score >= 2.5  # Need at least 50% score
            
            if not should_activate:
                reasons = ["Insufficient conditions for trailing stop activation"]
            
            return {
                'should_activate': should_activate,
                'reasons': reasons,
                'confidence': confidence,
                'score': score,
                'max_score': max_score
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in trailing stop logic: {e}")
            return {
                'should_activate': False,
                'reasons': ['Error in analysis'],
                'confidence': 0.0
            }
    
    def _calculate_optimal_trailing_distance(self, symbol, current_price, market_intelligence, profit_pct):
        """Enhanced trailing stop distance calculation using symbol-specific configuration and market intelligence"""
        try:
            # Get symbol-specific configuration
            symbol_config = ENTRY_TP_SL_CONFIG.get(symbol, {})
            
            # Get technical analysis data
            technical = market_intelligence.get('technical', {})
            atr_pct = technical.get('atr_pct', 0.01)
            volatility_level = self._assess_volatility_level(market_intelligence)
            
            # Use symbol-specific ATR multiplier for trailing
            trailing_atr_multiplier = symbol_config.get('trailing_atr_multiplier', 2.0)
            base_distance_pct = atr_pct * trailing_atr_multiplier
            
            print(f"🎯 [Master Agent] Enhanced trailing distance calculation for {symbol}:")
            print(f"   Symbol ATR multiplier: {trailing_atr_multiplier}")
            print(f"   Base ATR: {atr_pct:.4%}")
            print(f"   Base distance: {base_distance_pct:.4%}")
            
            # Enhanced volatility adjustment based on symbol configuration
            volatility_threshold = symbol_config.get('trailing_volatility_threshold', 0.6)
            if volatility_level > volatility_threshold:
                if symbol.startswith(('BTC', 'ETH')):  # Crypto - more aggressive
                    volatility_multiplier = 1.8
                elif 'USD' in symbol and len(symbol) == 6:  # Forex
                    volatility_multiplier = 1.3
                else:  # Commodities/Indices
                    volatility_multiplier = 1.5
            elif volatility_level > (volatility_threshold * 0.7):
                volatility_multiplier = 1.2
            else:
                volatility_multiplier = 1.0
            
            print(f"   Volatility level: {volatility_level:.2f} (threshold: {volatility_threshold:.2f})")
            print(f"   Volatility multiplier: {volatility_multiplier:.2f}")
            
            # Dynamic profit-based adjustment
            if profit_pct > 0.08:  # >8% profit - very tight trailing
                profit_multiplier = 0.7
            elif profit_pct > 0.05:  # >5% profit - tight trailing
                profit_multiplier = 0.8
            elif profit_pct > 0.03:  # >3% profit - moderate trailing
                profit_multiplier = 0.9
            elif profit_pct > 0.02:  # >2% profit - standard trailing
                profit_multiplier = 1.0
            else:  # <2% profit - wider trailing for safety
                profit_multiplier = 1.1
            
            print(f"   Current profit: {profit_pct:.2%}")
            print(f"   Profit multiplier: {profit_multiplier:.2f}")
            
            # Market condition adjustment
            trend_strength = self._assess_trend_strength(market_intelligence)
            if trend_strength > 0.8:  # Very strong trend - can use tighter trailing
                trend_multiplier = 0.9
            elif trend_strength > 0.6:  # Strong trend
                trend_multiplier = 1.0
            else:  # Weak trend - wider trailing for safety
                trend_multiplier = 1.2
            
            print(f"   Trend strength: {trend_strength:.2f}")
            print(f"   Trend multiplier: {trend_multiplier:.2f}")
            
            # Calculate final distance with all multipliers
            final_distance_pct = base_distance_pct * volatility_multiplier * profit_multiplier * trend_multiplier
            
            # Symbol-specific bounds
            if symbol.startswith(('BTC', 'ETH')):  # Crypto - wider bounds
                min_distance_pct = 0.008  # 0.8% minimum
                max_distance_pct = 0.05   # 5% maximum
            elif 'USD' in symbol and len(symbol) == 6:  # Forex - tighter bounds
                min_distance_pct = 0.003  # 0.3% minimum
                max_distance_pct = 0.02   # 2% maximum
            else:  # Commodities/Indices - standard bounds
                min_distance_pct = 0.005  # 0.5% minimum
                max_distance_pct = 0.03   # 3% maximum
            
            # Apply bounds
            final_distance_pct = max(min_distance_pct, min(final_distance_pct, max_distance_pct))
            
            # Convert to price distance
            trailing_distance = current_price * final_distance_pct
            
            print(f"   Final calculation:")
            print(f"   - Combined multipliers: {volatility_multiplier * profit_multiplier * trend_multiplier:.2f}")
            print(f"   - Final distance %: {final_distance_pct:.4%}")
            print(f"   - Trailing distance: {trailing_distance:.5f}")
            print(f"   - Bounds: {min_distance_pct:.3%} - {max_distance_pct:.3%}")
            
            return trailing_distance
            
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating enhanced trailing distance for {symbol}: {e}")
            # Fallback with symbol-specific default
            symbol_config = ENTRY_TP_SL_CONFIG.get(symbol, {})
            fallback_multiplier = symbol_config.get('trailing_atr_multiplier', 2.0)
            return current_price * (fallback_multiplier * 0.005)  # Conservative fallback
    
    # === PERFORMANCE TRACKING AND LEARNING ===
    
    def update_tp_sl_performance(self, symbol, outcome, actual_exit_price=None):
        """Update performance metrics for TP/SL decisions"""
        try:
            if symbol not in self.tp_sl_history:
                return
            
            decision_data = self.tp_sl_history[symbol]
            entry_price = decision_data['entry_price']
            direction = decision_data['direction']
            tp_level = decision_data['take_profit']
            sl_level = decision_data['stop_loss']
            
            # Calculate actual performance
            if actual_exit_price:
                if direction.upper() == 'BUY':
                    actual_return = (actual_exit_price - entry_price) / entry_price
                else:
                    actual_return = (entry_price - actual_exit_price) / entry_price
                
                # Determine if TP or SL was hit
                if direction.upper() == 'BUY':
                    hit_tp = actual_exit_price >= tp_level
                    hit_sl = actual_exit_price <= sl_level
                else:
                    hit_tp = actual_exit_price <= tp_level
                    hit_sl = actual_exit_price >= sl_level
                
                # Update performance metrics
                if symbol not in self.performance_metrics:
                    self.performance_metrics[symbol] = {
                        'total_decisions': 0,
                        'tp_hits': 0,
                        'sl_hits': 0,
                        'avg_return': 0,
                        'win_rate': 0,
                        'avg_rr_achieved': 0
                    }
                
                metrics = self.performance_metrics[symbol]
                metrics['total_decisions'] += 1
                
                if hit_tp:
                    metrics['tp_hits'] += 1
                elif hit_sl:
                    metrics['sl_hits'] += 1
                
                # Update running averages
                metrics['avg_return'] = (metrics['avg_return'] * (metrics['total_decisions'] - 1) + actual_return) / metrics['total_decisions']
                metrics['win_rate'] = metrics['tp_hits'] / metrics['total_decisions']
                
                # Calculate achieved risk-reward ratio
                if hit_tp or hit_sl:
                    risk = abs(sl_level - entry_price)
                    reward = abs(actual_exit_price - entry_price)
                    achieved_rr = reward / risk if risk > 0 else 0
                    metrics['avg_rr_achieved'] = (metrics['avg_rr_achieved'] * (metrics['total_decisions'] - 1) + achieved_rr) / metrics['total_decisions']
                
                print(f"📊 [Master Agent] Updated performance for {symbol}:")
                print(f"   Win Rate: {metrics['win_rate']:.2%}")
                print(f"   Avg Return: {metrics['avg_return']:.2%}")
                print(f"   Avg RR Achieved: {metrics['avg_rr_achieved']:.2f}")
                
                # Learn from the outcome
                self._learn_from_outcome(symbol, decision_data, outcome, actual_return)
            
        except Exception as e:
            print(f"❌ [Master Agent] Error updating performance: {e}")
    
    def _learn_from_outcome(self, symbol, decision_data, outcome, actual_return):
        """Learn from trading outcomes to improve future decisions"""
        try:
            market_intelligence = decision_data.get('market_intelligence', {})
            reasoning = decision_data.get('reasoning', {})
            
            # Analyze what worked and what didn't
            if outcome == 'TP_HIT' and actual_return > 0:
                # Successful trade - reinforce successful patterns
                self._reinforce_successful_patterns(symbol, market_intelligence, reasoning)
            elif outcome == 'SL_HIT' and actual_return < 0:
                # Failed trade - learn from mistakes
                self._learn_from_failures(symbol, market_intelligence, reasoning)
            
            # Adjust decision weights based on performance
            self._adjust_decision_weights(symbol, market_intelligence, actual_return)
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in learning process: {e}")
    
    def _reinforce_successful_patterns(self, symbol, market_intelligence, reasoning):
        """Reinforce patterns that led to successful trades"""
        try:
            # Identify successful conditions
            technical = market_intelligence.get('technical', {})
            volatility_pct = technical.get('atr_pct', 0.01)
            trend_strength = market_intelligence.get('trend_analyzer', ('HOLD', 0.5))[1]
            
            # Slightly increase weights for conditions that worked
            if volatility_pct < 0.015 and trend_strength > 0.7:
                # Low volatility + strong trend = good conditions
                self.decision_weights['trend_weight'] = min(0.25, self.decision_weights['trend_weight'] * 1.05)
            elif volatility_pct > 0.03:
                # High volatility worked - increase volatility weight
                self.decision_weights['volatility_weight'] = min(0.3, self.decision_weights['volatility_weight'] * 1.02)
            
            print(f"🎯 [Master Agent] Reinforced successful patterns for {symbol}")
            
        except Exception as e:
            print(f"❌ [Master Agent] Error reinforcing patterns: {e}")
    
    def _learn_from_failures(self, symbol, market_intelligence, reasoning):
        """Learn from failed trades to avoid similar mistakes"""
        try:
            # Identify failure conditions
            technical = market_intelligence.get('technical', {})
            volatility_pct = technical.get('atr_pct', 0.01)
            bb_position = technical.get('bb_position', 0.5)
            
            # Adjust weights to reduce influence of factors that led to failure
            if bb_position > 0.8 or bb_position < 0.2:
                # Extreme Bollinger Band positions led to failure
                self.decision_weights['volatility_weight'] = max(0.15, self.decision_weights['volatility_weight'] * 0.95)
            
            if volatility_pct > 0.03:
                # High volatility led to failure - be more conservative
                self.decision_weights['atr_weight'] = max(0.25, self.decision_weights['atr_weight'] * 0.98)
            
            print(f"📚 [Master Agent] Learned from failure for {symbol}")
            
        except Exception as e:
            print(f"❌ [Master Agent] Error learning from failure: {e}")
    
    def _adjust_decision_weights(self, symbol, market_intelligence, actual_return):
        """Dynamically adjust decision weights based on performance"""
        try:
            # Get symbol performance metrics
            if symbol in self.performance_metrics:
                metrics = self.performance_metrics[symbol]
                win_rate = metrics['win_rate']
                
                # Adjust weights based on overall performance
                if win_rate > 0.6:  # Good performance
                    # Slightly increase confidence in current approach
                    for key in self.decision_weights:
                        self.decision_weights[key] = min(0.4, self.decision_weights[key] * 1.01)
                elif win_rate < 0.4:  # Poor performance
                    # Rebalance weights towards more conservative approach
                    self.decision_weights['atr_weight'] = min(0.35, self.decision_weights['atr_weight'] * 1.02)
                    self.decision_weights['support_resistance_weight'] = min(0.2, self.decision_weights['support_resistance_weight'] * 1.02)
                
                # Normalize weights to sum to 1.0
                total_weight = sum(self.decision_weights.values())
                for key in self.decision_weights:
                    self.decision_weights[key] /= total_weight
                
                print(f"⚖️ [Master Agent] Adjusted decision weights for {symbol} (Win Rate: {win_rate:.2%})")
            
        except Exception as e:
            print(f"❌ [Master Agent] Error adjusting weights: {e}")
    
    def get_performance_summary(self):
        """Get comprehensive performance summary"""
        try:
            if not self.performance_metrics:
                return {"message": "No performance data available"}
            
            summary = {
                'total_symbols': len(self.performance_metrics),
                'overall_metrics': {
                    'avg_win_rate': 0,
                    'avg_return': 0,
                    'total_decisions': 0,
                    'best_symbol': None,
                    'worst_symbol': None
                },
                'symbol_breakdown': {},
                'decision_weights': self.decision_weights.copy()
            }
            
            total_win_rate = 0
            total_return = 0
            total_decisions = 0
            best_performance = -999
            worst_performance = 999
            
            for symbol, metrics in self.performance_metrics.items():
                summary['symbol_breakdown'][symbol] = metrics.copy()
                
                total_win_rate += metrics['win_rate']
                total_return += metrics['avg_return']
                total_decisions += metrics['total_decisions']
                
                if metrics['avg_return'] > best_performance:
                    best_performance = metrics['avg_return']
                    summary['overall_metrics']['best_symbol'] = symbol
                
                if metrics['avg_return'] < worst_performance:
                    worst_performance = metrics['avg_return']
                    summary['overall_metrics']['worst_symbol'] = symbol
            
            # Calculate overall averages
            num_symbols = len(self.performance_metrics)
            summary['overall_metrics']['avg_win_rate'] = total_win_rate / num_symbols
            summary['overall_metrics']['avg_return'] = total_return / num_symbols
            summary['overall_metrics']['total_decisions'] = total_decisions
            
            return summary
            
        except Exception as e:
            print(f"❌ [Master Agent] Error generating performance summary: {e}")
            return {"error": str(e)}

class AdvancedEnsembleManager:
    """Advanced Ensemble System with Bagging, Boosting, v Stacking"""
    
    def __init__(self):
        self.base_models = {}
        self.meta_model = None
        self.ensemble_weights = {}
        self.model_performance = {}
        self.bagging_models = {}
        self.boosting_models = {}
        self.stacking_models = {}
        
    def initialize_ensemble_system(self):
        """Khởi tạo hệ thống ensemble"""
        # Initialize base models
        self.base_models = {
            'random_forest': RandomForestModel(),
            'gradient_boosting': GradientBoostingModel(),
            'neural_network': NeuralNetworkModel(),
            'svm': SVMModel(),
            'linear_regression': LinearRegressionModel(),
            'xgboost': XGBoostModel()
        }
        
        # Initialize ensemble techniques
        self._initialize_bagging()
        self._initialize_boosting()
        self._initialize_stacking()
        
        # Initialize ensemble weights
        self._initialize_ensemble_weights()
    
    def _initialize_bagging(self):
        """Khởi tạo Bagging models"""
        self.bagging_models = {
            'rf_bagging': RandomForestBagging(),
            'svm_bagging': SVMBagging(),
            'nn_bagging': NeuralNetworkBagging()
        }
    
    def _initialize_boosting(self):
        """Khởi tạo Boosting models"""
        self.boosting_models = {
            'ada_boost': AdaBoostModel(),
            'gradient_boost': GradientBoostingModel(),
            'xgboost_boost': XGBoostBoosting()
        }
    
    def _initialize_stacking(self):
        """Khởi tạo Stacking models"""
        self.stacking_models = {
            'level1_models': list(self.base_models.keys()),
            'meta_learner': MetaLearnerModel()
        }
    
    def _initialize_ensemble_weights(self):
        """Khởi tạo weights cho ensemble"""
        # Dynamic weights based on performance
        self.ensemble_weights = {
            'bagging': 0.3,
            'boosting': 0.3,
            'stacking': 0.4
        }
    
    def predict_ensemble(self, data, symbol):
        """Equal Using ensemble methods"""
        try:
            # Get predictions from all ensemble methods
            bagging_prediction = self._predict_bagging(data, symbol)
            boosting_prediction = self._predict_boosting(data, symbol)
            stacking_prediction = self._predict_stacking(data, symbol)
            
            # Weighted ensemble prediction
            final_prediction = self._combine_predictions(
                bagging_prediction, boosting_prediction, stacking_prediction
            )
            
            return final_prediction
            
        except Exception as e:
            logging.error(f"Error in ensemble prediction: {e}")
            return "HOLD", 0.5
    
    def _predict_bagging(self, data, symbol):
        """Equal Using Bagging"""
        try:
            predictions = []
            confidences = []
            
            for model_name, model in self.bagging_models.items():
                pred, conf = model.predict(data, symbol)
                predictions.append(pred)
                confidences.append(conf)
            
            # Majority voting
            prediction_counts = {}
            for pred in predictions:
                prediction_counts[pred] = prediction_counts.get(pred, 0) + 1
            
            final_prediction = max(prediction_counts, key=prediction_counts.get)
            final_confidence = max(confidences) * 0.8  # Bagging typically reduces confidence
            
            return final_prediction, final_confidence
            
        except Exception as e:
            logging.error(f"Error in bagging prediction: {e}")
            return "HOLD", 0.5
    
    def _predict_boosting(self, data, symbol):
        """Equal Using Boosting"""
        try:
            predictions = []
            confidences = []
            
            for model_name, model in self.boosting_models.items():
                pred, conf = model.predict(data, symbol)
                predictions.append(pred)
                confidences.append(conf)
            
            # Weighted voting based on model performance
            weighted_votes = {}
            total_weight = 0
            
            for i, (pred, conf) in enumerate(zip(predictions, confidences)):
                model_name = list(self.boosting_models.keys())[i]
                weight = conf * self.model_performance.get(model_name, {}).get(symbol, 0.5)
                
                if pred not in weighted_votes:
                    weighted_votes[pred] = 0
                weighted_votes[pred] += weight
                total_weight += weight
            
            if total_weight > 0:
                final_prediction = max(weighted_votes, key=weighted_votes.get)
                final_confidence = weighted_votes[final_prediction] / total_weight
            else:
                final_prediction = "HOLD"
                final_confidence = 0.5
            
            return final_prediction, final_confidence
            
        except Exception as e:
            logging.error(f"Error in boosting prediction: {e}")
            return "HOLD", 0.5
    
    def _predict_stacking(self, data, symbol):
        """Equal Using Stacking"""
        try:
            # Level 1: Get predictions from base models
            level1_predictions = {}
            level1_confidences = {}
            
            for model_name, model in self.base_models.items():
                pred, conf = model.predict(data, symbol)
                level1_predictions[model_name] = pred
                level1_confidences[model_name] = conf
            
            # Level 2: Meta-learner combines predictions
            meta_model = self.stacking_models['meta_learner']
            final_prediction, final_confidence = meta_model.predict(
                level1_predictions, level1_confidences, symbol
            )
            
            return final_prediction, final_confidence
            
        except Exception as e:
            logging.error(f"Error in stacking prediction: {e}")
            return "HOLD", 0.5
    
    def _combine_predictions(self, bagging_pred, boosting_pred, stacking_pred):
        """Combine predictions from all ensemble methods"""
        try:
            # Extract predictionfixndonfidences
            bag_pred, bag_conf = bagging_pred
            boost_pred, boost_conf = boosting_pred
            stack_pred, stack_conf = stacking_pred
            
            # Weightedombination
            weights = self.ensemble_weights
            total_weight = sum(weights.values())
            
            weighted_votes = {}
            weighted_votes[bag_pred] = bag_conf * weights['bagging']
            weighted_votes[boost_pred] = boost_conf * weights['boosting']
            weighted_votes[stack_pred] = stack_conf * weights['stacking']
            
            # Final prediction
            final_prediction = max(weighted_votes, key=weighted_votes.get)
            final_confidence = weighted_votes[final_prediction] / total_weight
            
            return final_prediction, final_confidence
            
        except Exception as e:
            logging.error(f"Error combining predictions: {e}")
            return "HOLD", 0.5
    
    def update_model_performance(self, symbol, prediction, actual_outcome):
        """Update performance of models"""
        try:
            # Update individual model performance
            for model_name in self.base_models.keys():
                if model_name not in self.model_performance:
                    self.model_performance[model_name] = {}
                if symbol not in self.model_performance[model_name]:
                    self.model_performance[model_name][symbol] = 0.5
                
                # Simple performance update (in real implementation, this would be more sophisticated)
                current_perf = self.model_performance[model_name][symbol]
                if actual_outcome > 0:  # Positive outcome
                    new_perf = min(0.95, current_perf + 0.01)
                else:  # Negative outcome
                    new_perf = max(0.05, current_perf - 0.01)
                
                self.model_performance[model_name][symbol] = new_perf
            
            # Update ensemble weights based on performance
            self._update_ensemble_weights(symbol)
            
        except Exception as e:
            logging.error(f"Error updating model performance: {e}")
    
    def _update_ensemble_weights(self, symbol):
        """Update weights of ensemble methods"""
        try:
            # Calculate performance for each ensemble method
            bagging_perf = self._calculate_method_performance('bagging', symbol)
            boosting_perf = self._calculate_method_performance('boosting', symbol)
            stacking_perf = self._calculate_method_performance('stacking', symbol)
            
            # Normalize weights
            total_perf = bagging_perf + boosting_perf + stacking_perf
            if total_perf > 0:
                self.ensemble_weights['bagging'] = bagging_perf / total_perf
                self.ensemble_weights['boosting'] = boosting_perf / total_perf
                self.ensemble_weights['stacking'] = stacking_perf / total_perf
            
        except Exception as e:
            logging.error(f"Error updating ensemble weights: {e}")
    
    def _calculate_method_performance(self, method, symbol):
        """Calculate performance of an ensemble method"""
        # Simplified performance calculation
        return 0.5  # Default performance

# Base Model Classes
class RandomForestModel:
    def predict(self, data, symbol):
        # Simulate Random Forest prediction
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.4, 0.8)
        return prediction, confidence

class GradientBoostingModel:
    def predict(self, data, symbol):
        # Simulate Gradient Boosting prediction
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.5, 0.9)
        return prediction, confidence

class NeuralNetworkModel:
    def predict(self, data, symbol):
        # Simulate Neural Network prediction
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.3, 0.7)
        return prediction, confidence

class SVMModel:
    def predict(self, data, symbol):
        # Simulate SVM prediction
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.4, 0.8)
        return prediction, confidence

class LinearRegressionModel:
    def predict(self, data, symbol):
        # Simulate Linear Regression prediction
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.3, 0.6)
        return prediction, confidence

class XGBoostModel:
    def predict(self, data, symbol):
        # Simulate XGBoost prediction
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.6, 0.9)
        return prediction, confidence

# Bagging Classes
class RandomForestBagging:
    def predict(self, data, symbol):
        # Simulate Random Forest Bagging
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.5, 0.8)
        return prediction, confidence

class SVMBagging:
    def predict(self, data, symbol):
        # Simulate SVM Bagging
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.4, 0.7)
        return prediction, confidence

class NeuralNetworkBagging:
    def predict(self, data, symbol):
        # Simulate Neural Network Bagging
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.3, 0.6)
        return prediction, confidence

# Boosting Classes
class AdaBoostModel:
    def predict(self, data, symbol):
        # Simulate AdaBoost prediction
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.5, 0.8)
        return prediction, confidence

class GradientBoostingModel:
    def predict(self, data, symbol):
        # Simulate Gradient Boosting prediction
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.6, 0.9)
        return prediction, confidence

class XGBoostBoosting:
    def predict(self, data, symbol):
        # Simulate XGBoost Boosting
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.7, 0.9)
        return prediction, confidence

# Stacking Classes
class MetaLearnerModel:
    def predict(self, level1_predictions, level1_confidences, symbol):
        # Simulate Meta-learner prediction
        prediction = np.random.choice(['BUY', 'SELL', 'HOLD'])
        confidence = np.random.uniform(0.6, 0.9)
        return prediction, confidence

class EnhancedTradingBot:
    # TM V THAY THFunction this in L P EnhancedTradingBot

    def __init__(self):
        print("🚀 [Bot Init] Starting EnhancedTradingBot initialization...")
        
        # Core trading attributes
        print("🔧 [Bot Init] Setting up core trading attributes...")
        self.active_symbols = set(SYMBOLS)  # Initialize with all symbols from SYMBOLS list
        self.trending_models = {}    # Dictionary to store trending models
        self.ranging_models = {}     # Dictionary to store ranging models
        self.open_positions = load_open_positions()  # Load existing positions from file
        self.data_manager = None  # Will be initialized after news_manager
        # Initialize news_manager attribute first
        self.news_manager = None
        print(" [Bot Init] Core trading attributes set")
        
        try:
            print("📰 [Bot Init] Initializing News Manager...")
            self.news_manager = NewsEconomicManager()  # News manager instance
            print(" [Bot Init] News Manager initialized successfully")
            
            # Initialize Daily News Scheduler
            print("📅 [Bot Init] Initializing Daily News Scheduler...")
            self.news_scheduler = DailyNewsScheduler(
                news_manager=self.news_manager,
                symbols_to_track=SYMBOLS  # Track ALL symbols for comprehensive news coverage
            )
            # Start the scheduler
            self.news_scheduler.start_scheduler()
            print(" [Bot Init] Daily News Scheduler initialized and started")
            
        except Exception as e:
            print(f" [Bot Init] Failed to initialize News Manager: {e}")
            print(f" [Bot Init] Error type: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            print("⚠️ [Bot Init] Setting news_manager to None and continuing...")
            self.news_manager = None
            self.news_scheduler = None
        
        # Initialize data_manager with news_manager reference
        print("📊 [Bot Init] Initializing Data Manager...")
        try:
            self.data_manager = EnhancedDataManager(news_manager=self.news_manager)
            print(" [Bot Init] Data Manager initialized successfully")
        except Exception as e:
            print(f" [Bot Init] Failed to initialize Data Manager: {e}")
            import traceback
            traceback.print_exc()
            raise e
        
        print("🔧 [Bot Init] Initializing other components...")
        try:
            self.news_filter = EnhancedNewsFilter()    # Enhanced news filter
            print(" [Bot Init] News Filter initialized")
            
            self.freshness_monitor = DataFreshnessMonitor()  # Data freshness monitor
            print(" [Bot Init] Freshness Monitor initialized")
            
            self.session_manager = EnhancedSessionManager()  # Enhanced session manager
            print(" [Bot Init] Session Manager initialized")
            
            self.rl_evaluator = EnhancedRLEvaluation()  # Enhanced RL evaluation
            print(" [Bot Init] RL Evaluator initialized")
            
            self.auto_retrain_manager = AutoRetrainManager(self)  # Auto-retrain manager
            print(" [Bot Init] Auto Retrain Manager initialized")
            
            self.api_monitor = APIMonitoringSystem()  # API monitoring system
            print(" [Bot Init] API Monitor initialized")
        except Exception as e:
            print(f" [Bot Init] Failed to initialize components: {e}")
            import traceback
            traceback.print_exc()
            raise e

        # RL and portfolio management
        print("🎯 [Bot Init] Setting up RL and portfolio management...")
        self.use_rl = True
        self.portfolio_rl_agent = None
        self.drift_monitor = None
        
        # Enhanced RL Flexibility Features
        self.adaptive_confidence_thresholds = {
            'BTCUSD': 0.52,
            'ETHUSD': 0.52, 
            'XAUUSD': 0.55,  # Gold needs higher confidence
            'SPX500': 0.58,  # Index needs highest confidence
            'EURUSD': 0.55   # Forex needs higher confidence
        }
        
        self.market_regime_detector = MarketRegimeDetector()
        self.rl_performance_tracker = RLPerformanceTracker()
        self.dynamic_action_space = DynamicActionSpace()
        
        # Multi-Task Learning Support
        self.task_specific_models = {
            'trending': {},
            'ranging': {},
            'news_driven': {},
            'volatile': {}
        }
        self.task_selector = TaskSelector()
        
        # Transfer Learning Support
        self.transfer_learning_manager = TransferLearningManager()
        
        # Advanced Master Agent System for TP/SL Decisions
        print("🎯 [Bot Init] Initializing Master Agent for TP/SL decisions...")
        self.master_agent_coordinator = MasterAgent()
        print("✅ [Bot Init] Master Agent initialized successfully")
        
        # Advanced Ensemble System
        self.ensemble_manager = AdvancedEnsembleManager()
        self.ensemble_manager.initialize_ensemble_system()

        # Risk management
        self.consecutive_data_failures = 0
        self.circuit_breaker_active = False

        # Weekendlose management
        self.weekend_close_executed = False

        # Experiment systemintegration
        self.optimized_configs = STRATEGY_CONFIG_BY_SYMBOL
        self.config_metadata = CONFIG_METADATA
        self.last_config_update = None
        
        # Performance metrics
        self.performance_metrics = {
            "total_trades": 0,
            "winning_trades": 0,
            "total_pips": 0,
            "win_rate": 0
        }
        
        # Database connection with connection pooling
        self.conn = None
        self._db_lock = asyncio.Lock()  # Thread safety for database operations
        
        # Notification thresholds
        self.SL_CHANGE_NOTIFICATION_THRESHOLD_PIPS = 5.0  # Minimum pip change to notify
        
        # Debug news manager status
        self._debug_news_manager_status()
        
        # Test news manager functionality
        self.test_news_manager_functionality()
        
        self._init_database()
        
        print(" [Bot Init] EnhancedTradingBot initialization completed successfully!")

    def _debug_news_manager_status(self):
        """Debug news manager initialization status"""
        print("\n [Debug] News Manager Status Check:")
        print(f"   - hasattr(self, 'news_manager'): {hasattr(self, 'news_manager')}")
        
        if hasattr(self, 'news_manager'):
            print(f"   - news_manager is not None: {self.news_manager is not None}")
            if self.news_manager is not None:
                print(f"   - news_manager type: {type(self.news_manager)}")
                print(f"   - news_manager has llm_analyzer: {hasattr(self.news_manager, 'llm_analyzer')}")
                if hasattr(self.news_manager, 'llm_analyzer'):
                    print(f"   - llm_analyzer is not None: {self.news_manager.llm_analyzer is not None}")
                    if self.news_manager.llm_analyzer is not None:
                        print(f"   - llm_analyzer has model: {hasattr(self.news_manager.llm_analyzer, 'model')}")
                        if hasattr(self.news_manager.llm_analyzer, 'model'):
                            print(f"   - model is not None: {self.news_manager.llm_analyzer.model is not None}")
                print(f"   - news_manager has news_providers: {hasattr(self.news_manager, 'news_providers')}")
                if hasattr(self.news_manager, 'news_providers'):
                    print(f"   - news_providers count: {len(self.news_manager.news_providers)}")
            else:
                print("   news_manager is None")
        else:
            print("   news_manager attribute does not exist")
        print("=" * 50)
    
    def test_news_manager_functionality(self):
        """Test news manager functionality with comprehensive error handling"""
        print("🧪 [Bot Init] Testing news manager functionality...")
        try:
            if self.news_manager is None:
                print("⚠️ [Bot Init] News manager is None, skipping functionality test")
                return
            
            print(" [Bot Init] News manager exists, testing basic functionality...")
            
            # Test basic news manager methods
            if hasattr(self.news_manager, 'get_latest_news'):
                print(" [Bot Init] News manager has get_latest_news method")
            else:
                print("⚠️ [Bot Init] News manager missing get_latest_news method")
                
            if hasattr(self.news_manager, 'get_economic_calendar'):
                print(" [Bot Init] News manager has get_economic_calendar method")
            else:
                print("⚠️ [Bot Init] News manager missing get_economic_calendar method")
                
            print(" [Bot Init] News manager functionality test completed")
            
        except Exception as e:
            print(f" [Bot Init] Error testing news manager functionality: {e}")
            import traceback
            traceback.print_exc()
            
    def _init_database(self):
        """Initialize database connection with comprehensive error handling"""
        print("💾 [Bot Init] Initializing database connection...")
        try:
            # Database initialization code here
            print(" [Bot Init] Database connection initialized successfully")
        except Exception as e:
            print(f" [Bot Init] Failed to initialize database: {e}")
            import traceback
            traceback.print_exc()
            # Don't raise exception, continue without database
    
    def update_rl_performance(self, symbol, action, confidence, pnl):
        """Update RL performance metricfixfter trade completion"""
        try:
            # Calculate outcome (positive/negative)
            outcome = 1 if pnl > 0 else -1
            
            # Update performance tracker
            self.rl_performance_tracker.update_performance(symbol, action, confidence, outcome)
            
            # Log performance update
            logger = BOT_LOGGERS['RLStrategy']
            logger.info(f" [RL Performance] Updated {symbol}: action={action}, confidence={confidence:.2%}, pnl={pnl:.2f}, outcome={outcome}")
            
            # Check if threshold needs adjustment
            new_threshold = self.rl_performance_tracker.get_adaptive_threshold(symbol)
            old_threshold = self.adaptive_confidence_thresholds.get(symbol, 0.52)
            
            if abs(new_threshold - old_threshold) > 0.02:  # Significant change
                self.adaptive_confidence_thresholds[symbol] = new_threshold
                logger.info(f" [RL Adaptive] {symbol} threshold updated: {old_threshold:.2%} -> {new_threshold:.2%}")
            
            # Check if retraining is needed
            self._check_retraining_needed(symbol)
                
        except Exception as e:
            logging.error(f"Error updating RL performance: {e}")
    
    def _check_retraining_needed(self, symbol):
        """Check if RL model needs retraining based on performance metrics"""
        try:
            logger = BOT_LOGGERS['RLStrategy']
            
            if symbol not in self.rl_performance_tracker.symbol_performance:
                return False
                
            perf = self.rl_performance_tracker.symbol_performance[symbol]
            
            # Need at least 20 trades to make decision
            if perf['total_actions'] < 20:
                return False
            
            success_rate = perf['successful_actions'] / perf['total_actions']
            
            # Retraining triggers:
            retrain_triggers = []
            
            # 1. Sualless rate too low (< 30%)
            if success_rate < 0.30:
                retrain_triggers.append(f"Sualless rate too low: {success_rate:.1%}")
            
            # 2. Recent performance decline (last 10 trades)
            if len(perf['outcome_history']) >= 10:
                recent_outcomes = perf['outcome_history'][-10:]
                recent_success_rate = sum(1 for x in recent_outcomes if x > 0) / len(recent_outcomes)
                if recent_success_rate < 0.20:
                    retrain_triggers.append(f"Recent performance decline: {recent_success_rate:.1%}")
            
            # 3. Confidence calibration issues
            if len(perf['confidence_history']) >= 10:
                recent_confidences = perf['confidence_history'][-10:]
                avg_confidence = sum(recent_confidences) / len(recent_confidences)
                if avg_confidence > 0.8 and success_rate < 0.4:  # High confidence but low success
                    retrain_triggers.append(f"Overconfidenfrom modelodel: {avg_confidence:.1%} confidence vs {success_rate:.1%} success")
            
            # 4. Data drift detection (simplified)
            if len(perf['outcome_history']) >= 50:
                first_half = perf['outcome_history'][:25]
                second_half = perf['outcome_history'][-25:]
                first_success_rate = sum(1 for x in first_half if x > 0) / len(first_half)
                second_success_rate = sum(1 for x in second_half if x > 0) / len(second_half)
                
                if abs(first_success_rate - second_success_rate) > 0.3:
                    retrain_triggers.append(f"Data drift detected: {first_success_rate:.1%} -> {second_success_rate:.1%}")
            
            # Log retraining recommendation
            if retrain_triggers:
                logger.warning(f" [RL Retrain] {symbol} needs retraining:")
                for trigger in retrain_triggers:
                    logger.warning(f"   - {trigger}")
                logger.warning(f"   - Total trades: {perf['total_actions']}")
                logger.warning(f"   - Sualless rate: {success_rate:.1%}")
                logger.warning(f"   - Recommendation: Retrain RL model for {symbol}")
                
                # Send Discord alert
                self._send_retrain_alert(symbol, retrain_triggers, success_rate)
                
                return True
            else:
                logger.info(f"[RL Retrain] {symbol} performance ifixalleptable (success rate: {success_rate:.1%})")
                return False
                
        except Exception as e:
            logging.error(f"Error checking retraining need for {symbol}: {e}")
            return False
    
    def _send_retrain_alert(self, symbol, triggers, success_rate):
        """Send Discord alert about retraining recommendation"""
        try:
            if not DISCORD_WEBHOOK:
                return
                
            embed = {
                "title": f" RL Model Retraining Recommended",
                "description": f"**Symbol:** {symbol}\n**Sualless Rate:** {success_rate:.1%}",
                "color": 0xff6b6b,  # Redolor
                "fields": [
                    {
                        "name": "Retraining Triggers",
                        "value": "\n".join([f" {trigger}" for trigger in triggers]),
                        "inline": False
                    },
                    {
                        "name": "Recommendation",
                        "value": "Consider retraining RL model with recent data",
                        "inline": False
                    }
                ],
                "timestamp": datetime.now().isoformat(),
                "footer": {
                    "text": "Trading Bot - RL Performance Monitor"
                }
            }
            
            payload = {
                "embeds": [embed],
                "username": "RL Performance Monitor"
            }
            
            response = requests.post(DISCORD_WEBHOOK, json=payload, timeout=10)
            if response.status_code in [200, 204]:
                logging.info(f"[Discord] Retrain alert sent for {symbol}")
            else:
                logging.warning(f"❌ [Discord] Failed to send retrain alert: {response.status_code}")
                
        except Exception as e:
            logging.error(f"Error sending retrain alert: {e}")
    
    def _combine_all_decisions(self, rl_action, rl_confidence, master_action, master_confidence, ensemble_action, ensemble_confidence, symbol):
        """Combine decisions from RL, Master Agent, and Ensemble with Dynamic Confidence Adjustment"""
        try:
            # Dynamic weight adjustment based on confidence levels
            weights = self._calculate_dynamic_weights(rl_confidence, master_confidence, ensemble_confidence)
            
            # Weighted voting system with dynamic weights
            decisions = {
                rl_action: rl_confidence * weights['rl'],
                master_action: master_confidence * weights['master'],
                ensemble_action: ensemble_confidence * weights['ensemble']
            }
            
            # Select decision with highest weighted vote
            final_decision = max(decisions, key=decisions.get)
            final_confidence = decisions[final_decision]
            
            # Apply final confidence adjustment based on decision consistency
            consistency_factor = self._calculate_decision_consistency(rl_action, master_action, ensemble_action)
            final_confidence *= consistency_factor
            
            # Clamp final confidence
            final_confidence = max(0.1, min(0.95, final_confidence))
            
            # Log decision combination with enhanced details
            logger = BOT_LOGGERS['RLStrategy']
            logger.info(f" [Dynamic Decision Fusion] {symbol}:")
            logger.info(f"   - RL: {rl_action} ({rl_confidence:.2%}) [Weight: {weights['rl']:.2f}]")
            logger.info(f"   - Master: {master_action} ({master_confidence:.2%}) [Weight: {weights['master']:.2f}]")
            logger.info(f"   - Ensemble: {ensemble_action} ({ensemble_confidence:.2%}) [Weight: {weights['ensemble']:.2f}]")
            logger.info(f"   - Consistency: {consistency_factor:.3f}")
            logger.info(f"   - Final: {final_decision} ({final_confidence:.2%})")
            
            # Trigger online learning feedback loop
            self._trigger_online_learning_feedback(symbol, rl_action, master_action, ensemble_action, final_decision, final_confidence)
            
            return final_decision, final_confidence
            
        except Exception as e:
            logging.error(f"Error combining decisions: {e}")
            return "HOLD", 0.5
    
    def _calculate_dynamic_weights(self, rl_confidence, master_confidence, ensemble_confidence):
        """Calculate dynamic weights based on confidence levels"""
        try:
            # Base weights
            base_weights = {'rl': 0.4, 'master': 0.35, 'ensemble': 0.25}
            
            # Confidence-based adjustments
            total_confidence = rl_confidence + master_confidence + ensemble_confidence
            
            if total_confidence > 0:
                # Normalize weights based on relative confidence
                confidence_ratios = {
                    'rl': rl_confidence / total_confidence,
                    'master': master_confidence / total_confidence,
                    'ensemble': ensemble_confidence / total_confidence
                }
                
                # Blend base weights with confidence ratios
                weights = {}
                for key in base_weights:
                    weights[key] = base_weights[key] * 0.7 + confidence_ratios[key] * 0.3
            else:
                weights = base_weights
            
            # Normalize weights to sum to 1
            total_weight = sum(weights.values())
            if total_weight > 0:
                weights = {k: v / total_weight for k, v in weights.items()}
            
            return weights
            
        except Exception as e:
            logging.error(f"Error calculating dynamic weights: {e}")
            return {'rl': 0.4, 'master': 0.35, 'ensemble': 0.25}
    
    def _calculate_decision_consistency(self, rl_action, master_action, ensemble_action):
        """Calculate consistency factor between decisions"""
        try:
            decisions = [rl_action, master_action, ensemble_action]
            
            # Countoccurrences of each decision
            decision_counts = {}
            for decision in decisions:
                decision_counts[decision] = decision_counts.get(decision, 0) + 1
            
            # Calculate consistency ratio
            max_count = max(decision_counts.values())
            consistency_ratio = max_count / len(decisions)
            
            # Convert to adjustment factor (0.8 - 1.2)
            consistency_factor = 0.8 + (consistency_ratio * 0.4)
            
            return consistency_factor
            
        except Exception as e:
            logging.error(f"Error calculating decision consistency: {e}")
            return 1.0
    
    def _trigger_online_learning_feedback(self, symbol, rl_action, master_action, ensemble_action, final_decision, final_confidence):
        """activate online learning feedback loop"""
        try:
            # T o feedback data cho online learning
            feedback_data = {
                'symbol': symbol,
                'timestamp': pd.Timestamp.now(),
                'rl_action': rl_action,
                'master_action': master_action,
                'ensemble_action': ensemble_action,
                'final_decision': final_decision,
                'final_confidence': final_confidence,
                'decision_consistency': self._calculate_decision_consistency(rl_action, master_action, ensemble_action)
            }
            
            # Send feedback to online learning manager
            if hasattr(self, 'auto_retrain_manager') and hasattr(self.auto_retrain_manager, 'online_learning'):
                self.auto_retrain_manager.online_learning.process_decision_feedback(feedback_data)
            
            logging.info(f" [Online Learning] Feedback sent for {symbol}: {final_decision} ({final_confidence:.2%})")
            
        except Exception as e:
            logging.error(f"Error triggering online learning feedback: {e}")
    
    def _combine_all_decisions_with_online_learning(self, rl_action, rl_confidence, master_action, master_confidence, ensemble_action, ensemble_confidence, online_action, online_confidence, symbol):
        """Combine decisions from RL, Master Agent, Ensemble, and Online Learning with Dynamic Confidence Adjustment"""
        try:
            # Dynamic weight adjustment based on confidence levels
            weights = self._calculate_dynamic_weights_with_online_learning(
                rl_confidence, master_confidence, ensemble_confidence, online_confidence
            )
            
            # Weighted voting system with dynamic weights
            decisions = {
                rl_action: rl_confidence * weights['rl'],
                master_action: master_confidence * weights['master'],
                ensemble_action: ensemble_confidence * weights['ensemble'],
                online_action: online_confidence * weights['online']
            }
            
            # Select decision with highest weighted vote
            final_decision = max(decisions, key=decisions.get)
            final_confidence = decisions[final_decision]
            
            # Apply final confidence adjustment based on decision consistency
            consistency_factor = self._calculate_decision_consistency_with_online_learning(
                rl_action, master_action, ensemble_action, online_action
            )
            final_confidence *= consistency_factor
            
            # Clamp final confidence
            final_confidence = max(0.1, min(0.95, final_confidence))
            
            # Log decision combination with enhanced details
            logger = BOT_LOGGERS['RLStrategy']
            logger.info(f" [Dynamic Decision Fusion with Online Learning] {symbol}:")
            logger.info(f"   - RL: {rl_action} ({rl_confidence:.2%}) [Weight: {weights['rl']:.2f}]")
            logger.info(f"   - Master: {master_action} ({master_confidence:.2%}) [Weight: {weights['master']:.2f}]")
            logger.info(f"   - Ensemble: {ensemble_action} ({ensemble_confidence:.2%}) [Weight: {weights['ensemble']:.2f}]")
            logger.info(f"   - Online: {online_action} ({online_confidence:.2%}) [Weight: {weights['online']:.2f}]")
            logger.info(f"   - Consistency: {consistency_factor:.3f}")
            logger.info(f"   - Final: {final_decision} ({final_confidence:.2%})")
            
            # Trigger online learning feedback loop
            self._trigger_online_learning_feedback_with_online(symbol, rl_action, master_action, ensemble_action, online_action, final_decision, final_confidence)
            
            return final_decision, final_confidence
            
        except Exception as e:
            logging.error(f"Error combining decisions with online learning: {e}")
            return "HOLD", 0.5
    
    def _calculate_dynamic_weights_with_online_learning(self, rl_confidence, master_confidence, ensemble_confidence, online_confidence):
        """Calculate dynamic weights including online learning"""
        try:
            # Base weights with online learning
            base_weights = {'rl': 0.3, 'master': 0.3, 'ensemble': 0.2, 'online': 0.2}
            
            # Confidence-based adjustments
            total_confidence = rl_confidence + master_confidence + ensemble_confidence + online_confidence
            
            if total_confidence > 0:
                # Normalize weights based on relative confidence
                confidence_ratios = {
                    'rl': rl_confidence / total_confidence,
                    'master': master_confidence / total_confidence,
                    'ensemble': ensemble_confidence / total_confidence,
                    'online': online_confidence / total_confidence
                }
                
                # Blend base weights with confidence ratios
                weights = {}
                for key in base_weights:
                    weights[key] = base_weights[key] * 0.7 + confidence_ratios[key] * 0.3
            else:
                weights = base_weights
            
            # Normalize weights to sum to 1
            total_weight = sum(weights.values())
            if total_weight > 0:
                weights = {k: v / total_weight for k, v in weights.items()}
            
            return weights
            
        except Exception as e:
            logging.error(f"Error calculating dynamic weights with online learning: {e}")
            return {'rl': 0.3, 'master': 0.3, 'ensemble': 0.2, 'online': 0.2}
    
    def _calculate_decision_consistency_with_online_learning(self, rl_action, master_action, ensemble_action, online_action):
        """Calculate consistency factor including online learning"""
        try:
            decisions = [rl_action, master_action, ensemble_action, online_action]
            
            # Countoccurrences of each decision
            decision_counts = {}
            for decision in decisions:
                decision_counts[decision] = decision_counts.get(decision, 0) + 1
            
            # Calculate consistency ratio
            max_count = max(decision_counts.values())
            consistency_ratio = max_count / len(decisions)
            
            # Convert to adjustment factor (0.7 - 1.3) - slightly wider range for 4 decisions
            consistency_factor = 0.7 + (consistency_ratio * 0.6)
            
            return consistency_factor
            
        except Exception as e:
            logging.error(f"Error calculating decision consistency with online learning: {e}")
            return 1.0
    
    def _trigger_online_learning_feedback_with_online(self, symbol, rl_action, master_action, ensemble_action, online_action, final_decision, final_confidence):
        """activate online learning feedback loop bao g m online decision"""
        try:
            # T o feedback data cho online learning
            feedback_data = {
                'symbol': symbol,
                'timestamp': pd.Timestamp.now(),
                'rl_action': rl_action,
                'master_action': master_action,
                'ensemble_action': ensemble_action,
                'online_action': online_action,
                'final_decision': final_decision,
                'final_confidence': final_confidence,
                'decision_consistency': self._calculate_decision_consistency_with_online_learning(rl_action, master_action, ensemble_action, online_action)
            }
            
            # Send feedback to online learning manager
            if hasattr(self, 'auto_retrain_manager') and hasattr(self.auto_retrain_manager, 'online_learning'):
                self.auto_retrain_manager.online_learning.process_decision_feedback(feedback_data)
            
            logging.info(f" [Online Learning] Enhanced feedback sent for {symbol}: {final_decision} ({final_confidence:.2%})")
            
        except Exception as e:
            logging.error(f"Error triggering enhanced online learning feedback: {e}")

    def _trigger_online_learning_feedback_enhanced(self, symbol, final_decision, final_confidence, market_data=None):
        """Enhanced online learning feedback loop with market data integration"""
        try:
            # Create enhanced feedback data with market context
            feedback_data = {
                'symbol': symbol,
                'timestamp': pd.Timestamp.now(),
                'final_decision': final_decision,
                'final_confidence': final_confidence,
                'market_data_available': market_data is not None and not market_data.empty if hasattr(market_data, 'empty') else market_data is not None
            }
            
            # Add market data features if available
            if market_data is not None and hasattr(market_data, 'empty') and not market_data.empty:
                try:
                    # Add recent market indicators to feedback
                    if len(market_data) > 0:
                        latest_data = market_data.iloc[-1]
                        feedback_data.update({
                            'market_price': latest_data.get('close', None),
                            'market_volume': latest_data.get('volume', None),
                            'market_volatility': market_data['close'].pct_change().std() if 'close' in market_data.columns and len(market_data) > 1 else None
                        })
                except Exception as market_error:
                    logging.warning(f"Error processing market data for feedback: {market_error}")
            
            # Send enhanced feedback to online learning manager
            if hasattr(self, 'auto_retrain_manager') and hasattr(self.auto_retrain_manager, 'online_learning'):
                self.auto_retrain_manager.online_learning.process_decision_feedback(feedback_data)
            
            logging.info(f"[Online Learning Enhanced] Feedback sent for {symbol}: {final_decision} ({final_confidence:.2%})")
            
        except Exception as e:
            logging.error(f"Error triggering enhanced online learning feedback: {e}")

    def _init_database(self):
        """Initialize database connection"""
        try:
            # Create database file path
            db_path = "trading_bot.db"
            
            # Ensure directory exists (only if db_path contains a directory)
            db_dir = os.path.dirname(db_path)
            if db_dir:  # Only create directory if path contains one
                os.makedirs(db_dir, exist_ok=True)
            
            # Connect to database
            self.conn = sqlite3.connect(db_path)
            
            # Create trades table
            self._create_trades_table()
            
            # Run database migrations
            self._run_database_migrations()
            
            print(f"Database initialized: {db_path}")
            print(f"[DB Debug] Database file exists: {os.path.exists(db_path)}")
            print(f"[DB Debug] Database file size: {os.path.getsize(db_path) if os.path.exists(db_path) else 'N/A'} bytes")
            
            # Resend signals on startup if enabled
            print(f" [Debug] DISCORD_CONFIG: {DISCORD_CONFIG}")
            print(f" [Debug] RESEND_SIGNALS_ON_STARTUP: {DISCORD_CONFIG.get('RESEND_SIGNALS_ON_STARTUP', False)}")
            if DISCORD_CONFIG.get("RESEND_SIGNALS_ON_STARTUP", False):
                print("🔄 [Discord] Resend signals on startup enabled - starting resend process...")
                # Delay dd m b o bot d kh i to hon ton
                time.sleep(3)
                self.resend_signals_on_startup()
            else:
                print(" [Discord] Resend signals on startup is DISABLED")
            
        except Exception as e:
            print(f"Database initialization failed: {e}")
            self.conn = None

    def _create_trades_table(self):
        """Create trades table in database"""
        try:
            cursor = self.conn.cursor()
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS trades (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    signal TEXT NOT NULL,
                    entry_price REAL NOT NULL,
                    exit_price REAL,
                    opened_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    closed_at TIMESTAMP,
                    reason TEXT,
                    pips REAL,
                    confidence REAL,
                    -- all from m I CHO TCA --
                    signal_price REAL,    -- Gi mid t i thi dim T+n hi+u
                    execution_slippage_pips REAL, -- tru t gi khi t hnh dng
                    spread_cost_pips REAL -- Chi ph spread
                )
            """
            )
            
            # Migration: Add opened_at column if it doesn't exist
            try:
                cursor.execute("ALTER TABLE trades ADD COLUMN opened_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP")
                print("[DB] Migration: Added opened_at column to trades table")
            except sqlite3.OperationalError as e:
                if "duplicate column name" in str(e).lower():
                    print("[DB] Migration: opened_at column already exists")
                else:
                    print(f"[DB] Migration warning: {e}")
            
            self.conn.commit()
            print("[DB] 'trades' table is ready with TCA columns.")
        except Exception as e:
            print(f" Error creating 'trades' table: {e}")

    def _run_database_migrations(self):
        """Run database migrations to update schema"""
        try:
            cursor = self.conn.cursor()
            
            # Migration 1: Add opened_at column if it doesn't exist
            try:
                cursor.execute("ALTER TABLE trades ADD COLUMN opened_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP")
                print("[DB] Migration 1: Added opened_at column to trades table")
            except sqlite3.OperationalError as e:
                if "duplicate column name" in str(e).lower():
                    print("[DB] Migration 1: opened_at column already exists")
                else:
                    print(f"[DB] Migration 1 warning: {e}")
            
            # Migration 2: Add any other missing columns
            try:
                cursor.execute("ALTER TABLE trades ADD COLUMN signal_price REAL")
                print("[DB] Migration 2: Added signal_price column to trades table")
            except sqlite3.OperationalError as e:
                if "duplicate column name" in str(e).lower():
                    print("[DB] Migration 2: signal_price column already exists")
                else:
                    print(f"[DB] Migration 2 warning: {e}")
            
            try:
                cursor.execute("ALTER TABLE trades ADD COLUMN execution_slippage_pips REAL")
                print("[DB] Migration 3: Added execution_slippage_pips column to trades table")
            except sqlite3.OperationalError as e:
                if "duplicate column name" in str(e).lower():
                    print("[DB] Migration 3: execution_slippage_pips column already exists")
                else:
                    print(f"[DB] Migration 3 warning: {e}")
            
            try:
                cursor.execute("ALTER TABLE trades ADD COLUMN spread_cost_pips REAL")
                print("[DB] Migration 4: Added spread_cost_pips column to trades table")
            except sqlite3.OperationalError as e:
                if "duplicate column name" in str(e).lower():
                    print("[DB] Migration 4: spread_cost_pips column already exists")
                else:
                    print(f"[DB] Migration 4 warning: {e}")
            
            self.conn.commit()
            print("[DB]  All database migrations completed successfully")
            
        except Exception as e:
            print(f" Database migration failed: {e}")

    def update_performance_metrics(self):
        """Update performance metrics from database"""
        if not self.conn:
            return
        try:
            cursor = self.conn.cursor()
            total_trades_result = cursor.execute(
                "SELECT COUNT(*) FROM trades WHERE exit_price IS NOT NULL"
            ).fetchone()
            total_trades = total_trades_result[0] if total_trades_result else 0

            winning_trades_result = cursor.execute(
                "SELECT COUNT(*) FROM trades WHERE pips > 0 AND exit_price IS NOT NULL"
            ).fetchone()
            winning_trades = winning_trades_result[0] if winning_trades_result else 0

            total_pips_result = cursor.execute(
                "SELECT SUM(pips) FROM trades WHERE exit_price IS NOT NULL"
            ).fetchone()
            total_pips = total_pips_result[0] if total_pips_result[0] is not None else 0

            win_rate = (winning_trades / total_trades) * 100 if total_trades > 0 else 0
            self.performance_metrics = {
                "total_trades": total_trades,
                "winning_trades": winning_trades,
                "total_pips": total_pips,
                "win_rate": win_rate,
            }
        except Exception as e:
            print(f"Li c p nh t chshi u su t: {e}")

    def evaluate_symbol_performance(self, symbol):
        """
        nh gi performance of symbol d a trn PERFORMANCE_THRESHOLDS
        Trvrating: "excellent", "good", "acceptable", "poor"
        """
        try:
            if not self.conn:
                return "poor", "Database not available"
            
            cursor = self.conn.cursor()
            
            # Check xem from opened_at from n t i not
            cursor.execute("PRAGMA table_info(trades)")
            columns = [column[1] for column in cursor.fetchall()]
            
            if 'opened_at' not in columns:
                print(f" [Performance] from 'opened_at' not t n t i in database. Using 'id' ds p x p.")
                # Fallback: Using id ds p x p thay v opened_at
                trades_query = """
                    SELECT pips, entry_price, exit_price, id, closed_at
                    FROM trades 
                    WHERE symbol = ? AND exit_price IS NOT NULL
                    ORDER BY id DESC
                    LIMIT 100
                """
            else:
                trades_query = """
                    SELECT pips, entry_price, exit_price, opened_at, closed_at
                    FROM trades 
                    WHERE symbol = ? AND exit_price IS NOT NULL
                    ORDER BY opened_at DESC
                    LIMIT 100
                """
            
            trades = cursor.execute(trades_query, (symbol,)).fetchall()
            
            if len(trades) < 10:  # needs t nh t 10 trades ddnh gi
                return "poor", f"Insufficient data: {len(trades)} trades"
            
            # Calculate metrics
            pips_list = [trade[0] for trade in trades]
            total_pips = sum(pips_list)
            winning_trades = len([p for p in pips_list if p > 0])
            total_trades = len(pips_list)
            win_rate = winning_trades / total_trades
            
            # Calculate profit factor
            gross_profit = sum([p for p in pips_list if p > 0])
            gross_loss = abs(sum([p for p in pips_list if p < 0]))
            profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
            
            # Calculate max drawdown
            cumulative_pips = []
            running_total = 0
            for pips in pips_list:
                running_total += pips
                cumulative_pips.append(running_total)
            
            peak = cumulative_pips[0]
            max_drawdown = 0
            for value in cumulative_pips:
                if value > peak:
                    peak = value
                drawdown = peak - value
                if drawdown > max_drawdown:
                    max_drawdown = drawdown
            
            max_drawdown_pct = max_drawdown / abs(peak) if peak != 0 else 0
            
            # Calculate average trade
            avg_trade = total_pips / total_trades
            
            # Calculate consecutive losses
            consecutive_losses = 0
            max_consecutive_losses = 0
            for pips in pips_list:
                if pips < 0:
                    consecutive_losses += 1
                    max_consecutive_losses = max(max_consecutive_losses, consecutive_losses)
                else:
                    consecutive_losses = 0
            
            # Calculate recovery factor (simplified)
            recovery_factor = gross_profit / max_drawdown if max_drawdown > 0 else float('inf')
            
            # So snh with PERFORMANCE_THRESHOLDS
            performance_data = {
                "win_rate": win_rate,
                "profit_factor": profit_factor,
                "max_drawdown": max_drawdown_pct,
                "avg_trade": avg_trade,
                "consecutive_losses": max_consecutive_losses,
                "recovery_factor": recovery_factor
            }
            
            # nh gi theo total level
            if (win_rate >= PERFORMANCE_THRESHOLDS["excellent"]["win_rate"] and
                profit_factor >= PERFORMANCE_THRESHOLDS["excellent"]["profit_factor"] and
                max_drawdown_pct <= PERFORMANCE_THRESHOLDS["excellent"]["max_drawdown"] and
                avg_trade >= PERFORMANCE_THRESHOLDS["excellent"]["avg_trade"] and
                max_consecutive_losses <= PERFORMANCE_THRESHOLDS["excellent"]["consecutive_losses"]):
                return "excellent", performance_data
            
            elif (win_rate >= PERFORMANCE_THRESHOLDS["good"]["win_rate"] and
                  profit_factor >= PERFORMANCE_THRESHOLDS["good"]["profit_factor"] and
                  max_drawdown_pct <= PERFORMANCE_THRESHOLDS["good"]["max_drawdown"] and
                  avg_trade >= PERFORMANCE_THRESHOLDS["good"]["avg_trade"] and
                  max_consecutive_losses <= PERFORMANCE_THRESHOLDS["good"]["consecutive_losses"]):
                return "good", performance_data
            
            elif (win_rate >= PERFORMANCE_THRESHOLDS["acceptable"]["win_rate"] and
                  profit_factor >= PERFORMANCE_THRESHOLDS["acceptable"]["profit_factor"] and
                  max_drawdown_pct <= PERFORMANCE_THRESHOLDS["acceptable"]["max_drawdown"] and
                  avg_trade >= PERFORMANCE_THRESHOLDS["acceptable"]["avg_trade"] and
                  max_consecutive_losses <= PERFORMANCE_THRESHOLDS["acceptable"]["consecutive_losses"]):
                return "acceptable", performance_data
            
            else:
                return "poor", performance_data
                
        except Exception as e:
            print(f"Li dnh gi performance cho {symbol}: {e}")
            return "poor", f"Error: {e}"

    def adjust_strategy_based_on_performance(self, symbol, performance_rating):
        """
        Adjust strategy based on performance rating
        """
        try:
            print(f" [Performance Adjustment] Adjusting strategy for {symbol} (Rating: {performance_rating})")
            
            # L y c configuration current
            current_config = self.get_optimized_config(symbol)
            
            if performance_rating == "excellent":
                # Tang position size v confidence threshold
                print(f"   {symbol}: Performance xu t s c - Tang position size")
                # fromhtang weight in SYMBOL_ALLOCATION
                
            elif performance_rating == "good":
                # Ginguyn c configuration
                print(f"   {symbol}: Performance t t - Ginguyn c configuration")
                
            elif performance_rating == "acceptable":
                # Gi m position size nh 
                print(f"    {symbol}: Performance ch p receive d - Gi m position size nh ")
                
            elif performance_rating == "poor":
                # Gi m position size used kv tang risk management
                print(f"   {symbol}: Poor performance - Reducing position size and increasing risk management")
                
                # from model th i gi m weight in SYMBOL_ALLOCATION
                if symbol in SYMBOL_ALLOCATION:
                    current_weight = SYMBOL_ALLOCATION[symbol]["weight"]
                    SYMBOL_ALLOCATION[symbol]["weight"] = current_weight * 0.5
                    print(f"    Reducing weight from {current_weight} to {SYMBOL_ALLOCATION[symbol]['weight']}")
                
        except Exception as e:
            print(f"Li adjustment strategy cho {symbol}: {e}")

    def auto_disable_poor_performers(self):
        """
        Enhanced performance monitoring with more lenient thresholds and better data handling
        """
        logger = logging.getLogger('PerformanceMonitor')
        try:
            logger.info("Starting performance check for all symbols")
            print(" [Performance Monitor] Checking performance of all symbols...")
            
            disabled_symbols = []
            for symbol in list(self.active_symbols):
                rating, data = self.evaluate_symbol_performance(symbol)
                
                # Enhanced data validation and metrics extraction
                if isinstance(data, dict):
                    win_rate = data.get('win_rate', 0)
                    total_trades = data.get('total_trades', 0)
                    profit_factor = data.get('profit_factor', 0)
                    max_drawdown = data.get('max_drawdown', 0)
                else:
                    win_rate = 0
                    total_trades = 0
                    profit_factor = 0
                    max_drawdown = 0
                
                # Log performance metrics for each symbol
                logger.info(f"{symbol} performance: Rating={rating}, WinRate={win_rate:.2%}, Trades={total_trades}, ProfitFactor={profit_factor:.2f}")
                
                # More lenient performance criteria - only disable if very poor performance AND sufficient trades
                should_disable = (
                    rating == "poor" and 
                    total_trades >= 5 and  # Need at least 5 trades to make a decision
                    win_rate < 0.15 and     # Win rate below 15%
                    profit_factor < 0.8    # Profit factor below 0.8
                )
                
                if should_disable:
                    logger.warning(f"Disabling {symbol} due to poor performance: WinRate={win_rate:.2%}, Trades={total_trades}")
                    print(f"   {symbol}: Poor performance - model temporarily disabled")
                    self.active_symbols.discard(symbol)
                    disabled_symbols.append(symbol)
                    
                    # Enhanced Discord message with better metrics
                    message = f"**SYMBOL TM THI TT**\n"
                    message += f"**Symbol:** {symbol}\n"
                    message += f"**Reason:** Poor performance\n"
                    
                    if isinstance(data, dict) and total_trades > 0:
                        message += f"**Metrics:** Win Rate: {win_rate:.2%}, "
                        message += f"Total Trades: {total_trades}, "
                        message += f"Profit Factor: {profit_factor:.2f}, "
                        message += f"Max Drawdown: {max_drawdown:.2%}\n"
                    else:
                        message += f"**Metrics:** Insufficient trade data (trades: {total_trades})\n"
                    
                    # Send enhanced Discord alert with performance data
                    performance_data = {
                        "Win Rate": f"{win_rate:.2%}",
                        "Total Trades": total_trades,
                        "Profit Factor": f"{profit_factor:.2f}",
                        "Max Drawdown": f"{max_drawdown:.2%}"
                    }
                    self.send_discord_alert(message, "WARNING", "HIGH", performance_data)
                elif rating == "poor" and total_trades < 5:
                    logger.info(f"Keeping {symbol} active despite poor rating - insufficient trades ({total_trades})")
                    print(f"   {symbol}: Poor performance but insufficient trades ({total_trades}) - keeping active")
            
            if disabled_symbols:
                print(f" [Performance Monitor]  t t {len(disabled_symbols)} symbols: {disabled_symbols}")
            else:
                print("[Performance Monitor] T t csymbols d u c performance ch p receive d")
                
        except Exception as e:
            print(f"Li in auto_disable_poor_performers: {e}")

    def check_and_adjust_performance(self):
        """
        Check v adjustment performance of t t csymbols
        """
        try:
            print(" [Performance Check] Starting performance check...")
            
            for symbol in self.active_symbols:
                rating, data = self.evaluate_symbol_performance(symbol)
                # Ensure data is a dictionary
                if isinstance(data, dict):
                    win_rate = data.get('win_rate', 0)
                else:
                    win_rate = 0
                print(f"    {symbol}: {rating.upper()} (Win Rate: {win_rate:.2%})")
                
                # Adjust strategy based on performance
                self.adjust_strategy_based_on_performance(symbol, rating)
            
            # Tused t t poor performers
            self.auto_disable_poor_performers()
            
        except Exception as e:
            print(f"Li in check_and_adjust_performance: {e}")

    def get_optimized_config(self, symbol: str) -> Dict[str, Any]:
        """L y c configuration t i uu cho symbol texperiment results"""
        if symbol in self.optimized_configs:
            config = self.optimized_configs[symbol]
            print(f" Using optimization config for {symbol}: {config.get('strategy_name', 'unknown')} + {config.get('model_name', 'unknown')}")
            return config
        else:
            print(f" No optimization config found for {symbol}, using default")
            return {}

    def load_or_train_models(self, force_retrain_symbols=None):
        """Load hoc train models vi configuration ti uu"""
        print("🤖 [Models] Loading/Training models with optimization configurations...")
        
        try:
            print(f"📊 [Models] Processing {len(SYMBOLS)} symbols: {SYMBOLS}")

            for symbol in SYMBOLS:
                print(f"🔍 [Models] Processing symbol: {symbol}")
                try:
                    optimized_config = self.get_optimized_config(symbol)
                    print(f" [Models] Got config for {symbol}: {bool(optimized_config)}")

                    if optimized_config:
                        # Use optimization configuration
                        strategy_name = optimized_config.get('strategy_name', 'ensemble_trending')
                        model_name = optimized_config.get('model_name', 'RandomForest')
                        hyperparams = optimized_config.get('hyperparameters', {})

                        print(f" [Models] Loading optimized model for {symbol}: {strategy_name} + {model_name}")

                        # Load model checkpoint if available
                        checkpoint_path = f"models/{symbol}/{symbol}_checkpoint.json"
                        if os.path.exists(checkpoint_path):
                            try:
                                with open(checkpoint_path, 'r') as f:
                                    checkpoint = json.load(f)
                                    print(f" [Models] Loaded checkpoint for {symbol}")
                            except Exception as e:
                                print(f"❌ [Models] Failed to load checkpoint for {symbol}: {e}")
                                checkpoint = None
                        else:
                            checkpoint = None

                        # Apply optimized hyperparameters
                        if strategy_name in ['trending', 'ensemble_trending']:
                            if symbol not in self.trending_models:
                                self.trending_models[symbol] = {}
                            self.trending_models[symbol]['optimized_config'] = optimized_config
                            print(f" [Models] Applied trending config for {symbol}")
                        elif strategy_name in ['ranging', 'ensemble_ranging']:
                            if symbol not in self.ranging_models:
                                self.ranging_models[symbol] = {}
                            self.ranging_models[symbol]['optimized_config'] = optimized_config
                            print(f" [Models] Applied ranging config for {symbol}")

                    else:
                        # Fallback to default configuration
                        print(f"⚠️ [Models] Using default configuration for {symbol}")
                        
                except Exception as e:
                    print(f" [Models] Error processing symbol {symbol}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            print(" [Models] Model loading/training completed successfully")
            
        except Exception as e:
            print(f" [Models] Critical error in load_or_train_models: {e}")
            import traceback
            traceback.print_exc()
            raise e

        print("Model loading/training completed")

    def calculate_position_size(self, symbol: str, confidence: float, atr_value: float) -> float:
        """Calculate position size with optimized risk parameters"""
        optimized_config = self.get_optimized_config(symbol)

        if optimized_config and 'risk_parameters' in optimized_config:
            risk_params = optimized_config['risk_parameters']

            # Use optimized risk parameters
            max_position_size = risk_params.get('max_position_size', 0.02)
            risk_per_trade = risk_params.get('risk_per_trade', 0.01)
            confidence_multiplier = risk_params.get('confidence_multiplier', 1.0)

            # Calculate position size with optimized parameters
            base_size = min(max_position_size, risk_per_trade / atr_value)
            adjusted_size = base_size * confidence * confidence_multiplier

            print(f" Using optimized position sizing for {symbol}: {adjusted_size:.4f}")
            return adjusted_size
        else:
            # Fallback to old logic
            base_size = min(0.02, 0.01 / atr_value)
            adjusted_size = base_size * confidence
            return adjusted_size

    def validate_optimized_performance(self, symbol: str) -> bool:
        """Validate performance of c configuration t i uu"""
        optimized_config = self.get_optimized_config(symbol)

        if not optimized_config:
            return True  # No config to validate

        # Check if performance metrics meet expectations
        expected_sharpe = optimized_config.get('expected_sharpe', 1.0)
        expected_max_dd = optimized_config.get('expected_max_dd', 0.25)

        # Get current performance (simplifiedheck)
        current_performance = self.get_current_performance(symbol)

        if current_performance:
            current_sharpe = current_performance.get('sharpe_ratio', 0)
            current_max_dd = current_performance.get('max_drawdown', 1.0)

            # Check if current performance is within acceptable range
            sharpe_ratio_ok = current_sharpe >= expected_sharpe * 0.8
            max_dd_ok = current_max_dd <= expected_max_dd * 1.2

            if sharpe_ratio_ok and max_dd_ok:
                print(f"Performance validation passed for {symbol}")
                return True
            else:
                print(f" Performance validation failed for {symbol}")
                return False

        return True  # Default to pass if no current performance data

    def get_current_performance(self, symbol: str) -> Dict[str, float]:
        """Get current performance metrics for a symbol"""
        # This ifix simplified implementation
        # In a real scenario, you would calculate actual performance metrics
        return {
            'sharpe_ratio': 1.2,
            'max_drawdown': 0.15,
            'total_return': 0.25,
            'win_rate': 0.65
        }

    def check_for_config_updates(self):
        """Check c configuration mi from experiments"""
        try:
            # Check if strategy_config.py has beeifpdated
            config_file = "strategy_config.py"
            if os.path.exists(config_file):
                file_mtime = os.path.getmtime(config_file)

                if self.last_config_update is None or file_mtime > self.last_config_update:
                    print(" Detectedonfig update, reloading...")

                    # Reload the configuration
                    import importlib
                    import sys

                    if 'strategy_config' in sys.modules:
                        importlib.reload(sys.modules['strategy_config'])

                    # Update internal configs
                    global STRATEGY_CONFIG_BY_SYMBOL, CONFIG_METADATA
                    from strategy_config import STRATEGY_CONFIG_BY_SYMBOL, CONFIG_METADATA

                    self.optimized_configs = STRATEGY_CONFIG_BY_SYMBOL
                    self.config_metadata = CONFIG_METADATA
                    self.last_config_update = file_mtime

                    print(f"Config reloaded: {len(self.optimized_configs)} symbols")

                    # Retrain models with new config
                    self.load_or_train_models()

        except Exception as e:
            print(f" Error checking for config updates: {e}")

    def apply_optimized_strategy_params(self, symbol: str, strategy_instance):
        """Apply optimized strategy parameters to strategy instance"""
        optimized_config = self.get_optimized_config(symbol)

        if optimized_config and 'strategy_parameters' in optimized_config:
            strategy_params = optimized_config['strategy_parameters']

            # Apply parameters to strategy instance
            for param_name, param_value in strategy_params.items():
                if hasattr(strategy_instance, param_name):
                    setattr(strategy_instance, param_name, param_value)
                    print(f" Applied optimized parameter {param_name}={param_value} for {symbol}")

    def get_optimized_model_params(self, symbol: str, model_type: str) -> Dict[str, Any]:
        """Getoptimized model parameters for a specific symbol and model type"""
        optimized_config = self.get_optimized_config(symbol)

        if optimized_config and 'hyperparameters' in optimized_config:
            hyperparams = optimized_config['hyperparameters']

            # Filter parameters based on model type
            model_params = {}
            for param_name, param_value in hyperparams.items():
                # This ifix simplified filtering - in practice you'd have more sophisticated logic
                if model_type.lower() in param_name.lower() or 'general' in param_name.lower():
                    model_params[param_name] = param_value

            return model_params

        return {}
    def calculate_optimal_entry_tpsl(self, symbol, signal, current_price, technical_data, market_data):
        """Calculate optimal entry, TP, and SL using the new calculator"""
        try:
            result = self.entry_tpsl_calculator.calculate_optimal_entry_tpsl(
                symbol, signal, current_price, technical_data, market_data
            )

            # Log the calculation
            logging.info(f" [{symbol}] Optimal Entry/TP/SL calculated:")
            logging.info(f"   Entry: {result['entry_price']:.5f}")
            logging.info(f"   SL: {result['stop_loss']:.5f}")
            logging.info(f"   TP: {result['take_profit']:.5f}")
            logging.info(f"   RR Ratio: {result['rr_ratio']:.2f}")
            logging.info(f"   Confidence: {result['confidence']:.2%}")

            return result
        except Exception as e:
            logging.error(f"Error calculating optimal Entry/TP/SL for {symbol}: {e}")
            # Fallback to simple calculation
            return self._fallback_entry_tpsl_calculation(symbol, signal, current_price, technical_data)

    def _fallback_entry_tpsl_calculation(self, symbol, signal, current_price, technical_data):
        """Fallback calculation if the advanced calculator fails"""
        atr = technical_data.get('atr', current_price * 0.01)

        if signal == "BUY":
            entry_price = current_price
            stop_loss = current_price - (atr * 2)
            take_profit = current_price + (atr * 3)
        else:  # SELL
            entry_price = current_price
            stop_loss = current_price + (atr * 2)
            take_profit = current_price - (atr * 3)

        risk_distance = abs(entry_price - stop_loss)
        reward_distance = abs(take_profit - entry_price)
        rr_ratio = reward_distance / risk_distance if risk_distance > 0 else 1.5

        return {
            "entry_price": entry_price,
            "stop_loss": stop_loss,
            "take_profit": take_profit,
            "risk_distance": risk_distance,
            "reward_distance": reward_distance,
            "rr_ratio": rr_ratio,
            "atr": atr,
            "confidence": 0.5  # Default confidence
        }
    def _filter_data_for_curriculum(self, df, difficulty='hard'):
        """
        Function to create curriculum Learning.
        'easy': Xu hu ng r, bi n used th p.
        'medium': TFunctioncsideways.
        'hard': Ton bdata.
        """
        if difficulty == 'hard':
            return df

        # Calculate ADX and ATR for filtering
        adx = ADXIndicator(df['high'], df['low'], df['close']).adx()
        atr_norm = (AverageTrueRange(df['high'], df['low'], df['close']).average_true_range() / df['close'])

        if difficulty == 'easy':
            # Ly 30% data c xu hung mnh nht (ADX cao) v 30% c bin dng thp nht (ATR thp)
            sin_trend_threshold = adx.quantile(0.7)
            low_volatility_threshold = atr_norm.quantile(0.3)

            filtered_df = df[(adx > sin_trend_threshold) & (atr_norm < low_volatility_threshold)]
            print(f"   [curriculum] Easy mode: L c d {len(filtered_df)} / {len(df)} m u.")
            return filtered_df if not filtered_df.empty else df # Fallback

        if difficulty == 'medium':
            # L y 70% data ofDX th p (bao g levelsideways v trendnh )
            medium_trend_threshold = adx.quantile(0.7)
            filtered_df = df[adx < medium_trend_threshold]
            print(f"   [curriculum] Medium mode: L c d {len(filtered_df)} / {len(df)} m u.")
            return filtered_df if not filtered_df.empty else df # Fallback

        return df
    def check_api_connection(self):
            """Check connection to OANDA API."""
            try:
                response = requests.get(
                    f"{OANDA_URL}/accounts",
                    headers={"Authorization": f"Bearer {OANDA_API_KEY}"},
                    timeout=10,
                )
                if response.status_code == 200:
                    print("OANDA API connection successful.")
                    return True
                else:
                    print(f"Kt ni OANDA API th t b i. Status: {response.status_code}, Response: {response.text}")
                    return False
            except requests.exceptions.RequestException as e:
                print(f"Li k t n i OANDA API: {e}")
                return False
    # <<< TFunction Function M I this VO in L P EnhancedTradingBot >>>
    def _objective_rl_portfolio(self, trial, train_env, val_env):
        """
        Function m fromiu Optuna cho PortfolioEnvironment with tham st i uu hon.
        """
        params = {
            # T i uu ha n_steps cho hi u su t t t hon
            'n_steps': trial.suggest_categorical('n_steps', [2048, 4096, 8192]),

            # Gamma cao hon cho long-term planning
            'gamma': trial.suggest_float('gamma', 0.995, 0.9995),

            # Learning rate range dufromi uu cho PPO
            'learning_rate': trial.suggest_float('learning_rate', 3e-5, 3e-4, log=True),

            # Entropy coefficient dufromi uu cho exploration
            'ent_coef': trial.suggest_float('ent_coef', 0.0001, 0.01, log=True),

            # Clip range dufromi uu cho stability
            'clip_range': trial.suggest_float('clip_range', 0.1, 0.3),

            # Value function coefficient dufromi uu
            'vf_coef': trial.suggest_float('vf_coef', 0.25, 1.0),

            # Gradient clipping dufromi uu
            'max_grad_norm': trial.suggest_float('max_grad_norm', 0.3, 1.0),

            # GAE lambda dufromi uu cho bias-variance tradeoff
            'gae_lambda': trial.suggest_float('gae_lambda', 0.9, 0.99),

            # Training epochs dufromi uu
            'n_epochs': trial.suggest_int('n_epochs', 5, 15),

            # Batch size dufromi uu cho memory efficiency
            'batch_size': trial.suggest_categorical('batch_size', [128, 256, 512])
        }

        try:
            # Enhanced error handling for RL training
            logging.info(f"[RL Training] Starting training with parameters: {params}")

            # C i money policy architecture with advancednetworks
            policy_kwargs = dict(
                net_arch=dict(
                    pi=[512, 512, 256, 128],  # Deeper policy network
                    vf=[512, 512, 256, 128]   # Deeper value function network
                ),
                activation_fn=torch.nn.ReLU,  # ReLU activation
                ortho_init=False,            # Disable orthogonal initialization
                log_std_init=0.0,            # Initialize log_std
                squash_output=False,          # No output squashing
                share_features_extractor=False  # Separate feature extractors
            )

            model = PPO("MlpPolicy", train_env,
                       policy_kwargs=policy_kwargs,
                       verbose=0,
                       tensorboard_log=f"./ppo_tensorboard_{PRIMARY_TIMEFRAME.lower()}/",
                       **params)

            # Tang total timesteps dh from t hon with early stopping
            model.learn(total_timesteps=50000, progress_bar=False)

        except Exception as e:
            logging.error(f"[RL Training] Training failed: {e}")
            print(f"   [Optuna Portfolio] Trial failed. Error: {e}")
            return -1e9

        # Enhanced evaluation with multiple episodes
        total_rewards = []
        for episode in range(3):  # Evaluate on 3 episodes for stability
            obs, _ = val_env.reset()
            terminated, truncated = False, False
            episode_reward = 0

            while not (terminated or truncated):
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, _ = val_env.step(action)
                episode_reward += reward

            total_rewards.append(episode_reward)

        # Return average reward across episodes
        avg_reward = np.mean(total_rewards)
        print(f"   [Optuna Portfolio] Trial completed. Average reward: {avg_reward:.2f}")
        return avg_reward
    # EnhancedTradingBofrom modelethods

    def load_or_train_models(self, force_retrain_symbols=None):
        if force_retrain_symbols is None:
            force_retrain_symbols = []

        # <<< TreceiveANG M I: Check symbols needs retrain do stale data >>>
        stale_symbols = self.data_manager.get_stale_symbols()
        if stale_symbols:
            print(f"🔄 [Auto-Retrain] Detected {len(stale_symbols)} symbols need retraining due to stale data: {stale_symbols}")
            force_retrain_symbols.extend(stale_symbols)
            force_retrain_symbols = list(set(force_retrain_symbols))  # Remove duplicates

        if force_retrain_symbols:
            print(f"🔄 Starting forced retraining cycle for: {force_retrain_symbols}")
            self.data_manager.clear_cache()

        self.active_symbols.clear()
        print("Starting to build/update active symbol list...")
        full_data_cache = {}

        def _check_quality_gates(model_data, symbol, model_type):
            if not model_data:
                print(f"  ⚠️ Model {model_type} for {symbol}: No model_data")
                return False

            f1, std_f1, accuracy = 0, 1.0, 0

            # Check metadata before (from file saved)
            metadata_f1 = model_data.get("cv_mean_f1")
            if metadata_f1 is not None:
                f1 = metadata_f1
                std_f1 = model_data.get("cv_std_f1") or 1.0
                accuracy = model_data.get("cv_mean_accuracy") or 0.0
                logging.debug(f"Model {model_type} cho {symbol}: Using metadata (F1:{f1:.3f}, STD:{std_f1:.3f}, Accuracy:{accuracy:.3f})")
            elif "ensemble" in model_data and hasattr(model_data["ensemble"], 'cv_results') and model_data["ensemble"].cv_results:
                try:
                    key = next(iter(model_data['ensemble'].cv_results))
                    res = model_data['ensemble'].cv_results[key]
                    logging.debug(f"cv_results keys cho {symbol}: {list(res.keys())}")
                    f1 = res['mean_f1']
                    std_f1 = res['std_f1']
                    accuracy = res.get('mean_accuracy', res.get('accuracy', 0.5))  # Fallback if not c mean_accuracy
                    logging.debug(f"Model {model_type} cho {symbol}: Using cv_results (F1:{f1:.3f}, STD:{std_f1:.3f}, Accuracy:{accuracy:.3f})")
                except (KeyError, IndexError, StopIteration) as e:
                    print(f"  Model {model_type} cho {symbol}: Li truy c p cv_results: {e}")
                    return False
            else:
                print(f"  ⚠️ Model {model_type} for {symbol}: No valid metadata or cv_results")
                return False

            # Check quality gates
            gates_passed = f1 >= MIN_F1_SCORE_GATE and std_f1 <= MAX_STD_F1_GATE and accuracy >= MIN_ACCURACY_GATE
            logging.debug(f"Quality gates cho {symbol} {model_type}: F1={MIN_F1_SCORE_GATE}({f1:.3f}), STD={MAX_STD_F1_GATE}({std_f1:.3f}), Accuracy={MIN_ACCURACY_GATE}({accuracy:.3f})")

            if gates_passed:
                print(f"  {model_type} model for {symbol} SUCCESSFUL!")
                return True
            else:
                print(f"  Model {model_type} cho {symbol} khng dt chun (F1:{f1:.3f}, STD:{std_f1:.3f}, Accuracy:{accuracy:.3f}).")
                return False

        def _train_and_evaluate(symbol_to_train, df_filtered, model_type):
            best_model_data, best_f1 = None, -1
            if df_filtered is None or len(df_filtered) < MIN_SAMPLES_GATE:
                print(f"  not ddata ({len(df_filtered) if df_filtered is not None else 0}/{MIN_SAMPLES_GATE} m u) cho model {model_type}.")
                return None

            logging.debug(f"Starting training {model_type} for {symbol_to_train} with {len(df_filtered)} samples")

            for attempt in range(MAX_RETRAIN_ATTEMPTS):
                print(f"   -> ang training model {model_type} cho {symbol_to_train} l n {attempt + 1}/{MAX_RETRAIN_ATTEMPTS}...")
                model_data_new = self.train_enhanced_model(symbol_to_train, df_filtered)

                if model_data_new and "ensemble" in model_data_new:
                    try:
                        # Check ensemble c cv_results not
                        if hasattr(model_data_new['ensemble'], 'cv_results') and model_data_new['ensemble'].cv_results:
                            key = next(iter(model_data_new['ensemble'].cv_results))
                            cv_data = model_data_new['ensemble'].cv_results[key]
                            logging.debug(f"cv_results keys cho {symbol_to_train}: {list(cv_data.keys())}")
                            f1 = cv_data['mean_f1']
                            std_f1 = cv_data['std_f1']
                            accuracy = cv_data.get('mean_accuracy', cv_data.get('accuracy', 0.5))

                            print(f"   -> Khng dt cht lung {attempt + 1}: F1-Score = {f1:.3f}, STD = {std_f1:.3f}, Accuracy = {accuracy:.3f}")

                            if f1 > best_f1:
                                best_f1, best_model_data = f1, model_data_new
                                logging.debug(f"Updated best model with F1 = {f1:.3f}")

                            if best_f1 >= MIN_F1_SCORE_GATE:
                                print("     t F1-Score m fromiu! used s m.")
                                break
                        else:
                            print(f"    Model {model_type} attempt {attempt + 1}: No validv_results")
                            continue
                    except (KeyError, IndexError, StopIteration) as e:
                        print(f"   Model {model_type} l n {attempt + 1}: Li truy c p cv_results: {e}")
                        continue
                else:
                    print(f"   ⚠️ Model {model_type} attempt {attempt + 1}: No ensemble in model_data")
                    continue

            if best_model_data:
                logging.debug(f"Hon thnh training {model_type} cho {symbol_to_train}, best F1 = {best_f1:.3f}")
            else:
                print(f"  ❌ Cannot train model {model_type} for {symbol_to_train} after {MAX_RETRAIN_ATTEMPTS} attempts")

            return best_model_data

        # --- Vng l p processing model Ensemble (Trending/Ranging) ---
            # --- Vng l p processing model Ensemble (Trending/Ranging) ---
        for symbol in SYMBOLS:
            print("-" * 40)
            print(f"Processing models for symbol: {symbol}")

            # <<< BU C 3: TCH H P Function Check GIGIAO dH >>>
            if not is_market_open(symbol):
                # Debug: Log th i gian v ngy dhi u t i sao thtru ng used
                now_utc = datetime.now(pytz.UTC)
                now_local = datetime.now()
                weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
                print(f"   [Market Status] Market for {symbol} is closed. Skipping.")
                print(f"   [Market Debug] UTC: {now_utc.strftime('%Y-%m-%d %H:%M:%S')} ({weekday_names[now_utc.weekday()]})")
                print(f"   [Market Debug] Local: {now_local.strftime('%Y-%m-%d %H:%M:%S')} ({weekday_names[now_local.weekday()]})")
                print(f"   [Market Debug] is_weekend(): {is_weekend()}, is_crypto_symbol({symbol}): {is_crypto_symbol(symbol)}")
                continue # Chuyn sang symbol tip theo
            # <<< K T THfromCH H P >>>

            # (All remaining logic of environment data for training model original)
            is_forced = symbol in force_retrain_symbols
            is_symbol_active = False
            df_full_for_symbol = self.data_manager.create_enhanced_features(symbol)

            if df_full_for_symbol is None:
                # create_enhanced_features d log l i (v d : do data old)
                continue

            full_data_cache[symbol] = df_full_for_symbol

            # processing model Trending
            model_trending = load_latest_model(symbol, "ensemble_trending")
            if is_forced or not model_trending:
                df_trending = df_full_for_symbol[df_full_for_symbol['market_regime'] != 0].copy()
                best_trending_model = _train_and_evaluate(symbol, df_trending, "TRENDING")
                if _check_quality_gates(best_trending_model, symbol, "TRENDING"):
                    self.trending_models[symbol] = best_trending_model
                    save_model_with_metadata(symbol, best_trending_model, "ensemble_trending")
                    is_symbol_active = True
            elif _check_quality_gates(model_trending, symbol, "TRENDING"):
                self.trending_models[symbol] = model_trending
                is_symbol_active = True

            # processing model Ranging
            model_ranging = load_latest_model(symbol, "ensemble_ranging")
            if is_forced or not model_ranging:
                df_ranging = df_full_for_symbol[df_full_for_symbol['market_regime'] == 0].copy()
                logging.debug(f"Starting training RANGING for {symbol} with {len(df_ranging)} samples")
                best_ranging_model = _train_and_evaluate(symbol, df_ranging, "RANGING")
                if _check_quality_gates(best_ranging_model, symbol, "RANGING"):
                    self.ranging_models[symbol] = best_ranging_model
                    save_model_with_metadata(symbol, best_ranging_model, "ensemble_ranging")
                    is_symbol_active = True
                    print(f"  RANGING model cho {symbol} has d luu v activate")
                else:
                    print(f"  RANGING model cho {symbol} not d t quality gates")
            elif _check_quality_gates(model_ranging, symbol, "RANGING"):
                self.ranging_models[symbol] = model_ranging
                is_symbol_active = True
                print(f"   RANGING model for {symbol} is available and meets standards")
            else:
                print(f"  ⚠️ RANGING model for {symbol} is available but doesn't meet standards")

            # Debug thng tin chi ti t
            logging.debug(f"K t quCheck cho {symbol}:")
            print(f"   - is_symbol_active: {is_symbol_active}")
            print(f"   - trending_model c: {symbol in self.trending_models}")
            print(f"   - ranging_model c: {symbol in self.ranging_models}")

            if is_symbol_active:
                self.active_symbols.add(symbol)
                print(f"  Symbol {symbol} has been activated.")
            else:
                self.trending_models.pop(symbol, None)
                self.ranging_models.pop(symbol, None)
                print(f"  Symbol {symbol} BV HI U HA do not c model no d t chu n.")



        # --- Vng l p processing RL Agent ---
        if self.use_rl:
            rl_model_path = os.path.join(MODEL_DIR, "rl_portfolio_agent.zip")
            force_retrain_rl = any(s in self.active_symbols for s in force_retrain_symbols)

            print(f" [RL Agent] Check conditions training RL Agent...")
            print(f"   - use_rl: {self.use_rl}")
            print(f"   - active_symbols: {len(self.active_symbols)} symbols")
            print(f"   - force_retrain_rl: {force_retrain_rl}")
            print(f"   - rl_model_path exists: {os.path.exists(rl_model_path)}")

            if os.path.exists(rl_model_path) and not force_retrain_rl:
                print(f" Found Portfolio RL Agent file, loading and initializing environment...")
                self.portfolio_rl_agent = RLAgent(model_path=rl_model_path)
                # (Logic Check compatibility environmentoriginal)
            else:
                # <<< B T  U KH I LOGIC training RL  NNG C P >>>
                print("-" * 40)
                print(" Bt du qu trnh training Portfolio RL Agent...")

                dict_dfs, dict_feature_cols = {}, {}
                min_data_length = float('inf')
                active_symbols_for_rl = list(self.active_symbols)

                print(f" [RL Agent] Active symbols for RL: {active_symbols_for_rl}")

                if len(active_symbols_for_rl) < 2:
                    print("not dsymbol ho t used (>1) dtraining RL Agent.")
                    print(f"   - needs t nh t 2 symbols, hi n c: {len(active_symbols_for_rl)}")
                    print("   - Sthtraining with t t csymbols c model (not chactive)")

                    # Thwith t t csymbols c model
                    all_symbols_with_models = set()
                    for symbol in SYMBOLS:
                        if symbol in self.trending_models or symbol in self.ranging_models:
                            all_symbols_with_models.add(symbol)

                    print(f"   - Symbols c model: {list(all_symbols_with_models)}")

                    if len(all_symbols_with_models) >= 2:
                        print("   - Using t t csymbols c model dtraining RL Agent")
                        active_symbols_for_rl = list(all_symbols_with_models)
                    else:
                        print("No symbols with models for training RL Agent. Skipping.")
                        self.portfolio_rl_agent = None
                        return

                # Chu n bdata cho RL
                for symbol in active_symbols_for_rl:
                    model_ref_data = self.trending_models.get(symbol) or self.ranging_models.get(symbol)
                    if not model_ref_data: continue
                    df_full = full_data_cache.get(symbol)
                    if df_full is None or df_full.empty: continue
                    dict_dfs[symbol] = df_full
                    dict_feature_cols[symbol] = model_ref_data['feature_columns'] + ['confidence'] # TFunctionconfidence vo features
                    min_data_length = min(min_data_length, len(df_full))

                if min_data_length < 500: # needs ddata dchia
                     print(f"⚠️ Shared data too short ({min_data_length} candles) for RL training. Skipping.")
                     self.portfolio_rl_agent = None
                     return

                # from ng n t t cdataframe vcng ddi
                for symbol in active_symbols_for_rl:
                    dict_dfs[symbol] = dict_dfs[symbol].tail(min_data_length)

                # --- LOGIC SPLIT data AN TON 60-20-20 ---
                print(f"   [RL Training] Phn chia {min_data_length} n n: 60% Train, 20% Validation (cho Optuna), 20% Final Test...")
                train_end_idx = int(min_data_length * 0.60)
                val_end_idx = int(min_data_length * 0.80)

                dict_dfs_train = {s: df.iloc[:train_end_idx] for s, df in dict_dfs.items()}
                dict_dfs_val = {s: df.iloc[train_end_idx:val_end_idx] for s, df in dict_dfs.items()}
                dict_dfs_test = {s: df.iloc[val_end_idx:] for s, df in dict_dfs.items()} # T p this Optuna not th y
                # --- K T THC LOGIC SPLIT ---

                # T o mi trung cho Optuna v testcu cng
                train_env = PortfolioEnvironment(dict_dfs_train, dict_feature_cols, active_symbols_for_rl)
                val_env = PortfolioEnvironment(dict_dfs_val, dict_feature_cols, active_symbols_for_rl)
                test_env = PortfolioEnvironment(dict_dfs_test, dict_feature_cols, active_symbols_for_rl)

                print("\n--- Starting RL parameter optimization with Optuna (with money saving) ---")
                
                # Using OptunaStuducManager cho RL optimization
                storage_url = OPTUNA_CONFIG.get("STORAGE_URL", "sqlite:///optuna_study.db")
                study_name = f"{OPTUNA_CONFIG.get('STUDY_NAME', 'trading_bot')}_rl_portfolio"
                
                study_manager = OptunaStudyManager(storage_url)
                study = study_manager.create_or_load_study(
                    study_name=study_name,
                    direction="maximize"
                )
                
                study.optimize(lambda trial: self._objective_rl_portfolio(trial, train_env, val_env), n_trials=100)
                best_params = study.best_params
                print(f"   [Optuna] Optimal parameters found: {best_params}")

                # training agentcu cng trn 80% data
                print("\n--- Starting RL Agent training with optimal parameters ---")
                full_train_data = {s: df.iloc[:val_end_idx] for s, df in dict_dfs.items()}
                full_train_env = PortfolioEnvironment(full_train_data, dict_feature_cols, active_symbols_for_rl)

                final_agent = RLAgent()
                final_model = PPO("MlpPolicy", full_train_env, verbose=0, **best_params, tensorboard_log="./portfolio_ppo_tensorboard/")
                final_agent.model = final_model
                final_agent.train(full_train_env, total_timesteps=30000, save_path=rl_model_path)
                self.portfolio_rl_agent = final_agent
                print("  Portfolio RL Agent has d training!")

                # nh gi cu cng trn t p test not yet total th y
                print("\n---  nh gi cu cng trn t p Test Set (unseen data) ---")
                obs, _ = test_env.reset()
                terminated, truncated = False, False
                while not (terminated or truncated):
                    action, _ = final_agent.model.predict(obs, deterministic=True)
                    obs, _, terminated, truncated, _ = test_env.step(action)

                final_balance_test = test_env.balance
                performance_test = (final_balance_test / test_env.initial_balance - 1) * 100
                logging.info(f"   [Final Test] Li nhu n trn t p test: {performance_test:.2f}%")
                self.send_discord_alert(f"🎯 **New RL Training Results** 🎯\n- Profit on test set (unseen data): **{performance_test:.2f}%**")
                # <<< K T THC KH I LOGIC training RL >>>
    def train_enhanced_model(self, symbol, df_to_train): # <--- TFunction df_to_train
        """This function now takefix pre-filtered DataFrame."""
        print(f"Starting model training for {symbol}...")
        df = df_to_train # <--- Using DataFrame dufromruy n vo

        if df is None or df.empty:
            print(f"⚠️ No data available for training {symbol}.")
            return None
        # --- BU C C I money: TFunction FEATURE KINH TV NEWS SENTIMENT ---
        # Luu : Vi fromFunction sentiment l ch syu c u ngu n data trph.
        # duc chng ta t p trung vo data l ch kinh tc s n.
        df = self.news_manager.add_economic_event_features(df, symbol)
        
        # --- PHASE 2: TFunction NEWS SENTIMENT FEATURES (needs RETRAIN) ---
        df = self.news_manager.add_news_sentiment_features(df, symbol)
        # --- K T THC BU C C I money ---

        # T o label duc, sau khi d dfeatures
        for horizon in [1, 3, 5]:
            df[f"label_{horizon}"] = (df["close"].shift(-horizon) > df["close"]).astype(
                int
            )

        # Pipeline clean data original as old
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.dropna(subset=["label_3"], inplace=True)

        feature_cols = [col for col in df.columns if not col.startswith("label")]
        X = df[feature_cols].copy()
        y = df["label_3"].copy()

        X.fillna(method="ffill", inplace=True)
        X.fillna(method="bfill", inplace=True)

        cols_to_drop = X.columns[X.isna().all()].tolist()
        if cols_to_drop:
            X.drop(columns=cols_to_drop, inplace=True)
            feature_cols = [col for col in feature_cols if col not in cols_to_drop]

        y = y.reindex(X.index)
        all_data = pd.concat([X, y], axis=1)
        all_data.dropna(inplace=True)
        X = all_data[feature_cols]
        y = all_data["label_3"]

        # Enhanced minimum samples check
        min_samples = ML_CONFIG.get("MIN_SAMPLES_FOR_TRAINING", 300)
        if len(X) < min_samples:
            print(
                f"not ddata s ch cho {symbol}: chc {len(X)} records (yu c u {min_samples})."
            )
            return None

        # ... (code type btuong quan, ch n feature, training ensemble)
        print(
            f" l+m sߦch data, cn l i {len(X)} records dtraining cho {symbol}."
        )

        # Enhancedorrelation removal with stricter threshold
        correlation_threshold = ML_CONFIG.get("MAX_CORRELATION_THRESHOLD", 0.90)

        # processing all from not must numerifromruc khi tnh correlation
        numeric_columns = X.select_dtypes(include=[np.number]).columns
        X_numeric = X[numeric_columns]

        # Check v processing all from categorical
        categorical_columns = X.select_dtypes(include=['object', 'category']).columns
        if len(categorical_columns) > 0:
            print(f"   [Warning] Detected {len(categorical_columns)} categorical columns: {list(categorical_columns)}")
            print(f"   [Info] Only using {len(numeric_columns)} numeric columns for correlation calculation")

        # Chtnh correlation trn all from numeric
        if len(X_numeric.columns) > 1:
            correlation_matrix = X_numeric.corr().abs()
            upper_triangle = correlation_matrix.where(
                np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
            )
            high_corr_features = [
                column
                for column in upper_triangle.columns
                if any(upper_triangle[column] > correlation_threshold)
            ]

            # type ball features c correlation cao tX_numeric
            X_numeric = X_numeric.drop(columns=high_corr_features)

            # Update X with all filtered numeric features
            X = pd.concat([X_numeric, X[categorical_columns]], axis=1)
        else:
            print(f"   [Warning] not dfrom numerihas datatnh correlation cho {symbol}")
            X = X_numeric if len(X_numeric.columns) > 0 else X

        # Enhanced feature selection with multiple algoritFunctions
        # ChUsing all from numeric cho feature selection
        X_for_selection = X.select_dtypes(include=[np.number])

        if len(X_for_selection.columns) == 0:
            print(f"⚠️ No numeric columns available for feature selection for {symbol}")
            return None

        rf_temp = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            random_state=42,
            n_jobs=-1
        )
        rf_temp.fit(X_for_selection, y)

        # Get feature importance from Random Forest
        rf_importance = pd.DataFrame({
            "feature": X_for_selection.columns,
            "rf_importance": rf_temp.feature_importances_
        })

        # Also get feature importance from XGBoost for comparison
        try:
            xgb_temp = xgb.XGBClassifier(
                n_estimators=50,
                max_depth=6,
                random_state=42,
                n_jobs=-1
            )
            xgb_temp.fit(X_for_selection, y)
            xgb_importance = pd.DataFrame({
                "feature": X_for_selection.columns,
                "xgb_importance": xgb_temp.feature_importances_
            })

            # Combine importances
            feature_importance = rf_importance.merge(xgb_importance, on="feature")
            feature_importance["combined_importance"] = (
                feature_importance["rf_importance"] * 0.6 +
                feature_importance["xgb_importance"] * 0.4
            )
        except:
            feature_importance = rf_importance
            feature_importance["combined_importance"] = feature_importance["rf_importance"]

        # Filter by minimumimportance threshold
        min_importance = ML_CONFIG.get("FEATURE_IMPORTANCE_THRESHOLD", 0.01)
        feature_importance = feature_importance[
            feature_importance["combined_importance"] >= min_importance
        ]

        # Select top features
        top_features = feature_importance.nlargest(
            ML_CONFIG["FEATURE_SELECTION_TOP_K"],
            "combined_importance"
        )["feature"].tolist()

        # TFunction encoded features vo top_features (categorical g c has d type b )
        encoded_cols = [col for col in X.columns if col.endswith('_encoded') or
                       col.startswith('asset_class_') or col.startswith('volatility_profile_') or
                       col.startswith('volatility_regime_') or col.startswith('prefers_') or
                       col.startswith('asset_') or col.endswith('_interaction')]

        # TFunction encoded features
        top_features.extend(encoded_cols)

        # Check if v n cn categorical columns (not nn c)
        remaining_categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
        if len(remaining_categorical_cols) > 0:
            print(f"   [Warning] V n cn {len(remaining_categorical_cols)} from categorical: {list(remaining_categorical_cols)}")
            top_features.extend(remaining_categorical_cols)

        # type bduplicates v d m b o features t n t i
        top_features = list(dict.fromkeys([f for f in top_features if f in X.columns]))

        X_selected = X[top_features]

        logging.info(f" Training with {len(top_features)} best features for {symbol}")
        ensemble = EnsembleModel()
        ensemble.train_ensemble(X_selected, y)  # Function this sdnng c p bu c 2
        print("   [Drift] Khởi tạo DriftMonitor with data thalevelhi u mới...")
        self.drift_monitor = DriftMonitor(X_selected)
        model_data = {"ensemble": ensemble, "feature_columns": top_features}
        return model_data

    # This ifix helper function, no changes needed
    def get_enhanced_signal(self, symbol, for_open_position_check=False, df_features=None):
        """
        Lấy tín hiệu và độ tin cậy from model Ensemble suitable with tempty thi thường.
        REFACTORED: receive df_features tcache dtrnh fetch data nhiều lần.
        """
        try:
            # --- BU C 1: L Y data V XC  NH Tempty THI THTRU NG ---
            if df_features is None:
                # Fallback: fetch data if not c cache
                df_features = self.data_manager.create_enhanced_features(symbol)
                if df_features is None or len(df_features) < 100:
                    logging.warning(f"get_enhanced_signal: not ddata danalysis {symbol}")
                    return None, 0.0, None
            else:
                # Using data tcache
                if len(df_features) < 100:
                    logging.warning(f"get_enhanced_signal: data tcache not ddanalysis {symbol}")
                    return None, 0.0, None

            current_regime = df_features['market_regime'].iloc[-1]

            # --- BU C 2: CH N NG MODEL D A TRN Tempty THI ---
            if current_regime != 0:  # Th trung c xu hung
                model_data = self.trending_models.get(symbol)
            else:  # Th trung di ngang
                model_data = self.ranging_models.get(symbol)

            # --- BU C 3: Check MODEL V TI P T C LOGIC ---
            if model_data is None:
                logging.warning(f"get_enhanced_signal: No suitable model found for {symbol} (Regime: {current_regime})")
                return None, 0.0, None

            model = model_data.get("ensemble")
            feature_columns = model_data.get("feature_columns")

            if not model or not feature_columns:
                logging.warning(f"get_enhanced_signal: Model Or feature_columns not hợp lệ cho {symbol}")
                return None, 0.0, None

            # Pipeline clean data more (original logic old of b n)
            df_features.replace([np.inf, -np.inf], np.nan, inplace=True)
            df_features.fillna(method="ffill", inplace=True)
            df_features.fillna(method="bfill", inplace=True)
            cols_to_drop = df_features.columns[df_features.isna().all()].tolist()
            if cols_to_drop:
                df_features.drop(columns=cols_to_drop, inplace=True)
            df_features.dropna(inplace=True)

            if len(df_features) < 10:
                logging.warning(f"get_enhanced_signal: data dufromrung qu t sau khi l+m sߦch {symbol} ({len(df_features)} used)")
                return None, 0.0, None

            #  m b o all from features of model must t n t i in df_features
            available_columns = [col for col in feature_columns if col in df_features.columns]
            X = df_features[available_columns]

            # KH C PH C L I:  m b o feature ordering nh t qun
            X_ordered = pd.DataFrame(index=X.index)
            for col in feature_columns:
                if col in X.columns:
                    X_ordered[col] = X[col]
                else:
                    X_ordered[col] = 0.0
                    logging.warning(f"get_enhanced_signal: Missing feature '{col}' for {symbol}, using default value 0.0")

            #  m b o not needsaN values
            X_ordered = X_ordered.fillna(0.0)

            # Equal with model
            try:
                # Check if model l EnsembleModel
                if hasattr(model, 'predict_proba') and hasattr(model, 'models'):
                    # EnsembleModel chreceive 1 tham s 
                    prob_buy = model.predict_proba(X_ordered)
                else:
                    # Model thng thu ng
                    prob_buy = model.predict_proba(X_ordered)
                
                if prob_buy is None:
                    logging.warning(f"get_enhanced_signal: Model trߦ v+ None cho {symbol}")
                    return None, 0.0, None
                
                # processing k t quEqual
                if isinstance(prob_buy, np.ndarray):
                    if len(prob_buy.shape) > 1:
                        prob_buy = prob_buy[0, 1] if prob_buy.shape[1] > 1 else prob_buy[0, 0]
                    else:
                        prob_buy = prob_buy[0]
                else:
                    prob_buy = float(prob_buy)
                
                # Apply confidence smoothing
                prob_buy_smoothed = np.clip(prob_buy, 0.05, 0.95)
                
                # Add randomness to avoid confidence clustering
                import random
                random_offset = random.uniform(-0.05, 0.05)
                prob_buy_smoothed = prob_buy_smoothed + random_offset
                prob_buy_smoothed = np.clip(prob_buy_smoothed, 0.05, 0.95)
                
                # T o signal d a trn confidence
                if prob_buy_smoothed > 0.6:
                    signal = "BUY"
                    confidence = prob_buy_smoothed
                elif prob_buy_smoothed < 0.4:
                    signal = "SELL"
                    confidence = 1.0 - prob_buy_smoothed
                else:
                    signal = "HOLD"
                    confidence = 0.5
                
                logging.info(f"get_enhanced_signal: {symbol} - Signal: {signal}, Confidence: {confidence:.3f}, Raw: {prob_buy:.3f}")
                
                if for_open_position_check:
                    return None, float(confidence), None
                
                return signal, float(confidence), prob_buy
                
            except Exception as pred_error:
                logging.error(f"get_enhanced_signal: Prediction error for {symbol}: {pred_error}")
                return None, 0.0, None

        except Exception as e:
            logging.error(f"get_enhanced_signal: Li nghim trng cho {symbol} - {e}")
            return None, 0.0, None

    def _decode_actions_to_vector(self, action, symbols_in_env, model):
        """
        Gi i m hnh used from model RL thnh m t vector.
        0=HOLD, 1=BUY, 2=SELL.
        """
        import numpy as np
        action_space = model.action_space

        # Chuy n d i action sang m ng numpy dprocessing
        action_array = np.asarray(action).flatten()

        #  m b o vector trvlun has datadi equal slu ng symbol
        # v all gi trn min kho ng [0, 1, 2]
        valid_actions = [int(a) if a in [0, 1, 2] else 0 for a in action_array]

        # if vector trvng n hon ssymbol (tru ng h p hi m), di n ph n cn l i equal 0 (HOLD)
        if len(valid_actions) < len(symbols_in_env):
            valid_actions.extend([0] * (len(symbols_in_env) - len(valid_actions)))

        return valid_actions[:len(symbols_in_env)]

    # EnhancedTradingBofrom modelethods

    # EnhancedTradingBofrom modelethods

    async def _run_ensemble_strategy(self, live_data_cache=None):
        """
        Run Ensemble strategy on entire portfolio.
        REFACTORED: Accuracyept live_data_cache to avoid multiple data fetches.
        """
        print("--- [Portfolio Ensemble Strategy] Starting analysis for entire portfolio ---")
        
        try:
            # Using cached data if c, Hofrom o mi
            if live_data_cache is None:
                live_data_cache = {}
                
            # Fetch data m t l n cho t t cactive symbols (OPTIMIZED)
            print(f"   [Data Cache] Fetching data for {len(self.active_symbols)} symbols...")
            print(f"   [Data Cache] Active symbols: {list(self.active_symbols)}")
            
            # T i uu ha: Fetch song song thay v tu n t 
            async def fetch_symbol_data(symbol):
                try:
                    df_features = self.data_manager.create_enhanced_features(symbol)
                    if df_features is not None and len(df_features) >= 100:
                        print(f"   [Data Cache] {symbol}: {len(df_features)} candles")
                        return symbol, df_features
                    else:
                        print(f"   [Data Cache]  {symbol}: data khng d ({len(df_features) if df_features is not None else 0} candles)")
                        return symbol, None
                except Exception as e:
                    print(f"   [Data Cache] {symbol}: Li fetch data - {e}")
                    return symbol, None
            
            # Fetch song song t t csymbols
            tasks = [fetch_symbol_data(symbol) for symbol in self.active_symbols]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    print(f"   [Data Cache]  Exception in fetch: {result}")
                    continue
                symbol, df_features = result
                if df_features is not None:
                    live_data_cache[symbol] = df_features
            else:
                print(f"   [Data Cache] Using cached data cho {len(live_data_cache)} symbols")
            
            if not live_data_cache:
                print("   [Ensemble Strategy] ⚠️ No valid data available for analysis")
                return
            
            # analysis total symbol with data dache
            print(f"   [Ensemble Strategy] Starting analysis for {len(live_data_cache)} symbols...")
            print(f"   [Ensemble Strategy] Symbols with data: {list(live_data_cache.keys())}")
            print(f"   [Ensemble Strategy] Active symbols: {list(self.active_symbols)}")
            
            for symbol, df_features in live_data_cache.items():
                try:
                    # L y T+n hi+u tEnsemble model
                    signal, confidence, proba_array = self.get_enhanced_signal(symbol, df_features=df_features)
                    
                    if signal and confidence > ML_CONFIG["MIN_CONFIDENCE_TRADE"]:
                        print(f"   [Ensemble Strategy]  {symbol}: {signal} (Confidence: {confidence:.2%})")
                        await self.handle_position_logic(symbol, signal, confidence)
                    else:
                        print(f"   [Ensemble Strategy] ⚠️ {symbol}: No signal available")
                        
                except Exception as e:
                    print(f"   [Ensemble Strategy] Li analysis {symbol}: {e}")
                    continue
                    
        except Exception as e:
            print(f"[Ensemble Strategy] Li nghim trng: {e}")
            import traceback
            traceback.print_exc()

    # TM V THAY THTON BFunction run_portfolio_rl_strategy this equal O N CODE DU I Y

    async def run_portfolio_rl_strategy(self, live_data_cache=None):
        """
        Run RL strategy on entire portfolio.
        UPGRADED: Converted to asynofnd execute orders in parallel.
        REFACTORED VERSION: Auto-pad observation vector to match model expected shape.
        DETAILED LOGGING: Added debug logging for eacontainsnalysis step.
        """
        logger = BOT_LOGGERS['RLStrategy']
        
        if not self.use_rl or self.portfolio_rl_agent is None or self.portfolio_rl_agent.model is None:
            logger.warning("⚠️ [RL Strategy] RL Agent not ready yet, skipping cycle")
            logging.warning("RL Agent not ready yet, skipping cycle.")
            return

        logger.info(" [RL Strategy] Agent starting portfolio analysis")
        logging.info("--- [Portfolio RL Strategy]  Agent starting portfolio analysis ---")

        try:
            # L y danh sch symbol m Agent has d training trn d
            vec_env = self.portfolio_rl_agent.model.get_env()
            symbols_agent_knows = []
            if vec_env is not None:
                try:
                    symbols_agent_knows = vec_env.get_attr("symbols")[0]
                    logger.debug(f" [RL Strategy] Symbol_agent knows: {symbols_agent_knows}")
                except Exception:
                    pass # Fallback if get_attr fails

            # Fallback if not l y d env (tru ng h p phbi n khi load model)
            if not symbols_agent_knows:
                # Using these symbols count to ensure compatibility with RL model
                symbols_agent_knows = SYMBOLS.copy()
                logger.info(f" [RL Strategy] Fallback: Using these symbols count: {symbols_agent_knows}")
                logging.info(f"[RL Strategy] Fallback: Using these symbols count: {symbols_agent_knows}")
            else:
                logger.info(f"[RL Strategy] RL Agent d training trn: {symbols_agent_knows}")
                logging.info(f"[RL Strategy] RL Agent d training trn: {symbols_agent_knows}")
                logging.info(f"[RL Strategy] Active symbols: {list(self.active_symbols)}")
                logging.info(f"[RL Strategy] Symbols not fromrong RL Agent: {list(self.active_symbols - set(symbols_agent_knows))}")
                
                # FIXED: If RL Agent only knows limited symbols, expand to all active symbols
                # This ensures Online Learning works for all symbols
                missing_symbols = list(self.active_symbols - set(symbols_agent_knows))
                if missing_symbols:
                    logger.warning(f"⚠️ [RL Strategy] RL Agent missing symbols: {missing_symbols}")
                    logger.warning(f"⚠️ [RL Strategy] Expanding symbols_agent_knows to include all active symbols for Online Learning")
                    symbols_agent_knows = list(set(symbols_agent_knows + missing_symbols))
                    logger.info(f"✅ [RL Strategy] Expanded symbols_agent_knows: {symbols_agent_knows}")


            if not symbols_agent_knows:
                logging.warning("[RL Strategy] No symbolfixvailable for analysis.")
                return

            # --- BU C 1: XY used OBSERVATION (Tempty thi thtru ng) ---
            logger.debug("🔍 [RL Strategy] Starting to build observation for RL Agent")
            
            all_symbol_obs, live_prices, live_confidences = [], {}, {}

            live_data_cache = {}
            active_symbols_to_process = self.active_symbols.intersection(symbols_agent_knows)
            logger.debug(f" [RL Strategy] Active symbols to process: {list(active_symbols_to_process)}")
            logger.debug(f" [RL Strategy] Symbol_agent knows: {symbols_agent_knows}")
            print(f"   [RL Strategy] Active symbols to process: {list(active_symbols_to_process)}")
            print(f"   [RL Strategy] Symbol_agent knows: {symbols_agent_knows}")
            
            for symbol in active_symbols_to_process:
                try:
                    logger.debug(f" [RL Strategy] Fetching data for {symbol}...")
                    print(f"   [RL Strategy] Fetching data for {symbol}...")
                    df_features = self.data_manager.create_enhanced_features(symbol)
                    if df_features is not None and len(df_features) >= 100:
                        live_data_cache[symbol] = df_features
                        logger.debug(f"[RL Strategy] {symbol}: {len(df_features)} candles")
                        print(f"   [RL Strategy] {symbol}: {len(df_features)} candles")
                    else:
                        logger.warning(f" [RL Strategy] {symbol}: data khng d ({len(df_features) if df_features is not None else 0} candles)")
                        print(f"   [RL Strategy]  {symbol}: data khng d ({len(df_features) if df_features is not None else 0} candles)")
                except Exception as e:
                    print(f"   [RL Strategy] {symbol}: Li fetch data - {e}")
                    import traceback
                    traceback.print_exc()
                    continue

            # T o observation vector cho total symbol
            logger.debug(" [RL Strategy] Starting to create observation vector for all symbols")

            for symbol in symbols_agent_knows:
                logger.debug(f" [RL Strategy] Processing observation for {symbol}")
                
                market_obs = None
                model_ref_data = self.trending_models.get(symbol) or self.ranging_models.get(symbol)

                if symbol in live_data_cache and live_data_cache[symbol] is not None and model_ref_data:
                    logger.debug(f"[RL Strategy] {symbol}: has datali u v model reference")
                    df_features = live_data_cache[symbol]
                    ensemble_model, feature_columns = model_ref_data['ensemble'], model_ref_data['feature_columns']

                    # KH C PH C L I:  m b o feature ordering nh t qun
                    # 1. T o DataFrame with used thtfeatures nhu khi training
                    df_features_ordered = pd.DataFrame(index=df_features.index)
                    
                    # 2. TFunction t t cfeatures theo used thttfeature_columns
                    for col in feature_columns:
                        if col in df_features.columns:
                            df_features_ordered[col] = df_features[col]
                        else:
                            # TFunction gi trmc dnh data nh cho missing features
                            df_features_ordered[col] = 0.0
                            logging.warning(f"Missing feature '{col}' for {symbol}, using default value 0.0")
                    
                    # 3.  m b o not needsaN values
                    df_features_ordered = df_features_ordered.fillna(0.0)
                    
                    # 4. Chuy n d i threceiveumpy array dtrnh feature name issues
                    df_features_array = df_features_ordered.values
                    
                    # 5. Predict with DataFrame has d s p x applied tht 
                    try:
                        # Chuy n d i numpy array thnh DataFrame dpredict_proba ho t used used
                        df_features_for_pred = pd.DataFrame(df_features_array, columns=feature_columns, index=df_features.index)
                        # EnsembleModel chreceive 1 tham s(DataFrame)
                        prob_buy = ensemble_model.predict_proba(df_features_for_pred)
                    except Exception as e:
                        logging.error(f"Prediction failed for {symbol}: {e}")
                        # Fallback: Using confidence old Or gi trmc dnh data nh
                        prob_buy = 0.5
                    
                    # Apply same confidence smoothing logiofs in get_enhanced_signal
                    prob_buy_smoothed = np.clip(prob_buy, 0.05, 0.95)
                    
                    # Add randomness to avoid confidence clustering
                    if prob_buy_smoothed == 0.5:  # if l gi trfallback
                        import random
                        prob_buy_smoothed = 0.5 + random.uniform(-0.1, 0.1)
                        prob_buy_smoothed = np.clip(prob_buy_smoothed, 0.05, 0.95)
                    
                    # Apply uncertainty penalty for extreme predictions
                    if prob_buy > 0.9 or prob_buy < 0.1:
                        uncertainty_penalty = 0.1
                        if prob_buy > 0.9:
                            prob_buy_smoothed = prob_buy - uncertainty_penalty
                        else:
                            prob_buy_smoothed = prob_buy + uncertainty_penalty
                    
                    prob_buy_smoothed = np.clip(prob_buy_smoothed, 0.05, 0.95)
                    confidence = prob_buy_smoothed if prob_buy_smoothed >= 0.5 else 1.0 - prob_buy_smoothed
                    
                    # Additional confidence validation
                    if confidence > 0.95:
                        confidence = 0.85 + (confidence - 0.95) * 0.1
                    elif confidence < 0.05:
                        confidence = 0.15 - (0.05 - confidence) * 0.1
                    
                    confidence = np.clip(confidence, 0.1, 0.9)  # Cap between 10% and 90%

                    live_confidences[symbol] = confidence
                    # Quan trng: s dng feature columns d duc sp xp (khng thm confidence) d khp vi training
                    market_obs = df_features_for_pred[feature_columns].tail(1).to_numpy()[0]
                    live_prices[symbol] = df_features['close'].iloc[-1]

                if market_obs is None:
                    if model_ref_data:
                        num_features = len(model_ref_data['feature_columns'])
                        market_obs = np.zeros(num_features)
                    else:
                        # Fallback an ton if not c model no cho symbol this
                        num_features = ML_CONFIG["FEATURE_SELECTION_TOP_K"]
                        market_obs = np.zeros(num_features)

                pos_state, pnl_state, time_state = 0.0, 0.0, 0.0
                if symbol in self.open_positions:
                    pos = self.open_positions[symbol]
                    price = live_prices.get(symbol, pos['entry_price'])
                    pos_state = 1.0 if pos['signal'] == 'BUY' else -1.0
                    pnl = (price - pos['entry_price']) * pos_state
                    pnl_state = pnl / pos.get('initial_balance_at_open', 10000)
                    time_open = (datetime.now(pytz.utc) - pos['opened_at'].astimezone(pytz.utc)).total_seconds() / 3600
                    time_state = time_open / 24.0
                    
                    print(f"   [RL Position Debug] {symbol}: {pos['signal']} position encoded as pos_state={pos_state}, pnl={pnl:.2f}, time={time_open:.1f}h")

                all_symbol_obs.append(np.append(market_obs, [pos_state, pnl_state, time_state]))

            flat_obs = np.concatenate(all_symbol_obs).astype(np.float32)
            global_states = np.array([
                (10000 / 10000) - 1.0,
                len(self.open_positions) / len(symbols_agent_knows) if symbols_agent_knows else 0,
                0.0, 0.0
            ]).astype(np.float32)
            final_live_observation = np.append(flat_obs, global_states)

            # --- BU C 2: AGENT DON HNH  NG (with LOGIC fix L I PADDING) ---
            logger.debug(" [RL Strategy] Starting RL Agent prediction")

            expected_shape = self.portfolio_rl_agent.model.observation_space.shape
            logger.debug(f" [RL Strategy] Expected observation shape: {expected_shape}")
            logger.debug(f" [RL Strategy] currentobservation shape: {final_live_observation.shape}")

            # So snh v fix l i shape if needs
            if final_live_observation.shape != expected_shape:
                current_size = final_live_observation.shape[0]
                expected_size = expected_shape[0]
                
                logger.info(f" [RL Strategy] Adjusting observation shape: {current_size} -> {expected_size}")
                logging.info(f"Adjusting observation shape: {current_size} -> {expected_size}")

                if current_size < expected_size:
                    # Tch ph n global states (lun l 4 ph n tcu)
                    obs_without_global = final_live_observation[:-4]
                    global_s = final_live_observation[-4:]

                    # T o padding cOrc symbol bthi u
                    padding_size = (expected_size - 4) - len(obs_without_global)
                    padding = np.zeros(padding_size)

                    # Ghp l i observation used tht 
                    padded_obs = np.concatenate([obs_without_global, padding, global_s])
                    final_live_observation = padded_obs.astype(np.float32)
                    logger.info(f"[RL Strategy] Successfully padded observation to {final_live_observation.shape}")
                    logging.info(f"Successfully padded observation to {final_live_observation.shape}")
                else:
                    # Tru ng h p observation l n hon expected - from b t
                    final_live_observation = final_live_observation[:expected_size]
                    logger.info(f"[RL Strategy] Successfully truncated observation to {final_live_observation.shape}")
                    logging.info(f"Successfully truncated observation to {final_live_observation.shape}")

            final_live_observation = np.nan_to_num(final_live_observation)
            logger.debug(f" [RL Strategy] Final observation shape: {final_live_observation.shape}")

            logger.debug(" [RL Strategy] calling RL Agent predict...")
            action, _ = self.portfolio_rl_agent.model.predict(final_live_observation, deterministic=True)
            action_vector = self._decode_actions_to_vector(action, symbols_agent_knows, self.portfolio_rl_agent.model)
            
            logger.info(f" [RL Strategy] Action vector used: {list(zip(symbols_agent_knows, action_vector))}")
            logging.info(f"[RL Decision] Action vector used: {list(zip(symbols_agent_knows, action_vector))}")
            
            # DEBUG: Log chi ti t total symbol
            logger.debug(" [RL Strategy] Details for all symbols:")
            print(f"   [RL Debug] Action vector: {action_vector}")
            print(f"   [RL Debug] Symbols: {symbols_agent_knows}")
            for i, (symbol, action_code) in enumerate(zip(symbols_agent_knows, action_vector)):
                confidence = live_confidences.get(symbol, 0.5)
                has_position = symbol in self.open_positions
                is_active = symbol in self.active_symbols
                logger.debug(f" [RL Strategy] {symbol}: action={action_code}, conf={confidence:.2%}, has_pos={has_position}, active={is_active}")
                print(f"   [RL Debug] {symbol}: action={action_code}, conf={confidence:.2%}, has_pos={has_position}, active={is_active}")

            # --- BU C 3: TH fromHI HNH  NG B T  NG B---
            logger.debug("[RL Strategy] Starting from RL Agent action")
            
            tasks = []
            # Get original RL symbols count to know which symbols have actual RL actions
            original_rl_symbols = len(action_vector) if action_vector is not None else 0
            
            for i, symbol_to_act in enumerate(symbols_agent_knows):
                # For symbols beyond original RL model, use HOLD action (0) with medium confidence
                if i < original_rl_symbols:
                    action_code = action_vector[i]
                    confidence = live_confidences.get(symbol_to_act, 0.5)
                else:
                    # Symbols added for Online Learning - use HOLD action but still process through Online Learning
                    action_code = 0  # HOLD
                    confidence = 0.5  # Medium confidence
                    logger.info(f"🔄 [Online Learning] Processing {symbol_to_act} (not in original RL model) with Online Learning")
                
                has_position = symbol_to_act in self.open_positions
                
                logger.debug(f" [RL Strategy] processing {symbol_to_act}: action={action_code}, conf={confidence:.2%}, has_pos={has_position}")
                
                # NEW LOGIC: Check validitya action with position current
                if action_code in [1, 2] and has_position:
                    current_pos = self.open_positions[symbol_to_act]
                    current_signal = current_pos['signal']
                    
                    # Check xem action fromri ngu c with position current not
                    is_contradictory = (action_code == 1 and current_signal == 'SELL') or (action_code == 2 and current_signal == 'BUY')
                    
                    if is_contradictory:
                        logger.warning(f" [RL Strategy] {symbol_to_act}: RL quyt dnh {['HOLD', 'BUY', 'SELL'][action_code]} nhung d {current_signal} position - opposite!")
                        print(f"   [RL Skip] {symbol_to_act}: RL quyt dnh {['HOLD', 'BUY', 'SELL'][action_code]} nhung d {current_signal} position - opposite!")
                        
                        # nhn d liu RL model needs retrain do logic not used
                        if not hasattr(self, 'rl_integrity_issues'):
                            self.rl_integrity_issues = 0
                        self.rl_integrity_issues += 1
                        
                        if self.rl_integrity_issues >= 3:  # Sau 3 l n vi ph m
                            logger.error(f"⚠️ [RL Integrity] Detected {self.rl_integrity_issues} times RL agent made contradictory decisions!")
                            print(f"   [RL Integrity] Detected {self.rl_integrity_issues} times RL agenfrom modelade contradictory decisions!")
                            print(f"   [RL Integrity] Recommended to retrain RL model to fix logic.")
                            self.rl_integrity_issues = 0  # Reset counter
                        
                        continue
                    else:
                        logger.info(f" [RL Strategy] {symbol_to_act}: RL quyt dnh {['HOLD', 'BUY', 'SELL'][action_code]} nhung d {current_signal} position - bqua")
                        print(f"   [RL Skip] {symbol_to_act}: RL quyt dnh {['HOLD', 'BUY', 'SELL'][action_code]} nhung d {current_signal} position - bqua")
                        continue
                
                # processing symbols c RL action BUY/SELL v not c position
                print(f"   [Debug] {symbol_to_act}: action_code={action_code}, in_active_symbols={symbol_to_act in self.active_symbols}, has_position={has_position}")
                logger.info(f" [Debug] {symbol_to_act}: action_code={action_code}, in_active_symbols={symbol_to_act in self.active_symbols}, has_position={has_position}")
                logger.info(f" [Debug] Active symbols: {list(self.active_symbols)}")
                
                # Process all symbols through Online Learning (including HOLD actions)
                if symbol_to_act in self.active_symbols:
                    # Always process through Online Learning for all active symbols
                    symbol_data = live_data_cache.get(symbol_to_act, pd.DataFrame())
                    
                    # Get Online Learning prediction for this symbol
                    online_decision, online_confidence = self.auto_retrain_manager.online_learning.get_online_prediction_enhanced(
                        symbol_to_act, symbol_data
                    )
                    
                    logger.info(f"🔄 [Online Learning] {symbol_to_act}: Decision={online_decision}, Confidence={online_confidence:.2%}")
                    print(f"   [Online Learning] {symbol_to_act}: Decision={online_decision}, Confidence={online_confidence:.2%}")
                
                # Only proceed with full analysis for BUY/SELL actions without existing positions
                if action_code in [1, 2] and symbol_to_act in self.active_symbols and not has_position:
                    print(f"   [Debug] {symbol_to_act}: conditions met, starting Master Agent processing")
                    # Enhanced action decoding with market regime
                    market_regime, regime_confidence = self.market_regime_detector.detect_regime(live_data_cache.get(symbol_to_act, pd.DataFrame()))
                    
                    # Calculate volatility for dynamic action space
                    symbol_data = live_data_cache.get(symbol_to_act, pd.DataFrame())
                    volatility = symbol_data['close'].pct_change().std() * 100 if len(symbol_data) > 0 else 2.0
                    
                    # Get dynamic action space
                    available_actions = self.dynamic_action_space.get_action_space(symbol_to_act, market_regime, volatility)
                    
                    # Decode action with enhanced logic
                    action_name, confidence_multiplier = self.dynamic_action_space.decode_action(action_code, symbol_to_act, market_regime)
                    
                    # Apply adaptive confidence threshold
                    adaptive_threshold = self.rl_performance_tracker.get_adaptive_threshold(symbol_to_act)
                    adjusted_confidence = confidence * confidence_multiplier
                    
                    # Apply Transfer Learning
                    source_symbols = [s for s in self.active_symbols if s != symbol_to_act]
                    if source_symbols:
                        source_performance = {s: self.rl_performance_tracker.symbol_performance.get(s, {}).get('successful_actions', 0) / max(1, self.rl_performance_tracker.symbol_performance.get(s, {}).get('total_actions', 1)) for s in source_symbols}
                        transferred_performance, transfer_weights = self.transfer_learning_manager.transfer_knowledge(symbol_to_act, source_symbols, source_performance)
                        
                        # Adjust confidence based on transferred knowledge
                        transfer_multiplier = 1.0 + (transferred_performance - 0.5) * 0.2  # 10% adjustment
                        adjusted_confidence *= transfer_multiplier
                    
                    # Apply Master Agent Coordination
                    logger.info(f" [Master Agent] Starting analysis for {symbol_to_act}")
                    print(f"   [Master Agent] Starting analysis for {symbol_to_act}")
                    print(f"   [Debug] Master Agent Coordinator: {self.master_agent_coordinator}")
                    
                    try:
                        master_decision, master_confidence = self.master_agent_coordinator.coordinate_decision(
                            'trading_decision', symbol_data, symbol_to_act
                        )
                        logger.info(f" [Master Agent] Result for {symbol_to_act}: {master_decision} (confidence: {master_confidence:.2%})")
                        print(f"   [Master Agent] Result for {symbol_to_act}: {master_decision} (confidence: {master_confidence:.2%})")
                    except Exception as e:
                        logger.error(f"[Master Agent] Error analyzing {symbol_to_act}: {e}")
                        print(f"   [Master Agent] Error analyzing {symbol_to_act}: {e}")
                        master_decision, master_confidence = "HOLD", 0.5
                    
                    # Apply Advanced Ensemble Prediction
                    ensemble_decision, ensemble_confidence = self.ensemble_manager.predict_ensemble(
                        symbol_data, symbol_to_act
                    )
                    
                    # Combine RL, Master Agent, Ensemble, v Online Learning decisions
                    final_decision, final_confidence = self._combine_all_decisions_with_online_learning(
                        action_name, adjusted_confidence,
                        master_decision, master_confidence,
                        ensemble_decision, ensemble_confidence,
                        online_decision, online_confidence,
                        symbol_to_act
                    )
                    
                    logger.info(f" [RL Strategy] Enhanced decision for {symbol_to_act}:")
                    logger.info(f"   - RL Action: {action_name} (confidence: {adjusted_confidence:.2%})")
                    logger.info(f"   - Master Agent: {master_decision} (confidence: {master_confidence:.2%})")
                    logger.info(f"   - Ensemble: {ensemble_decision} (confidence: {ensemble_confidence:.2%})")
                    logger.info(f"   - Online Learning: {online_decision} (confidence: {online_confidence:.2%})")
                    logger.info(f"   - Final Decision: {final_decision} (confidence: {final_confidence:.2%})")
                    logger.info(f"   - Market Regime: {market_regime} (confidence: {regime_confidence:.2%})")
                    logger.info(f"   - Volatility: {volatility:.2f}%")
                    logger.info(f"   - Available Actions: {available_actions}")
                    logger.info(f"   - Adaptive Threshold: {adaptive_threshold:.2%}")
                    if source_symbols:
                        logger.info(f"   - Transfer Learning: {transferred_performance:.2%} from {len(source_symbols)} symbols")
                    
                    # Check confidence threshold
                    if final_confidence >= adaptive_threshold and symbol_to_act not in self.open_positions:
                        logger.info(f"[RL Strategy] Creating task: {final_decision} {symbol_to_act} with confidence {final_confidence:.2%}")
                        tasks.append(self.handle_position_logic(symbol_to_act, final_decision, final_confidence))
                    elif final_confidence >= adaptive_threshold and symbol_to_act in self.open_positions:
                        logger.info(f"[RL Strategy] Skipping {symbol_to_act}: Already has open position")
                    else:
                        logger.info(f" [RL Strategy] {symbol_to_act}: Final Confidence {final_confidence:.2%} < Threshold {adaptive_threshold:.2%}")
                    
                    # Trigger online learning feedback loop
                    self._trigger_online_learning_feedback_enhanced(symbol_to_act, final_decision, final_confidence, market_data=symbol_data)
                
                # processing symbols c confidence cao nhung RL action = HOLD (fallback analysis)
                elif action_code == 0 and symbol_to_act in self.active_symbols and symbol_to_act not in self.open_positions:
                    if confidence > 0.52:  # Confidence threshold cho fallback analysis
                        print(f"   [Debug] {symbol_to_act}: RL=HOLD nhung confidence cao, starting Master Agent processing")
                        logger.info(f" [Master Agent] Starting analysis for {symbol_to_act}")
                        print(f"   [Master Agent] Starting analysis for {symbol_to_act}")
                        print(f"   [Debug] Master Agent Coordinator: {self.master_agent_coordinator}")
                        
                        try:
                            master_decision, master_confidence = self.master_agent_coordinator.coordinate_decision(
                                'trading_decision', live_data_cache.get(symbol_to_act, pd.DataFrame()), symbol_to_act
                            )
                            logger.info(f" [Master Agent] Result for {symbol_to_act}: {master_decision} (confidence: {master_confidence:.2%})")
                            print(f"   [Master Agent] Result for {symbol_to_act}: {master_decision} (confidence: {master_confidence:.2%})")
                        except Exception as e:
                            logger.error(f"[Master Agent] Error analyzing {symbol_to_act}: {e}")
                            print(f"   [Master Agent] Error analyzing {symbol_to_act}: {e}")
                            master_decision, master_confidence = "HOLD", 0.5
                
                # Ly tn hiu t Ensemble model lm fallback
                try:
                    signal, ensemble_confidence, _ = self.get_enhanced_signal(symbol_to_act, df_features=live_data_cache.get(symbol_to_act))
                    if signal and ensemble_confidence > ML_CONFIG["MIN_CONFIDENCE_TRADE"] and symbol_to_act not in self.open_positions:
                        print(f"   [RL Fallback] {symbol_to_act}: RL=HOLD, Ensemble={signal} ({ensemble_confidence:.2%})")
                        tasks.append(self.handle_position_logic(symbol_to_act, signal, ensemble_confidence))
                    elif signal and ensemble_confidence > ML_CONFIG["MIN_CONFIDENCE_TRADE"] and symbol_to_act in self.open_positions:
                        print(f"   [RL Fallback] {symbol_to_act}: Skipping - Already has open position")
                except Exception as e:
                    print(f"   [RL Fallback] Error in fallback analysis for {symbol_to_act}: {e}")

            # FALLBACK: Check cc active symbols khng trong RL Agent
            symbols_not_in_rl = self.active_symbols - set(symbols_agent_knows)
            if symbols_not_in_rl:
                logging.info(f"[RL Fallback] Check {len(symbols_not_in_rl)} symbols khng trong RL Agent: {list(symbols_not_in_rl)}")
                print(f"   [RL Fallback] Check {len(symbols_not_in_rl)} symbols khng trong RL Agent: {list(symbols_not_in_rl)}")
                for symbol in symbols_not_in_rl:
                    try:
                        print(f"   [RL Fallback] Fetching data for {symbol}...")
                        # Using Ensemble strategy cho cc symbols ny
                        df_features = self.data_manager.create_enhanced_features(symbol)
                        if df_features is not None and len(df_features) >= 100:
                            # Add Master Agent analysis cho symbol not trong RL
                            print(f"   [Debug] {symbol}: Symbol khng trong RL, starting Master Agent processing")
                            logger.info(f" [Master Agent] Starting analysis for {symbol}")
                            print(f"   [Master Agent] Starting analysis for {symbol}")
                            
                            try:
                                master_decision, master_confidence = self.master_agent_coordinator.coordinate_decision(
                                    'trading_decision', df_features, symbol
                                )
                                logger.info(f" [Master Agent] Result for {symbol}: {master_decision} (confidence: {master_confidence:.2%})")
                                print(f"   [Master Agent] Result for {symbol}: {master_decision} (confidence: {master_confidence:.2%})")
                            except Exception as e:
                                logger.error(f"[Master Agent] Error analyzing {symbol}: {e}")
                                print(f"   [Master Agent] Error analyzing {symbol}: {e}")
                                master_decision, master_confidence = "HOLD", 0.5
                            
                            signal, confidence, _ = self.get_enhanced_signal(symbol, df_features=df_features)
                            if signal and confidence > ML_CONFIG["MIN_CONFIDENCE_TRADE"] and symbol not in self.open_positions:
                                print(f"   [RL Fallback] {symbol}: {signal} (Confidence: {confidence:.2%})")
                                tasks.append(self.handle_position_logic(symbol, signal, confidence))
                            elif signal and confidence > ML_CONFIG["MIN_CONFIDENCE_TRADE"] and symbol in self.open_positions:
                                print(f"   [RL Fallback] {symbol}: Skipping - Already has open position")
                            else:
                                print(f"   [RL Fallback] {symbol}: No reliable signal (Confidence: {confidence:.2%})")
                        else:
                            print(f"   [RL Fallback] {symbol}: data khng d (c: {len(df_features) if df_features is not None else 0} candles)")
                    except Exception as e:
                        logging.error(f"[RL Fallback] Li analysis {symbol}: {e}")
                        print(f"   [RL Fallback] {symbol}: Li analysis - {e}")
                        import traceback
                        traceback.print_exc()
                        continue
            else:
                print(f"   [RL Strategy] All active symbols are in RL Agent")

            if tasks:
                logger.info(f"[RL Strategy] Executing {len(tasks)} trading signals...")
                print(f"   [Async Execution] Executing {len(tasks)} trading signals...")
                await asyncio.gather(*tasks)
                logger.info(f"[RL Strategy] Completed executing {len(tasks)} trading signals")
                
                # --- BU C 4: Check CONCEPT DRIFT SAU KHI T T CSYMBOLS  U C analysis ---
                await self.check_concept_drift_for_processed_symbols(active_symbols_to_process)

        except Exception as e:
            import traceback
            logger.error(f"[RL Strategy] LI NGHIM TRNG in run_portfolio_rl_strategy: {e}")
            logger.error(f"[RL Strategy] Traceback: {traceback.format_exc()}")
            logging.error(f"LI NGHIM TRNG in run_portfolio_rl_strategy: {e}\n{traceback.format_exc()}")
        finally:
            logger.debug(" [RL Strategy] Ending Portfolio RL Strategy")

    async def check_concept_drift_for_processed_symbols(self, processed_symbols):
        """Check concept drift cho t t csymbols has d analysis"""
        try:
            print(f"   [Concept Drift]  Check concept drift cho {len(processed_symbols)} symbols...")
            
            for symbol in processed_symbols:
                try:
                    should_retrain, retrain_reason = self.auto_retrain_manager.check_retrain_triggers(symbol)
                    if should_retrain:
                        print(f" [Auto-Retrain] Trigger detected for {symbol}: {retrain_reason}")
                        # Ch y retrain in background d block main loop
                        asyncio.create_task(self.run_background_retrain(symbol, retrain_reason))
                    else:
                        print(f"[Concept Drift] {symbol}: not has datarift detected")
                        
                except Exception as e:
                    print(f"[Concept Drift] Li Check {symbol}: {e}")
                    continue
                    
        except Exception as e:
            print(f"[Concept Drift] Li total th : {e}")

    async def run_background_retrain(self, symbol, reason):
        """Ch y retrain in background"""
        try:
            print(f" [Background Retrain] Bt du retrain {symbol}...")
            self.auto_retrain_manager.trigger_retrain(symbol, reason)
            print(f"[Background Retrain] Hon thnh retrain {symbol}")
        except Exception as e:
            print(f"[Background Retrain] Li retrain {symbol}: {e}")

    def calculate_dynamic_position_size(self, symbol, confidence, llm_sentiment_score=0.0):
        """
        Calculate flexible position size, combining confidence, volatility, LLM sentiment and symbol allocation.
        Enhanced with safer risk limits and better validation.
        """
        try:
            base_risk = RISK_MANAGEMENT["MAX_RISK_PER_TRADE"]

            # Yu t 1: Symbol allocation t backtest analysis
            symbol_config = SYMBOL_ALLOCATION.get(symbol, {"weight": 1.0, "max_exposure": 0.1, "risk_multiplier": 1.0})
            allocation_multiplier = symbol_config.get("weight", 1.0)

            # Yu t 2: Tin cy ca model Ensemble/RL
            confidence_multiplier = 1.0 + (confidence - ML_CONFIG["CONFIDENCE_THRESHOLD"])

            # Yu t 3: Cm tnh tin tc t LLM
            # im t -1.0 dn 1.0 -> Chuyn thnh h s 0.75 dn 1.25
            # Nu LLM tiu cc, gim ri ro. Nu tch cc, tang ri ro.
            llm_multiplier = 1.0 + (llm_sentiment_score * 0.25)

            print(f"   [Risk] Symbol: {symbol}, Allocation: {allocation_multiplier:.1f}x, Confidence: {confidence_multiplier:.2f}, LLM: {llm_multiplier:.2f}")

            # Yu t 4: Market volatility
            volatility_adjustment = self._calculate_volatility_adjustment(symbol)

            # Calculate final risk with safer limits
            adjusted_risk = base_risk * allocation_multiplier * confidence_multiplier * llm_multiplier * volatility_adjustment

            # Safer risk limits - khng vut qu 1.2x base risk v khng dui 0.3x base risk
            max_safe_risk = base_risk * 1.2
            min_safe_risk = base_risk * 0.3
            
            final_risk = min(max(adjusted_risk, min_safe_risk), max_safe_risk)

            print(f"   [Risk] Final calculated risk for {symbol}: {final_risk*100:.2f}% (Base: {base_risk*100:.2f}%)")
            return final_risk

        except Exception as e:
            logging.error(f"Error calculating position size for {symbol}: {e}")
            # Return conservative fallback
            return RISK_MANAGEMENT["MAX_RISK_PER_TRADE"] * 0.5

    def _safe_get_value(self, data, key, default=0.0):
        """Safely get value from data structure with proper type handling"""
        try:
            if isinstance(data, dict) and key in data:
                value = data[key]
                if value is None:
                    return default
                if hasattr(value, 'iloc'):  # pandas Series
                    return float(value.iloc[-1]) if len(value) > 0 else default
                if hasattr(value, '__iter__') and not isinstance(value, str):  # other iterables
                    return float(value[-1]) if len(value) > 0 else default
                return float(value)
            return default
        except Exception as e:
            logging.warning(f"Error getting value for key '{key}': {e}")
            return default

    def _validate_numeric_value(self, value, min_val=None, max_val=None, default=0.0):
        """Validate and clamp numeric values"""
        try:
            num_value = float(value)
            if min_val is not None:
                num_value = max(num_value, min_val)
            if max_val is not None:
                num_value = min(num_value, max_val)
            return num_value
        except (ValueError, TypeError):
            return default

    def _log_and_print(self, level, message, print_message=None):
        """Unified logging and printing function"""
        if print_message is None:
            print_message = message
            
        if level == "info":
            logging.info(message)
            print(f"ℹ️ {print_message}")
        elif level == "warning":
            logging.warning(message)
            print(f"ℹ️ {print_message}")
        elif level == "error":
            logging.error(message)
            print(f" {print_message}")
        elif level == "success":
            logging.info(message)
            print(f" {print_message}")
        else:
            logging.info(message)
            print(print_message)

    def _calculate_volatility_adjustment(self, symbol):
        """Calculate volatility adjustment factor safely"""
        try:
            primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, PRIMARY_TIMEFRAME_DEFAULT)
            df_primary_tf = self.data_manager.fetch_multi_timeframe_data(
                symbol, count=RISK_MANAGEMENT["VOLATILITY_LOOKBACK"] + 5
            ).get(primary_tf)
            
            if df_primary_tf is not None and not df_primary_tf.empty:
                # Check if we have enough data for ATR calculation
                min_required_data = RISK_MANAGEMENT["VOLATILITY_LOOKBACK"] + 1
                if len(df_primary_tf) < min_required_data:
                    self._log_and_print("warning", f"Insufficient data for volatility adjustment for {symbol}: {len(df_primary_tf)} candles, need {min_required_data}")
                    return 0.8
                
                # Use a smaller window if we don't have enough data
                atr_window = min(RISK_MANAGEMENT["VOLATILITY_LOOKBACK"], len(df_primary_tf) - 1)
                
                atr_indicator = AverageTrueRange(
                    df_primary_tf["high"], 
                    df_primary_tf["low"], 
                    df_primary_tf["close"], 
                    window=atr_window
                )
                atr_normalized = (atr_indicator.average_true_range() / df_primary_tf["close"]).ffill().bfill()
                recent_volatility = atr_normalized.dropna().iloc[-1] if not atr_normalized.dropna().empty else 0.01
                volatility_adjustment = 1 / (1 + recent_volatility * 5)
                return self._validate_numeric_value(volatility_adjustment, 0.5, 1.5, 0.8)
            else:
                return 0.8
        except Exception as e:
            self._log_and_print("warning", f"Error calculating volatility adjustment for {symbol}: {e}")
            return 0.8

    def enhanced_risk_management(self, symbol, signal, current_price, confidence):
        try:
            df_primary_tf = self.data_manager.fetch_multi_timeframe_data(
                symbol, count=RISK_MANAGEMENT["VOLATILITY_LOOKBACK"] + 5
            ).get(PRIMARY_TIMEFRAME)
            if df_primary_tf is not None and not df_primary_tf.empty:
                # Check if we have enough data for ATR calculation
                min_required_data = RISK_MANAGEMENT["VOLATILITY_LOOKBACK"] + 1
                if len(df_primary_tf) < min_required_data:
                    self._log_and_print("warning", f"Insufficient data for enhanced risk management for {symbol}: {len(df_primary_tf)} candles, need {min_required_data}")
                    # Return fallback values
                    return {
                        "position_size": 0.01,
                        "stop_loss": current_price * 0.98 if signal == "BUY" else current_price * 1.02,
                        "take_profit": current_price * 1.02 if signal == "BUY" else current_price * 0.98
                    }
                
                # Use a smaller window if we don't have enough data
                atr_window = min(RISK_MANAGEMENT["VOLATILITY_LOOKBACK"], len(df_primary_tf) - 1)
                
                atr_indicator = AverageTrueRange(
                    df_primary_tf["high"],
                    df_primary_tf["low"],
                    df_primary_tf["close"],
                    window=atr_window,
                )
                atr_value = (
                    atr_indicator.average_true_range().dropna().iloc[-1]
                    if not atr_indicator.average_true_range().dropna().empty
                    else current_price * 0.005
                )
            else:
                atr_value = current_price * 0.005
        except Exception:
            atr_value = current_price * 0.005

        sl_atr_multiplier = RISK_MANAGEMENT.get("SL_ATR_MULTIPLIER", 1.5)

        # --- LOGIofN TON M I ---
        # 1. Calculate SL distance based on ATR
        sl_distance_atr = atr_value * sl_atr_multiplier

        # 2. Calculate minimum safe SL distance (e.g.: 0.3% of current price)
        min_safe_sl_distance = current_price * 0.003

        # 3. Ch n kho ng allh l n hon dlm SL cu cng
        sl_distance = max(sl_distance_atr, min_safe_sl_distance)
        # --- K T THC LOGIofN TON ---

        base_rr_ratio = RISK_MANAGEMENT.get("BASE_RR_RATIO", 1.5)
        rr_ratio = base_rr_ratio + (confidence - 0.5)
        tp_distance = sl_distance * rr_ratio

        if signal == "BUY":
            tp = current_price + tp_distance
            sl = current_price - sl_distance
        else:  # SELL
            tp = current_price - tp_distance
            sl = current_price + sl_distance

        print(
            f"    [Risk] SL distance for {symbol}: {sl_distance:.5f} (ATR-based: {sl_distance_atr:.5f}, Safe Min: {min_safe_sl_distance:.5f})"
        )

        return tp, sl

    def portfolio_risk_check(self):
        """Enhanced portfolio risk check with proper validation"""
        try:
            # Validate open_positions exists and is not None
            if not hasattr(self, 'open_positions') or self.open_positions is None:
                logging.warning("Portfolio risk check: open_positions is None or not available")
                return True  # Allow trading if no positions data
            
            if not isinstance(self.open_positions, dict):
                logging.warning("Portfolio risk check: open_positions is not a dictionary")
                return True
            
            # Calculate total risk percentage safely
            total_risk_percentage = 0.0
            for symbol, pos in self.open_positions.items():
                if isinstance(pos, dict) and 'risk_amount_percent' in pos:
                    risk_amount = pos.get('risk_amount_percent', 0)
                    if isinstance(risk_amount, (int, float)) and risk_amount >= 0:
                        total_risk_percentage += risk_amount
                    else:
                        logging.warning(f"Invalid risk_amount_percent for {symbol}: {risk_amount}")
            
            # Check against max portfolio risk
            max_portfolio_risk = RISK_MANAGEMENT.get("MAX_PORTFOLIO_RISK", 0.8)
            is_safe = total_risk_percentage < max_portfolio_risk
            
            if not is_safe:
                logging.warning(f"Portfolio risk limit exceeded: {total_risk_percentage:.2%} >= {max_portfolio_risk:.2%}")
            
            return is_safe
            
        except Exception as e:
            logging.error(f"Error in portfolio risk check: {e}")
            return False  # Conservative: block trading on error

    def _get_implied_usd_direction(self, symbol, signal):
        if not isinstance(symbol, str) or len(symbol) < 6:
            return None
        base, quote = symbol[:3].upper(), symbol[3:].upper()
        if quote == "USD":
            return "WEAK_USD" if signal == "BUY" else "STRONG_USD"
        if base == "USD":
            return "STRONG_USD" if signal == "BUY" else "WEAK_USD"
        if symbol in ["XAUUSD", "BTCUSD"]:
            return "WEAK_USD" if signal == "BUY" else "STRONG_USD"
        return None

    def correlation_check(self, new_symbol, new_signal):
        """
        Check conflict portfolio equal allh analysis mc dnh datati p xc of total used money.
        PHIN B N NNG C P.
        """
        # 1. Calculate exposure level of entire current portfolio
        portfolio_exposure = {}
        for position in self.open_positions.values():
            exposures = self._get_currency_exposures(
                position["symbol"], position["signal"]
            )
            for currency, direction in exposures.items():
                if currency not in portfolio_exposure:
                    portfolio_exposure[currency] = []
                portfolio_exposure[currency].append(direction)

        # 2. L y mc dnh datati p xc of l nh mi
        new_trade_exposure = self._get_currency_exposures(new_symbol, new_signal)

        # 3. Check conflict
        for currency, new_direction in new_trade_exposure.items():
            if currency in portfolio_exposure:
                # Xhas data nh hu ng d i ngh ch
                opposite_direction = "SHORT" if new_direction == "LONG" else "LONG"

                # if in danh mc c data dang c mt lnh nguc chiu vi lnh mi cng 1 money -> Chn
                if opposite_direction in portfolio_exposure[currency]:
                    print(
                        f"Rejected due to portfolio conflict: New order {new_signal} {new_symbol} ({new_direction} {currency}) "
                        f"conflicts with existing position ({opposite_direction} {currency})."
                    )
                    return False

        # if not c conflict no dufromm th y
        return True


    # THAY THTON BFunctionold equal Function M I this
    def has_high_impact_event_soon(self, symbol):
        """
        Check s kin kinh tquan tempty s p di n ra.
        PHIN B N NNG C P: HtrChs , Hng ha v Crypto.
        """
        try:
            # 1. T o m t b n ddlin k t cfromi s n with qu c gia/money tfrom li
            SYMBOL_oldRRENCY_MAP = {
                # Indices
                "SPX500": ["USD"], "NAS100": ["USD"], "US30": ["USD"],
                "DE40":   ["EUR"], "JP225":  ["JPY"], "AU200":  ["AUD"], "HK50": ["HKD", "Cthis"],
                # Commodities & Crypto
                "XAUUSD": ["USD"], "BTCUSD": ["USD"],
            }

            # 2. Xhas data nh chas data ng money/qu c gia needs theo di cho symbol current
            relevant_currencies = []
            if symbol in SYMBOL_oldRRENCY_MAP:
                relevant_currencies = SYMBOL_oldRRENCY_MAP[symbol]
            elif len(symbol) >= 6: # processing cOrc c p Forex
                relevant_currencies.extend([symbol[:3].upper(), symbol[3:].upper()])

            if not relevant_currencies:
                return None # not x l data nh dufromi n tlin quan

            # 3. L y v l c l ch kinh t 
            # <<<  fix: G I Function get_economic_calendar M I >>>
            calendar = self.news_manager.get_economic_calendar()
            if not calendar:
                return None

            now_utc = datetime.now(pytz.utc)
            buffer_hours = TRADE_FILTERS.get("EVENT_BUFFER_HOURS", 2) # L y tconfig

            for event in calendar:
                event_time_str = event.get("Date") # fix: Trading Economics used "Date"
                if not event_time_str: continue

                try:
                    # Chuy n d i th i gian s kin sang mi giUTC
                    event_time = pd.to_datetime(event_time_str).tz_convert('UTC')
                except Exception:
                    try: # Thphuong php khcif c l i
                        event_time = pd.to_datetime(event_time_str).tz_localize('UTC')
                    except Exception:
                        continue

                # 4. Check th i gian v thas data ng
                time_diff_hours = (event_time - now_utc).total_seconds() / 3600
                is_within_window = -buffer_hours < time_diff_hours < buffer_hours
                is_high_impact = event.get("Importance") == "High" or any(
                    keyword.lower() in event.get("Event", "").lower()
                    for keyword in self.high_impact_events
                )

                if is_within_window and is_high_impact:
                    # 5. Check slin quan of s kin with symbol
                    event_currency = event.get("currency", "").upper() # fix: Trading Economics used "currency"
                    if event_currency in relevant_currencies:
                        print(f" Ski n KTQT cho {symbol}: {event.get('Event')} lc {event_time.strftime('%Y-%m-%d %H:%M')} UTC.")
                        return {"title": event.get("Event"), "time": event_time}

            return None # not c s kin no suitable

        except Exception as e:
            print(f"Li in Function has_high_impact_event_soon: {e}")
            return None

    def determine_market_regime(self, symbol, use_cache=True):
        """
        Xhas data nh xu hu ng thtru ng chnh (Uptrend, Downtrend, Sideways)
        d a trn data khung D1.
        """
        # Check cache before dti t ki m API call
        if use_cache and symbol in self.market_regime_cache:
            cached_regime, cache_time = self.market_regime_cache[symbol]
            # if cache not yet qu 4 gith used l i
            if (datetime.now() - cache_time).total_seconds() < 4 * 3600:
                return cached_regime

        try:
            # L y data D1
            df_d1_data = self.data_manager.fetch_multi_timeframe_data(symbol, count=250)
            if "D1" not in df_d1_data:
                print(
                    f"[{symbol}]  not has datali u D1 dx l data nh xu hu ng. mc dnh data nh l Sideways."
                )
                return "Sideways"

            df_d1 = df_d1_data["D1"]

            # Calculate necessary indicators trn D1
            adx_d1 = ADXIndicator(
                df_d1["high"], df_d1["low"], df_d1["close"], window=14
            ).adx()
            ema200_d1 = EMAIndicator(df_d1["close"], window=200).ema_indicator()

            latest_close = df_d1["close"].iloc[-1]
            latest_adx = adx_d1.iloc[-1]
            latest_ema200 = ema200_d1.iloc[-1]

            regime = "Sideways"
            # if ADX > 25, thtru ng c xu hu ng r rng
            if latest_adx > 25:
                if latest_close > latest_ema200:
                    regime = "Uptrend"
                else:
                    regime = "Downtrend"

            # Luu vo cache
            self.market_regime_cache[symbol] = (regime, datetime.now())
            return regime

        except Exception as e:
            print(
                f"[{symbol}] Error determining trend data: {e}. Defaulting trend to Sideways."
            )
            return "Sideways"
    # EnhancedTradingBofrom modelethods
    def check_and_close_for_major_events(self):
        """
        Qut all s kin vi m. G i bo co tempty thi v used l receiveu needs.
        """
        # --- BU C 1: XY used BO CO Tempty THI ---
        status_report_parts = ["**[Pre-Check] Starting to scan all upcoming micro events...**"]
        print(status_report_parts[0]) # In ra log console

        # Check if c v th dang m (d quyt dnh s dng lnh) hoc bt thung bo
        should_check = self.open_positions or TRADE_FILTERS.get("SEND_PRE_CHECK_STATUS_ALERT", False)
        if not should_check:
            return

        try:
            # <<<  fix: G I Function get_economic_calendar M I >>>
            calendar = self.news_manager.get_economic_calendar()
            if calendar:
                status_report_parts.append(f"L y d {len(calendar)} s kin th tTrading Economics.")
                print(status_report_parts[-1])
            else:
                status_report_parts.append(f"not l y d l ch kinh t .")
                return

            now_utc = datetime.now(pytz.utc)
            lookahead_hours = 6
            events_to_act_on = []
            SYMBOL_oldRRENCY_MAP = {
                "SPX500": ["USD"], "NAS100": ["USD"], "US30": ["USD"],
                "DE40": ["EUR"], "JP225": ["JPY"], "AU200": ["AUD"], "HK50": ["HKD", "Cthis"],
                "XAUUSD": ["USD"], "BTCUSD": ["USD"], "ETHUSD": ["USD"]
            }

            for event in calendar:
                event_time_str = event.get("Date")
                if not event_time_str: continue
                try:
                    event_time = pd.to_datetime(event_time_str).tz_convert('UTC')
                except Exception: continue

                time_diff_hours = (event_time - now_utc).total_seconds() / 3600

                # <<<  fix L I: P KI U SANG STRING before KHI SO SNH >>>
                is_super_high_impact = str(event.get("Importance", "")).lower() == "high" or any(
                    keyword.lower() in str(event.get("Event", "")).lower() for keyword in self.high_impact_events
                )
                # <<< K T THC fix L I >>>

                if 0 < time_diff_hours < lookahead_hours and is_super_high_impact:
                    events_to_act_on.append({
                        "title": event.get("Event"),
                        "currency": event.get("currency", "").upper(),
                        "time_diff_hours": time_diff_hours
                    })

            # --- BU C 2: HNH  NG V G I THNG BO ---

            # K ch b n 1: C s kin -> ng l nh v g i needsh bo kh n
            if events_to_act_on and self.open_positions:
                positions_to_close = []
                for symbol, position in self.open_positions.items():
                    for event in events_to_act_on:
                        pos_currencies = SYMBOL_oldRRENCY_MAP.get(symbol, [])
                        if len(symbol) >= 6 and not pos_currencies:
                             pos_currencies = [symbol[:3].upper(), symbol[3:].upper()]

                        if event["currency"] in pos_currencies:
                            positions_to_close.append({
                                "symbol": symbol,
                                "reason": f"Closed before event '{event['title']}' ({event['time_diff_hours']:.1f}h before)"
                            })
                            break

                if positions_to_close:
                    alert_message = (
                        f"🚨 **WARNING: MASS CLOSE POSITIONS BEFORE NEWS** 🚨\n"
                        f"Detected importanfrom modelacro event coming soon. Proceeding to close positions to preserve capital:\n"
                    )
                    for pos_to_close in positions_to_close:
                        symbol_to_close = pos_to_close["symbol"]
                        reason_to_close = pos_to_close["reason"]
                        if symbol_to_close in self.open_positions:
                            current_price = self.data_manager.get_current_price(symbol_to_close)
                            if current_price:
                                alert_message += f"- **{symbol_to_close}**: {reason_to_close}\n"
                                self.close_position_enhanced(symbol_to_close, reason_to_close, current_price)
                    self.send_discord_alert(alert_message)
                return

            # K ch b n 2: not c s kin -> G i bo co tempty thi "an ton"
            status_report_parts.append("not c s kin vi m no s p di n ra. An ton dgiao dh.")
            print(status_report_parts[-1])
            if TRADE_FILTERS.get("SEND_PRE_CHECK_STATUS_ALERT", False):
                self.send_discord_alert("\n".join(status_report_parts))

        except Exception as e:
            print(f"Li in Functioncheck_and_close_for_major_events: {e}")
    def _execute_trending_strategy(self, symbol):
        """
        Execute trending strategy (your original logic).
        Includefixll improved filters.
        """
        print(
            f"[{symbol}] [Trending Strategy] 🔍 Analyzing with trending strategy..."
        )

        if symbol not in self.models:
            print(f"[{symbol}]  Model cho {symbol} not yet s n sng.")
            return

        # 1. L y T+n hi+u tm hnh ML
        tech_signal_raw, tech_confidence, _ = self.get_enhanced_signal(symbol)
        print(
            f"[{symbol}] [ML]  Equal: {tech_signal_raw}, probability: {tech_confidence:.2%}"
        )

        if (
            tech_signal_raw not in ["BUY", "SELL"]
            or tech_confidence < ML_CONFIG["MIN_CONFIDENCE_TRADE"]
        ):
            return

        df_features = self.data_manager.create_enhanced_features(symbol)
        if df_features is None or df_features.empty:
            return

        latest = df_features.iloc[-1]

        # 2. Bl c xu hu ng H1
        ema50_h1 = latest.get("ema50_H1")
        close_price = latest.get("close")
        if ema50_h1 and close_price:
            if tech_signal_raw == "BUY" and close_price < ema50_h1:
                print(f"[{symbol}] BUY order blocked! M15 price below H1 EMA50.")
                return
            if tech_signal_raw == "SELL" and close_price > ema50_h1:
                print(f"[{symbol}] SELL order blocked! M15 price above H1 EMA50.")
                return

        # 3. Bl c Price Action Score
        pa_score = self.calculate_price_action_score(df_features)
        PA_CONFIRMATION_THRESHOLD = 0.1
        if not (
            (tech_signal_raw == "BUY" and pa_score > PA_CONFIRMATION_THRESHOLD)
            or (tech_signal_raw == "SELL" and pa_score < -PA_CONFIRMATION_THRESHOLD)
        ):
            print(
                f"[{symbol}]  T+n hi+u MLnot d PA xcreceive (Score: {pa_score:.2f})."
            )
            return
        print(f"[{symbol}] T+n hi+u ML d PA xcreceive (Score: {pa_score:.2f}).")

        # 4. Bl c Phn k& Ki t s c
        if tech_signal_raw == "BUY":
            # Block BUY orders if signal predicts DOWNWARD direction
            if latest.get("bearish_divergence") == 1 or latest.get("bb_exhaustion_sell") == 1:
                print(f"[{symbol}] ⚠️ BUY order blocked! Detected bearish divergence/exhaustion signal.")
                return
        elif tech_signal_raw == "SELL":
            # Block SELL orders if signal predicts UPWARD direction
            if latest.get("bullish_divergence") == 1 or latest.get("bb_exhaustion_buy") == 1:
                print(f"[{symbol}] ⚠️ SELL order blocked! Detected bullish divergence/exhaustion signal.")
                return

        # if vu t qua t t c , processing vo l nh
        print(
            f"[{symbol}] [Trending Strategy] T+n hi+u h+p l+ : {tech_signal_raw} with Confidence {tech_confidence:.2%}"
        )
        self.handle_position_logic(symbol, tech_signal_raw, confidence=tech_confidence)

    # (in l p EnhancedTradingBot, thay thFunction this)
    # EnhancedTradingBofrom modelethods

    # TM V THAY THFunction this in L P EnhancedTradingBot

    # EnhancedTradingBofrom modelethods

    async def handle_position_logic(self, symbol, signal, confidence):
        """
        Refactored position logic with MasterAgent as central decision-maker.
        Process: Basic checks -> Gather market data -> Ask Master Agent for final decision -> Execute if approved.
        """
        try:
            # --- STEP 1: BASIC INITIAL CHECKS ---
            symbol_config = SYMBOL_ALLOCATION.get(symbol, {"weight": 1.0, "max_exposure": 0.1, "risk_multiplier": 1.0})
            
            # Check if symbol is in active symbols
            if symbol not in self.active_symbols:
                print(f"[{symbol}] Symbol not activated in active_symbols")
                return

            # Check minimum confidence threshold
            if confidence < ML_CONFIG["MIN_CONFIDENCE_TRADE"]:
                print(f"[{symbol}] Skipping: Confidence ({confidence:.2%}) too low.")
                return

            # --- STEP 2: GATHER ALL NECESSARY MARKET DATA ---
            print(f"🔍 [{symbol}] Gathering market data for Master Agent analysis...")
            
            # Get technical reasoning and features
            tech_reasoning = await self.gather_trade_reasoning(symbol, signal, confidence)
            if "Error" in tech_reasoning:
                error_message = tech_reasoning.get('Error', 'Unknown error occurred')
                print(f"[{symbol}] Skipping due to error when collecting reasoning: {error_message}")
                return

            # Get current price and features DataFrame
            current_price = self.data_manager.get_current_price(symbol)
            if current_price is None:
                print(f"[{symbol}] Skipping: Unable to get current price")
                return

            # Get features DataFrame for price action analysis
            features_df = self.data_manager.get_features_dataframe(symbol)
            if features_df is None or features_df.empty:
                print(f"[{symbol}] Warning: No features data available, using fallback")
                features_df = pd.DataFrame()

            # Calculate price action score
            pa_score = self.calculate_price_action_score(features_df)
            
            # Prepare market data package for Master Agent
            market_data = {
                'price_data': features_df,
                'current_price': current_price,
                'pa_score': pa_score,
                'technical_reasoning': tech_reasoning,
                'symbol_config': symbol_config
            }

            # --- STEP 3: CONSULT MASTER AGENT FOR FINAL DECISION ---
            print(f"🎯 [{symbol}] Consulting Master Agent for entry decision...")
            master_decision = self.master_agent_coordinator.decide_trade_entry(
                symbol=symbol,
                market_data=market_data,
                base_signal=signal,
                base_confidence=confidence
            )

            # --- STEP 4: PROCESS MASTER AGENT DECISION ---
            decision = master_decision.get("decision", "REJECT")
            justification = master_decision.get("justification", "No justification provided")
            
            if decision.upper() == "REJECT":
                print(f"❌ [{symbol}] Trade REJECTED by Master Agent: {justification}")
                self.send_discord_alert(
                    f"**ORDER REJECTED BY MASTER AGENT**\n"
                    f"- **Order:** `{signal} {symbol}`\n"
                    f"- **Reason:** *{justification}*\n"
                    f"- **Confidence:** {confidence:.2%}"
                )
                return
            
            elif decision.upper() == "WAIT":
                print(f"⏳ [{symbol}] Master Agent recommends WAITING: {justification}")
                return

            # --- STEP 5: EXECUTE APPROVED TRADE ---
            print(f"✅ [{symbol}] Trade APPROVED by Master Agent: {justification}")
            
            # Perform remaining checks only after Master Agent approval
            if TRADE_FILTERS.get("SKIP_NEAR_HIGH_IMPACT_EVENTS", True) and self.has_high_impact_event_soon(symbol):
                return
            if not self.portfolio_risk_check():
                print(f"[{symbol}] Skipping: Portfolio risk has reached maximum level.")
                return
            if not self.correlation_check(symbol, signal):
                return
            if len(self.open_positions) >= RISK_MANAGEMENT["MAX_OPEN_POSITIONS"]:
                print(f"[{symbol}] Skipping: Maximum number of positions reached.")
                return
            if TRADE_FILTERS.get("AVOID_WEEKEND", True) and is_weekend() and not is_crypto_symbol(symbol):
                print(f"[{symbol}] Skipping: Weekend trading.")
                return

            # Prepare final reasoning with Master Agent input
            final_reasoning = tech_reasoning.copy()
            final_reasoning["Master Agent Decision"] = justification
            final_reasoning["Master Agent Confidence"] = f"{master_decision.get('confidence', confidence):.2%}"
            
            # Calculate position size and risk management
            position_size = self.calculate_dynamic_position_size(symbol, confidence, 0.0)  # No LLM score in new approach
            tp, sl = self.enhanced_risk_management(symbol, signal, current_price, confidence)

            # Open the position
            self.open_position_enhanced(symbol, signal, current_price, tp, sl, position_size, confidence, final_reasoning)

        except Exception as e:
            import traceback
            error_message = f"Critical error processing {symbol}: {e}\n{traceback.format_exc()}"
            print(error_message)
            self.send_discord_alert(error_message)

    def open_position_enhanced(
            self, symbol, signal, entry_price, tp, sl, position_size_percent, confidence,
            reasoning
        ):
            # === MANDATORY NEWS FILTER CHECK ===
            filter_result = self.news_filter.check_news_filter(symbol)

            if filter_result["block_trading"]:
                logging.warning(f" Trading blocked for {symbol}: {filter_result['reason']}")
                self.send_discord_alert(
                    f" **TRADING BLOCKED**   \n"
                    f"- Symbol: {symbol}\n"
                    f"- Reason: {filter_result['reason']}\n"
                    f"- Event Time: {filter_result.get('event_time', 'N/A')}\n"
                    f"- Buffer: {filter_result['buffer_hours']} hours"
                )
                return False

            # === fix L I: p ki u all gi trsang float dd m b o tuong thch JSON ===
            position_details = {
                "symbol": symbol,
                "signal": signal,
                "entry_price": float(entry_price),
                "tp": float(tp),
                "sl": float(sl),
                "position_size_raw_percent": float(position_size_percent),
                "confidence": float(confidence),
                "initial_confidence": float(confidence),
                "last_notified_confidence": float(confidence),
                "risk_amount_percent": float(position_size_percent),
                "opened_at": datetime.now(pytz.timezone("Asia/Bangkok")),
                "initial_sl": float(sl),
                "last_sl_notified": float(sl),
                "reasoning": reasoning
            }
            # ========================================================================

            self.open_positions[symbol] = position_details
            save_open_positions(self.open_positions)

            # Extract Master Agent decision from reasoning
            master_decision = None
            if "Master Agent Decision" in reasoning:
                master_decision = {
                    "decision": "APPROVE",  # If we reach here, it was approved
                    "justification": reasoning.get("Master Agent Decision", "N/A"),
                    "confidence": reasoning.get("Master Agent Confidence", "N/A")
                }

            self.send_enhanced_alert(
                symbol, signal, entry_price, tp, sl, confidence, position_size_percent,
                reasoning, master_decision
            )
            print(
                f"  M{signal} {symbol} @{entry_price:.5f} (Conf:{confidence:.2%}, Risk:{position_size_percent*100:.2f}%)"
            )
            self.update_performance_metrics()
            
            # Check performance sau mi trade mi
            try:
                rating, data = self.evaluate_symbol_performance(symbol)
                # Ensure data is a dictionary
                if isinstance(data, dict):
                    win_rate = data.get('win_rate', 0)
                else:
                    win_rate = 0
                
                if rating == "poor":
                    print(f" [Performance Alert] {symbol} has poor performance after new trade")
                    # fromhg i needsh bo Discord Or di u ch nh strategy
            except Exception as e:
                print(f" [Performance Check] Error checking performance for {symbol}: {e}")

    # TM V THAY THFunction this in l p EnhancedTradingBot

    def close_position_enhanced(self, symbol, reason, exit_price, send_alert=True):
        if symbol not in self.open_positions:
            return
        position = self.open_positions.pop(symbol)
        save_open_positions(self.open_positions)
        closed_at = datetime.now(pytz.timezone("Asia/Bangkok"))

        # --- LOGIfromCA M I ---
        pip_value = self.calculate_pip_value(symbol)
        signal_price = position.get("signal_price", position["entry_price"]) # L y gi lc t+n hi+u

        # 1. Calculate Slippage: Difference between signal price and execution price
        slippage_pips = 0
        if pip_value != 0:
            if position["signal"] == "BUY":
                slippage_pips = (position["entry_price"] - signal_price) / pip_value
            else: # SELL
                slippage_pips = (signal_price - position["entry_price"]) / pip_value

        # 2. Calculate Spreadost (estimated)
        spread_pips = position.get("spread_at_open", 0.0) # StFunction bu c sau

        # 3. Calculate final Pips (excluding spread)
        pips = 0
        if pip_value != 0:
            if position["signal"] == "BUY":
                pips = (exit_price - position["entry_price"]) / pip_value
            else: # SELL
                pips = (position["entry_price"] - exit_price) / pip_value
        # pips -= spread_pips # Trdi chi ph spread dc P/L th c
        # --- K T THC LOGIfromCA ---

        if self.conn:
            try:
                cursor = self.conn.cursor()
                # TFunctionall gi trmi vo tuple
                trade_data = (
                    symbol, position["signal"], position["entry_price"],
                    exit_price, closed_at.strftime("%Y-%m-%d %H:%M:%S"),
                    reason, pips, position.get("initial_confidence", 0.0),
                    signal_price, slippage_pips, spread_pips # New values
                )
                # Cp nht li lnh INSERT
                cursor.execute(
                    """INSERT INTO trades (symbol, signal, entry_price, exit_price,
                                          closed_at, reason, pips, confidence,
                                          signal_price, execution_slippage_pips, spread_cost_pips)
                       VALUES (, , , , , , , , , , )""",
                    trade_data,
                )
                self.conn.commit()
            except Exception as e:
                print(f"❌ Lỗi ghi DB cho {symbol}: {e}")

        # Save closed position data to file
        self.save_closed_position(symbol, position, reason, exit_price, pips, closed_at)
        
        self.update_performance_metrics()
        if send_alert:
            self.send_close_alert_enhanced(symbol, position, reason, exit_price, pips)
        print(f"     Closed {position['signal']} {symbol} @{exit_price:.5f}. Reason: {reason}. Pips: {pips:.1f}")
        
        # Check performance after trade
        try:
            rating, data = self.evaluate_symbol_performance(symbol)
            # Ensure data is a dictionary
            if isinstance(data, dict):
                win_rate = data.get('win_rate', 0)
            else:
                win_rate = 0
            
            if rating == "poor":
                print(f" [Performance Alert] {symbol} has poor performance after trade")
                # fromhg i needsh bo Discord Or di u ch nh strategy
        except Exception as e:
            print(f" [Performance Check] Li Check performance cho {symbol}: {e}")

    def save_closed_position(self, symbol, position, reason, exit_price, pips, closed_at):
        """Save closed position data to closed_positions.json file"""
        try:
            closed_position_data = {
                "symbol": symbol,
                "signal": position.get("signal", ""),
                "entry_price": position.get("entry_price", 0),
                "exit_price": exit_price,
                "pips": pips,
                "reason": reason,
                "opened_at": position.get("opened_at").strftime("%Y-%m-%d %H:%M:%S") if position.get("opened_at") else "",
                "closed_at": closed_at.strftime("%Y-%m-%d %H:%M:%S"),
                "initial_confidence": position.get("initial_confidence", 0.0),
                "is_winning": pips > 0
            }
            
            # Read existing closed positions
            try:
                with open("/workspace/closed_positions.json", "r") as f:
                    closed_positions = json.load(f)
            except (FileNotFoundError, json.JSONDecodeError):
                closed_positions = []
            
            # Add new closed position
            closed_positions.append(closed_position_data)
            
            # Save updated closed positions
            with open("/workspace/closed_positions.json", "w") as f:
                json.dump(closed_positions, f, indent=2)
            
            print(f"✅ Saved closed position data for {symbol} to closed_positions.json")
            
        except Exception as e:
            print(f"❌ Error saving closed position data: {e}")
    
    def calculate_winrate_from_closed_positions(self, symbol=None, days=None):
        """Calculate winrate from closed positions data"""
        try:
            # Read closed positions
            try:
                with open("/workspace/closed_positions.json", "r") as f:
                    closed_positions = json.load(f)
            except (FileNotFoundError, json.JSONDecodeError):
                return {"total_trades": 0, "winning_trades": 0, "winrate": 0.0}
            
            # Filter by symbol if specified
            if symbol:
                closed_positions = [pos for pos in closed_positions if pos["symbol"] == symbol]
            
            # Filter by days if specified
            if days:
                cutoff_date = datetime.now() - timedelta(days=days)
                filtered_positions = []
                for pos in closed_positions:
                    try:
                        pos_date = datetime.strptime(pos["closed_at"], "%Y-%m-%d %H:%M:%S")
                        if pos_date >= cutoff_date:
                            filtered_positions.append(pos)
                    except ValueError:
                        continue
                closed_positions = filtered_positions
            
            # Calculate winrate
            total_trades = len(closed_positions)
            winning_trades = sum(1 for pos in closed_positions if pos.get("is_winning", False))
            winrate = (winning_trades / total_trades * 100) if total_trades > 0 else 0.0
            
            return {
                "total_trades": total_trades,
                "winning_trades": winning_trades,
                "winrate": winrate
            }
            
        except Exception as e:
            print(f"❌ Error calculating winrate: {e}")
            return {"total_trades": 0, "winning_trades": 0, "winrate": 0.0}
    
    def get_overall_winrate(self):
        """Get overall winrate from closed positions"""
        winrate_data = self.calculate_winrate_from_closed_positions()
        return winrate_data["winrate"]

    def calculate_pip_value(self, symbol):
        symbol = symbol.upper()

        # NFunctionchs 
        indices = ["SPX500", "NAS100", "US30", "JP225", "HK50", "AU200", "DE40", "HK33", "UK100"]
        for index in indices:
            if index in symbol:
                return 1.0  #  i with chs , 1 di m = 1.0

        # NFunction Hng ha
        if "XAU" in symbol or "XAG" in symbol: # Vng, B c
            return 0.01
        if "WTICO_USD" in symbol or "BCO_USD" in symbol: # D u
            return 0.01

        # NFunctioncrypto
        if "BTC" in symbol or "ETH" in symbol:
            return 0.01

        # NFunction Forex
        if "JPY" in symbol:
            return 0.01

        # mc dnh data nh cOrc c p Forex khc
        return 0.0001


    # (in l p EnhancedTradingBot, thay thFunction this)
    def send_enhanced_alert(self, symbol, signal, entry_price, tp, sl, confidence, position_size_percent, reasoning, master_decision=None):
        strategy_type = "[RL]" if self.use_rl else "[Ensemble]"

        # Format reasoning with emoji v structure d p hon
        reasoning_text = " **Lu n di m vo l nh:**\n"
        
        # Confidence
        reasoning_text += f" **Confidence:** {confidence:.2%}\n"
        
        # Technical factors (top 5)
        if "Main Technical Factors" in reasoning:
            tech_factors = reasoning["Main Technical Factors"]
            if isinstance(tech_factors, str) and tech_factors != "not c":
                reasoning_text += f" **Main Technical Factors:** {tech_factors}\n"
        
        # Trend analysis
        if "Trend Analysis" in reasoning:
            trend_analysis = reasoning["Trend Analysis"]
            if isinstance(trend_analysis, str) and trend_analysis != "not c":
                reasoning_text += f" **Trend Analysis:** {trend_analysis}\n"
        
        # News
        if "Related News" in reasoning:
            news_info = reasoning["Related News"]
            if isinstance(news_info, str) and news_info != "not c":
                reasoning_text += f" **Related News:** {news_info}\n"
        
        # Master Agent decision
        if master_decision:
            decision_emoji = " " if master_decision.get("decision") == "APPROVE" else " "
            reasoning_text += f" **Master Agent:** {decision_emoji} {master_decision.get('justification', 'N/A')}\n"
            # Fix sentiment score display - use actual value or default to 0.0
            sentiment_score = master_decision.get('sentiment_score', 0.0)
            if isinstance(sentiment_score, str):
                try:
                    sentiment_score = float(sentiment_score)
                except:
                    sentiment_score = 0.0
            
            # Addontext for sentiment score
            if sentiment_score == 0.0:
                # Check if LLM is operational to determine context
                if not self.news_manager.llm_analyzer or not self.news_manager.llm_analyzer.model:
                    sentiment_context = " (LLM notoperational)"
                else:
                    sentiment_context = " (Neutral sentiment)"
            elif sentiment_score > 0.1:
                sentiment_context = " (Positive sentiment)"
            elif sentiment_score < -0.1:
                sentiment_context = " (Negative sentiment)"
            else:
                sentiment_context = " (Neutral sentiment)"
                
            reasoning_text += f" **LLM Analysis Score:** {sentiment_score:.2f}{sentiment_context}\n"
        else:
            reasoning_text += f" **Master Agent:**  LLM notoperational, auto-approve\n"
            reasoning_text += f" **LLM Analysis Score:** 0.00\n"

        # Create beautiful Discord message
        message = (
            f" **TRADING SIGNAL {strategy_type}: {signal} {symbol}**   \n"
            f"                                        \n"
            f"{reasoning_text}\n"
            f" **Order Information:**\n"
            f" **Entry Price:** `{entry_price:.5f}`\n"
            f" **Take Profit:** `{tp:.5f}`\n"
            f" **Stop Loss:** `{sl:.5f}`\n"
            f" **Order Risk:** `{position_size_percent*100:.2f}%` of account\n"
            f"                                        \n"
            f" **Performance:** Win Rate: {self.get_overall_winrate():.2f}%"
        )
        
        self.send_discord_alert(message)

    def send_close_alert_enhanced(self, symbol, position, reason, exit_price, pips):
        profit_emoji = "  " if pips >= 0 else "  "
        duration = datetime.now(pytz.timezone("Asia/Bangkok")) - position["opened_at"]
        hours, remainder = divmod(duration.total_seconds(), 3600)
        minutes, _ = divmod(remainder, 60)
        duration_str = f"{int(hours)}h {int(minutes)}m"
        
        # Get winrate from closed positions file
        winrate = self.get_overall_winrate()
        
        message = (
            f"{profit_emoji} **POSITION CLOSED: {position.get('signal','')} {symbol}** {profit_emoji}\n"
            f"- Close reason: {reason}\n"
            f"- Exit price: {exit_price:.5f}\n"
            f"- Entry price: {position.get('entry_price',0):.5f}\n"
            f"- Result: {pips:.1f} pips\n"
            f"- Duration: {duration_str}\n"
            f"- ML Confidence at entry: {position.get('initial_confidence', 0.0):.2%}\n"
            f"-------------------------------------\n"
            f"Win Rate: {winrate:.2f}%"
        )
        self.send_discord_alert(message)

    def send_discord_alert(self, message, alert_type="INFO", priority="NORMAL", data=None):
        """Enhanced Discord alert function with rich formatting and better error handling"""
        try:
            # Validate message before processing
            if not message or not str(message).strip():
                logging.warning("⚠️ Discord alert skipped: Empty message")
                return
            
            # Clean message content and ensure UTF-8 encoding
            message = str(message).strip()
            message = message.replace('\x00', '').replace('\r', '')
            
            # Fix common encoding issues in the message
            # Replace corrupted Vietnamese text with English equivalents
            replacements = {
                'd hy': 'updated for',
                'SL mi': 'New SL',
                'vth': 'position',
                'lch s': 'historical',
                'ang used': 'Closing',
                'vthsau': 'positions',
                'dtrnh': 'to avoid',
                'cu tu n': 'weekend',
                'THNG BO': 'NOTICE',
                'NG L NH old I TU N': 'WEEKEND POSITION CLOSURE'
            }
            
            for old_text, new_text in replacements.items():
                message = message.replace(old_text, new_text)
            
            # Ensure proper UTF-8 encoding
            if isinstance(message, bytes):
                message = message.decode('utf-8', errors='replace')
            else:
                # Re-encode and decode to fix any encoding issues
                message = message.encode('utf-8', errors='replace').decode('utf-8', errors='replace')
            
            # Get current time in GMT+7
            current_time = get_current_time(TIMEZONE_CONFIG["DEFAULT_TIMEZONE"])
            time_str = current_time.strftime("%Y-%m-%d %H:%M:%S GMT+7")
            
            # Enhanced message formatting based on alert type
            enhanced_message = self._format_discord_message(message, alert_type, priority, time_str, data)

            # Use direct webhook approach
            logger = logging.getLogger('DiscordAlerts')
            logger.info(f"Sending {alert_type} alert to Discord")
            print(f"📤 [Discord] Sending {alert_type} alert via direct webhook...")
            
            # Validate webhook URL
            if not DISCORD_WEBHOOK or "webhook" not in DISCORD_WEBHOOK.lower():
                logger.warning("⚠️ Discord webhook not configured")
                print("⚠️ Discord webhook not configured")
                return

            # Create enhanced payload with better formatting and UTF-8 encoding
            payload = {
                "content": enhanced_message, 
                "username": "🤖 Trading Bot Pro",
                "avatar_url": "https://cdn.discordapp.com/emojis/1234567890123456789.png"
            }
            
            # Ensure the payload is properly encoded as UTF-8
            # Discord expects UTF-8 encoded JSON
            # Note: ensure_ascii=False allows Unicode characters to pass through
            payload_json = json.dumps(payload, ensure_ascii=False)
            payload = json.loads(payload_json)
            
            # Add embed for high priority alerts
            if priority == "HIGH" or alert_type in ["ERROR", "CRITICAL", "WARNING"]:
                payload["embeds"] = [self._create_discord_embed(message, alert_type, time_str, data)]
            
            # Send request with proper UTF-8 encoding headers
            headers = {
                'Content-Type': 'application/json; charset=utf-8'
            }
            response = requests.post(
                DISCORD_WEBHOOK, 
                data=json.dumps(payload, ensure_ascii=False).encode('utf-8'),
                headers=headers,
                timeout=15
            )
            
            print(f"📊 [Discord] Response status: {response.status_code}")
            logger.info(f"Discord response: {response.status_code}")
            
            if response.status_code in [200, 204]:
                logger.info("Discord alert sent successfully")
                print(" Discord alert sent successfully")
            else:
                logger.error(f"Discord alert failed: {response.status_code} - {response.text}")
                print(f" Discord alert failed: {response.status_code}")
                print(f"📝 Response text: {response.text}")
                
                # Provide specific guidance for common errors
                if response.status_code == 401:
                    print("🔑 [Discord Fix] Error 401 - Invalid Webhook Token:")
                    print("   1. Check if webhook URL is correct")
                    print("   2. Verify webhook token hasn't expired")
                    print("   3. Create a new webhook if needed")
                    print("   4. Update DISCORD_WEBHOOK variable in code")
                elif response.status_code == 404:
                    print("🔍 [Discord Fix] Error 404 - Webhook Not Found:")
                    print("   1. Webhook may have d deleted")
                    print("   2. Create a new webhook")
                    print("   3. Update DISCORD_WEBHOOK variable")
                elif response.status_code == 429:
                    print("⏰ [Discord Fix] Error 429 - Rate Limited:")
                    print("   1. Too many requests sent")
                    print("   2. Wait before sending more messages")
                
        except Exception as e:
            logger = logging.getLogger('DiscordAlerts')
            logger.error(f"Discord alert error: {e}")
            print(f" Discord alert error: {e}")
            import traceback
            logger.error(f"Discord alert traceback: {traceback.format_exc()}")
            print(f"Discord alert traceback: {traceback.format_exc()}")
            # Print to console as last resort
    
    def _format_discord_message(self, message, alert_type, priority, time_str, data=None):
        """Format Discord message with appropriate styling"""
        # Priority indicators
        priority_emojis = {
            "LOW": "🟢",
            "NORMAL": "🟡", 
            "HIGH": "🟠",
            "CRITICAL": "🔴"
        }
        
        # Alert type indicators
        type_emojis = {
            "INFO": "ℹ️",
            "SUCCESS": "✅",
            "WARNING": "⚠️",
            "ERROR": "❌",
            "CRITICAL": "🚨",
            "TRADE": "💰",
            "PERFORMANCE": "📊",
            "SYSTEM": "🔧"
        }
        
        priority_emoji = priority_emojis.get(priority, "⚪")
        type_emoji = type_emojis.get(alert_type, "📢")
        
        # Format the message - ensure UTF-8 encoding
        formatted_message = f"{priority_emoji} {type_emoji} **{alert_type}**\n\n"
        formatted_message += f"{message}\n\n"
        
        # Add additional data if provided
        if data and isinstance(data, dict):
            formatted_message += "**📋 Details:**\n"
            for key, value in data.items():
                # Ensure value is properly encoded
                value_str = str(value)
                formatted_message += f" **{key}:** {value_str}\n"
            formatted_message += "\n"
        
        formatted_message += f"**⏰ Time:** {time_str}"
        
        # Final encoding check
        formatted_message = formatted_message.encode('utf-8', errors='replace').decode('utf-8')
        
        return formatted_message
    
    def _create_discord_embed(self, message, alert_type, time_str, data=None):
        """Create Discord embed for rich formatting"""
        # Color mapping for different alert types
        colors = {
            "INFO": 3447003,      # Blue
            "SUCCESS": 3066993,    # Green
            "WARNING": 15105570,   # Orange
            "ERROR": 15158332,     # Red
            "CRITICAL": 10038562,  # Dark Red
            "TRADE": 3066993,      # Green
            "PERFORMANCE": 3447003, # Blue
            "SYSTEM": 7506394      # Purple
        }
        
        # Ensure message is properly encoded
        if isinstance(message, str):
            message = message.encode('utf-8', errors='replace').decode('utf-8')
        
        embed = {
            "title": f"🤖 Trading Bot Alert",
            "description": message,
            "color": colors.get(alert_type, 3447003),
            "timestamp": datetime.now().isoformat(),
            "footer": {
                "text": f"Trading Bot Pro • {time_str}"
            }
        }
        
        # Add fields if data is provided
        if data and isinstance(data, dict):
            embed["fields"] = []
            for key, value in data.items():
                embed["fields"].append({
                    "name": key,
                    "value": str(value),
                    "inline": True
                })
        
        return embed

    def test_discord_webhook(self):
        """Test Discord webhook connectivity"""
        try:
            print(f"🧪 [Discord] Testing webhook connectivity...")
            test_payload = {"content": "🧪 Test message from Trading Bot", "username": "Trading Bot"}
            response = requests.post(DISCORD_WEBHOOK, json=test_payload, timeout=10)
            print(f"📊 [Discord] Response status: {response.status_code}")
            print(f"📋 [Discord] Response headers: {dict(response.headers)}")
            if response.status_code in [200, 204]:
                print(f"[Discord] Webhook test successful!")
                return True
            else:
                print(f"⚠️ [Discord] Webhook returned {response.status_code}")
                if response.status_code == 204:
                    print(f" [Discord] Webhook returned 204 - this is actually SUCCESS for Discord!")
                    print(f"   - Discord webhook 204 means 'No Content' but message was sent")
                    print(f"   - Check your Discord channel for the test message")
                    return True
                return False
        except Exception as e:
            print(f"[Discord] Webhook test failed: {e}")
            return False
    
    def get_current_positions_summary(self):
        """L y tm t t t t cpositions current"""
        try:
            positions_summary = []
            
            # L y positions tdatabase
            if hasattr(self, 'db_manager') and self.db_manager:
                try:
                    positions = self.db_manager.get_all_positions()
                    for pos in positions:
                        if pos.get('status') == 'OPEN':
                            symbol = pos.get('symbol', 'UNKNOWN')
                            direction = pos.get('direction', 'UNKNOWN')
                            entry_price = pos.get('entry_price', 0)
                            size = pos.get('size', 0)
                            sl = pos.get('stop_loss', 0)
                            tp = pos.get('take_profit', 0)
                            opened_at = pos.get('opened_at', 'Unknown')
                            
                            # L y gi current
                            current_price = self.get_current_price(symbol)
                            if current_price:
                                pips = self.calculate_pips(symbol, entry_price, current_price, direction)
                                pips_str = f"{pips:+.1f} pips"
                            else:
                                pips_str = "N/A"
                            
                            position_info = {
                                'symbol': symbol,
                                'direction': direction,
                                'entry_price': entry_price,
                                'current_price': current_price,
                                'size': size,
                                'stop_loss': sl,
                                'take_profit': tp,
                                'pips': pips_str,
                                'opened_at': opened_at
                            }
                            positions_summary.append(position_info)
                            
                except Exception as e:
                    logging.warning(f"⚠️ [Positions] Cannot get positions from DB: {e}")
            
            # L y positions tmemory if c
            if hasattr(self, 'open_positions') and self.open_positions:
                print(f" [Debug] Found {len(self.open_positions)} positions in open_positions")
                for symbol, pos in self.open_positions.items():
                    print(f" [Debug] Checking position {symbol}: {pos}")
                    # Bot luu positions with signal (BUY/SELL) chnot must direction
                    signal = pos.get('signal', 'UNKNOWN')
                    direction = signal  # BUY/SELL
                    entry_price = pos.get('entry_price', 0)
                    size = pos.get('position_size_raw_percent', 0)  # Bot used field this
                    sl = pos.get('sl', 0)  # Bot used 'sl' chnot must 'stop_loss'
                    tp = pos.get('tp', 0)  # Bot used 'tp' chnot must 'take_profit'
                    opened_at = pos.get('opened_at', 'Unknown')
                    
                    # L y gi current
                    current_price = self.get_current_price(symbol)
                    if current_price:
                        pips = self.calculate_pips(symbol, entry_price, current_price, direction)
                        pips_str = f"{pips:+.1f} pips"
                    else:
                        pips_str = "N/A"
                    
                    position_info = {
                        'symbol': symbol,
                        'direction': direction,
                        'entry_price': entry_price,
                        'current_price': current_price,
                        'size': size,
                        'stop_loss': sl,
                        'take_profit': tp,
                        'pips': pips_str,
                        'opened_at': opened_at
                    }
                    positions_summary.append(position_info)
                    print(f" [Debug] Added position {symbol}: {position_info}")
            
            print(f" [Debug] Total positions found: {len(positions_summary)}")
            return positions_summary
            
        except Exception as e:
            logging.error(f"[Positions] Error getting positions summary: {e}")
            print(f"[Debug] Error in get_current_positions_summary: {e}")
            return []
    
    def get_active_positions_summary(self):
        """Get summary of ONLY active positions (status = OPEN)"""
        try:
            active_positions_summary = []
            
            # Ly positions t database
            if hasattr(self, 'db_manager') and self.db_manager:
                try:
                    positions = self.db_manager.get_all_positions()
                    active_count = 0
                    
                    for pos in positions:
                        # Only get positions with status = 'OPEN'
                        if pos.get('status') == 'OPEN':
                            active_count += 1
                            symbol = pos.get('symbol', 'UNKNOWN')
                            direction = pos.get('direction', 'UNKNOWN')
                            entry_price = pos.get('entry_price', 0)
                            size = pos.get('size', 0)
                            sl = pos.get('stop_loss', 0)
                            tp = pos.get('take_profit', 0)
                            opened_at = pos.get('opened_at', 'Unknown')
                            
                            # Ly gi hin ti
                            current_price = self.get_current_price(symbol)
                            if current_price:
                                pips = self.calculate_pips(symbol, entry_price, current_price, direction)
                                pips_str = f"{pips:+.1f} pips"
                            else:
                                pips_str = "N/A"
                            
                            position_info = {
                                'symbol': symbol,
                                'direction': direction,
                                'entry_price': entry_price,
                                'size': size,
                                'stop_loss': sl,
                                'take_profit': tp,
                                'opened_at': opened_at,
                                'current_price': current_price,
                                'pips': pips_str,
                                'status': 'ACTIVE'  # Mark as active
                            }
                            
                            active_positions_summary.append(position_info)
                    
                    print(f" [Active Positions] Found {active_count} active positions")
                    
                except Exception as e:
                    print(f" [Active Positions] Error getting positions from database: {e}")
                    logging.error(f"[Active Positions] Database error: {e}")
            
            # Fallback: Ly t open_positions nu database khng c
            if not active_positions_summary and hasattr(self, 'open_positions'):
                print(" [Active Positions] Fallback: Ly t open_positions")
                for symbol, position in self.open_positions.items():
                    if position.get('status') == 'OPEN':
                        position_info = {
                            'symbol': symbol,
                            'direction': position.get('direction', 'UNKNOWN'),
                            'entry_price': position.get('entry_price', 0),
                            'size': position.get('size', 0),
                            'stop_loss': position.get('stop_loss', 0),
                            'take_profit': position.get('take_profit', 0),
                            'opened_at': position.get('opened_at', 'Unknown'),
                            'current_price': position.get('current_price', 0),
                            'pips': position.get('pips', 'N/A'),
                            'status': 'ACTIVE'
                        }
                        active_positions_summary.append(position_info)
            
            return active_positions_summary
            
        except Exception as e:
            print(f" [Active Positions] Li trong get_active_positions_summary: {e}")
            logging.error(f"[Active Positions] Error: {e}")
            return []
    
    def test_discord_alert(self):
        """Test Discord alert function"""
        try:
            print(f" [Discord] Testing Discord alert function...")
            test_message = " Test Discord alert from Trading Bot"
            self.send_discord_alert(test_message)
            print(f"[Discord] Discord alert test completed")
            return True
        except Exception as e:
            print(f"[Discord] Discord alert test failed: {e}")
            return False
    
    def send_positions_summary_discord(self, positions_summary):
        """Send positions summary via Discord"""
        try:
            if not positions_summary:
                message = " **Position Summary**\n\n **No open positions**\n\nBot is ready to receive new signals!"
                self.send_discord_alert(message)
                return
            
            # Create message for positions
            message = " **Active Positions Summary**\n\n"
            
            for i, pos in enumerate(positions_summary, 1):
                symbol = pos.get('symbol', 'UNKNOWN')
                direction = pos.get('direction', 'UNKNOWN')
                entry_price = pos.get('entry_price', 0)
                current_price = pos.get('current_price', 0)
                size = pos.get('size', 0)
                sl = pos.get('stop_loss', 0)
                tp = pos.get('take_profit', 0)
                pips = pos.get('pips', 'N/A')
                opened_at = pos.get('opened_at', 'Unknown')
                
                # Icon d a trn direction
                direction_icon = "  " if direction.upper() == "BUY" else "  " if direction.upper() == "SELL" else " "
                
                # Mu s has data a trn pips
                if "+" in str(pips):
                    pips_color = "  "
                elif "-" in str(pips):
                    pips_color = "  "
                else:
                    pips_color = " "
                
                message += f"**{i}. {direction_icon} {symbol} {direction.upper()}**\n"
                message += f"    Entry: {entry_price:.5f}\n"
                message += f"    current: {current_price:.5f}\n"
                message += f"   {pips_color} P&L: {pips}\n"
                message += f"     SL: {sl:.5f}\n"
                message += f"    TP: {tp:.5f}\n"
                message += f"    Opened: {opened_at}\n\n"
            
            # TFunction thng tin total quan
            total_positions = len(positions_summary)
            message += f" **Overview:** {total_positions} open positions\n"
            message += f"**Updated:** {get_current_time(TIMEZONE_CONFIG['DEFAULT_TIMEZONE']).strftime('%Y-%m-%d %H:%M:%S GMT+7')}"
            
            self.send_discord_alert(message)
            
        except Exception as e:
            logging.error(f"[Discord] Error sending positions summary: {e}")
            print(f"[Discord] Error sending positions summary: {e}")
    
    def send_performance_summary_discord(self):
        """Send performance summary via Discord"""
        try:
            if not hasattr(self, 'performance_metrics'):
                return
            
            metrics = self.performance_metrics
            message = " **Performance Summary**\n\n"
            
            # Win Rate from closed positions file
            win_rate = self.get_overall_winrate()
            win_rate_icon = "  " if win_rate >= 60 else "  " if win_rate >= 40 else "  "
            message += f"{win_rate_icon} **Win Rate:** {win_rate:.1f}%\n"
            
            # Total Trades
            total_trades = metrics.get('total_trades', 0)
            message += f" **Total Trades:** {total_trades}\n"
            
            # Win/Loss
            winning_trades = metrics.get('winning_trades', 0)
            losing_trades = metrics.get('losing_trades', 0)
            message += f"**Winning Trades:** {winning_trades}\n"
            message += f"**Losing Trades:** {losing_trades}\n"
            
            # Average Win/Loss
            avg_win = metrics.get('average_win', 0)
            avg_loss = metrics.get('average_loss', 0)
            message += f" **Avg Win:** {avg_win:+.1f} pips\n"
            message += f" **Avg Loss:** {avg_loss:+.1f} pips\n"
            
            # Profit Factor
            profit_factor = metrics.get('profit_factor', 0)
            pf_icon = "  " if profit_factor >= 1.5 else "  " if profit_factor >= 1.0 else "  "
            message += f"{pf_icon} **Profit Factor:** {profit_factor:.2f}\n"
            
            message += f"\n**C p nh t:** {get_current_time(TIMEZONE_CONFIG['DEFAULT_TIMEZONE']).strftime('%Y-%m-%d %H:%M:%S GMT+7')}"
            
            self.send_discord_alert(message)
            
        except Exception as e:
            logging.error(f"[Discord] Error sending performance summary: {e}")
            print(f"[Discord] Error sending performance summary: {e}")
    
    def resend_signals_on_startup(self):
        """Resend active positions via Discord on bot startup"""
        try:
            print(" [Debug] resend_signals_on_startup() d duc gi")
            print(f" [Debug] DISCORD_CONFIG trong Function: {DISCORD_CONFIG}")
            
            if not DISCORD_CONFIG.get("RESEND_SIGNALS_ON_STARTUP", False):
                print(" [Discord] Resend signals on startup is disabled")
                return
            
            print(" [Discord] Starting resend signals on startup...")
            
            # Delay to avoid spam
            delay_seconds = DISCORD_CONFIG.get("RESEND_DELAY_SECONDS", 2)
            max_messages = DISCORD_CONFIG.get("MAX_RESEND_MESSAGES", 10)
            message_count = 0
            
            # 1. Send bot startup notification
            startup_message = " **Trading Bot has restarted!**\n\n"
            startup_message += " **System Status:**\n"
            startup_message += " Bot is ready and operational\n"
            startup_message += " Discord webhook connected successfully\n"
            startup_message += " Database connected successfully\n"
            startup_message += f"**Startup Time:** {get_current_time(TIMEZONE_CONFIG['DEFAULT_TIMEZONE']).strftime('%Y-%m-%d %H:%M:%S GMT+7')}"
            
            self.send_discord_alert(startup_message)
            message_count += 1
            
            if message_count >= max_messages:
                return
            
            # Delay
            time.sleep(delay_seconds)
            
            # 2. Send current positions summary (ONLY OPEN POSITIONS)
            if DISCORD_CONFIG.get("RESEND_POSITIONS", True):
                print(" [Discord] Sending active positions summary...")
                active_positions_summary = self.get_active_positions_summary()
                
                if active_positions_summary:
                    self.send_positions_summary_discord(active_positions_summary)
                    message_count += 1
                else:
                    print(" [Discord] No active positions to send")
                
                if message_count >= max_messages:
                    return
                
                # Delay
                time.sleep(delay_seconds)
            
            # 3. Send performance summary
            if DISCORD_CONFIG.get("RESEND_PERFORMANCE", False):
                print(" [Discord] Sending performance summary...")
                self.send_performance_summary_discord()
                message_count += 1
                
                if message_count >= max_messages:
                    return
                
                # Delay
                time.sleep(delay_seconds)
            
            # 4. Gi thng tin trailing stops nu c
            if DISCORD_CONFIG.get("RESEND_TRAILING_STOPS", True):
                print(" [Discord] Sending trailing stops information...")
                self.send_trailing_stops_info_discord()
                message_count += 1
                
                if message_count >= max_messages:
                    return
                
                # Delay
                time.sleep(delay_seconds)
            
            # 5. Send completion notification
            completion_message = " **Resend signals completed!**\n\n"
            completion_message += f" **Sent {message_count} notifications**\n"
            completion_message += " Bot is ready to continue trading\n"
            completion_message += f"**Completed at:** {get_current_time(TIMEZONE_CONFIG['DEFAULT_TIMEZONE']).strftime('%Y-%m-%d %H:%M:%S GMT+7')}"
            
            self.send_discord_alert(completion_message)
            
            print(f"[Discord] Resend signals completed - {message_count} messages sent")
            
        except Exception as e:
            logging.error(f"[Discord] Error resending signals: {e}")
            print(f"[Discord] Error resending signals: {e}")
    
    def send_trailing_stops_info_discord(self):
        """G i thng tin trailing stops qua Discord"""
        try:
            if not hasattr(self, 'positions') or not self.positions:
                return
            
            trailing_stops_info = []
            
            for symbol, pos in self.positions.items():
                if pos.get('status') == 'OPEN' and pos.get('trailing_stop'):
                    trailing_stops_info.append({
                        'symbol': symbol,
                        'direction': pos.get('direction', 'UNKNOWN'),
                        'entry_price': pos.get('entry_price', 0),
                        'current_price': pos.get('current_price', 0),
                        'trailing_stop': pos.get('trailing_stop', 0),
                        'trailing_distance': pos.get('trailing_distance', 0)
                    })
            
            if not trailing_stops_info:
                return
            
            message = "  **Thng tin Trailing Stops**\n\n"
            
            for i, ts in enumerate(trailing_stops_info, 1):
                symbol = ts['symbol']
                direction = ts['direction']
                entry_price = ts['entry_price']
                current_price = ts['current_price']
                trailing_stop = ts['trailing_stop']
                trailing_distance = ts['trailing_distance']
                
                # Icon d a trn direction
                direction_icon = "  " if direction.upper() == "BUY" else "  " if direction.upper() == "SELL" else " "
                
                message += f"**{i}. {direction_icon} {symbol} {direction.upper()}**\n"
                message += f"    Entry: {entry_price:.5f}\n"
                message += f"    current: {current_price:.5f}\n"
                message += f"     Trailing Stop: {trailing_stop:.5f}\n"
                message += f"    Distance: {trailing_distance:.1f} pips\n\n"
            
            message += f"**Updated:** {get_current_time(TIMEZONE_CONFIG['DEFAULT_TIMEZONE']).strftime('%Y-%m-%d %H:%M:%S GMT+7')}"
            
            self.send_discord_alert(message)
            
        except Exception as e:
            logging.error(f"[Discord] Error sending trailing stops info: {e}")
            print(f"[Discord] Error sending trailing stops info: {e}")

    def check_historical_sl_hit(self, symbol, position, historical_candles=5):
        """
        Check xem SL d bch min l ch scneeds n before d hay not
         c bi t h u ch khi bot bgin do n trn Colab
        """
        try:
            print(f"   [Historical SL] {symbol}: Check {historical_candles} nn truc...")
            
            # Ly data lch s gn dy vi count nh hon d trnh li minimum candles
            primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, PRIMARY_TIMEFRAME_DEFAULT)
            multi_tf_data = self.data_manager.fetch_multi_timeframe_data(symbol, count=max(20, historical_candles + 10))
            
            if multi_tf_data is None:
                print(f"   [Historical SL] {symbol}: Khng th ly d liu t API")
                return False
                
            recent_data = multi_tf_data.get(primary_tf)
            
            if recent_data is None or recent_data.empty:
                print(f"   [Historical SL] {symbol}: Khng c d liu lch s cho timeframe {primary_tf}")
                return False
            
            # Kim tra xem c d d liu khng
            if len(recent_data) < historical_candles:
                print(f"   [Historical SL] {symbol}: Ch c {len(recent_data)} candles, cn t nht {historical_candles}")
                return False
            
            # L y cneeds n g n nh t (trn n current)
            historical_candles_data = recent_data.tail(historical_candles + 1).iloc[:-1]  # Bn n cu cng (current)
            
            sl_price = position["sl"]
            signal_type = position["signal"]
            
            print(f"   [Historical SL] {symbol}: SL={sl_price:.5f}, Signal={signal_type}")
            
            # Check total n n in l ch s 
            for idx, candle in historical_candles_data.iterrows():
                candle_time = idx.strftime("%Y-%m-%d %H:%M:%S")
                high_price = candle['high']
                low_price = candle['low']
                
                sl_hit = False
                hit_price = 0
                
                if signal_type == "BUY":
                    # with l nh BUY, SL bch m khi gi th p nh t <= SL
                    if low_price <= sl_price:
                        sl_hit = True
                        hit_price = low_price
                else:  # SELL
                    # with l nh SELL, SL bch m khi gi cao nh t >= SL
                    if high_price >= sl_price:
                        sl_hit = True
                        hit_price = high_price
                
                if sl_hit:
                    print(f"   [Historical SL] {symbol}:  SL  CH M! N n {candle_time}")
                    print(f"   [Historical SL] {symbol}: High={high_price:.5f}, Low={low_price:.5f}, SL={sl_price:.5f}")
                    print(f"   [Historical SL] {symbol}: Gi ch m SL={hit_price:.5f}")
                    
                    # ng l receivegay l p t c with gi SL - Discord alert controlled by configuration
                    self.close_position_enhanced(symbol, f"Historical SL Hit at {candle_time}", sl_price, send_alert=SEND_HISTORICAL_SL_ALERTS)
                    return True
            
            print(f"   [Historical SL] {symbol}: not c SLno bch min {historical_candles} n n before")
            return False
            
        except Exception as e:
            print(f"   [Historical SL] {symbol}: Li Check lch s SL: {e}")
            return False

    def check_emergency_sl_positions(self):
        """
        Check kh n c p all vthc r i ro cao (g n ch m SL)
        """
        if not self.open_positions:
            return
            
        print("   [Emergency SL] Check all positions with high risk...")
        
        for symbol, position in list(self.open_positions.items()):
            try:
                # L y gi real-time
                current_price = self.data_manager.get_current_price(symbol)
                if current_price is None:
                    continue
                
                # Tnh kho ng allh dn SL
                if position["signal"] == "BUY":
                    distance_to_sl = current_price - position["sl"]
                    sl_percentage = (distance_to_sl / position["entry_price"]) * 100
                else:  # SELL
                    distance_to_sl = position["sl"] - current_price
                    sl_percentage = (distance_to_sl / position["entry_price"]) * 100
                
                # if kho ng allh dn SL < 0.5% th Check kh n c p
                if sl_percentage < 0.5:
                    print(f"   [Emergency SL] {symbol}: High risk! {sl_percentage:.2f}% remaining to SL")
                    
                    # Check l i SL/TP with gi real-time
                    if position["signal"] == "BUY":
                        if current_price <= position["sl"]:
                            print(f"   [Emergency SL] {symbol}: CH M SL KH N C P! {current_price:.5f} <= {position['sl']:.5f}")
                            emergency_reason = self._analyze_emergency_stop_reason(symbol, position, current_price, "BUY")
                            self.close_position_enhanced(symbol, f"Emergency Stop Loss: {emergency_reason}", current_price)
                    else:  # SELL
                        if current_price >= position["sl"]:
                            print(f"   [Emergency SL] {symbol}: CH M SL KH N C P! {current_price:.5f} >= {position['sl']:.5f}")
                            emergency_reason = self._analyze_emergency_stop_reason(symbol, position, current_price, "SELL")
                            self.close_position_enhanced(symbol, f"Emergency Stop Loss: {emergency_reason}", current_price)
                            
            except Exception as e:
                print(f"   [Emergency SL] Li Check {symbol}: {e}")
                continue
    
    def _analyze_emergency_stop_reason(self, symbol, position, current_price, signal_direction):
        """
        Analyze and provide detailed reasoning for emergency stop loss
        """
        try:
            print(f"🚨 [Emergency Stop Analysis] Analyzing reason for {symbol}")
            
            reasons = []
            risk_factors = []
            market_conditions = []
            
            # 1. Calculate loss metrics
            entry_price = position["entry_price"]
            stop_loss = position["sl"]
            
            if signal_direction == "BUY":
                loss_amount = entry_price - current_price
                loss_pct = (loss_amount / entry_price) * 100
            else:  # SELL
                loss_amount = current_price - entry_price
                loss_pct = (loss_amount / entry_price) * 100
            
            reasons.append(f"Loss: {loss_pct:.2f}% ({loss_amount:.6f})")
            
            # 2. Time-based analysis
            entry_time = position.get("timestamp", datetime.now())
            if isinstance(entry_time, str):
                try:
                    entry_time = datetime.fromisoformat(entry_time.replace('Z', '+00:00'))
                except:
                    entry_time = datetime.now()
            
            time_held = datetime.now() - entry_time
            hours_held = time_held.total_seconds() / 3600
            
            if hours_held < 1:
                risk_factors.append(f"Quick loss (<1h held)")
            elif hours_held > 168:  # 7 days
                risk_factors.append(f"Extended hold ({hours_held/24:.1f} days)")
            
            reasons.append(f"Held: {hours_held:.1f}h")
            
            # 3. Market volatility analysis
            try:
                multi_tf_data = self.data_manager.fetch_multi_timeframe_data(symbol, count=50)
                if multi_tf_data:
                    primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, PRIMARY_TIMEFRAME)
                    df = multi_tf_data.get(primary_tf)
                    
                    if df is not None and len(df) > 10:
                        # Calculate recent volatility
                        recent_volatility = df['close'].pct_change().rolling(10).std().iloc[-1] * 100
                        if recent_volatility > 2.0:  # High volatility
                            market_conditions.append(f"High volatility ({recent_volatility:.2f}%)")
                        
                        # Check for gap moves
                        latest_change = abs(df['close'].pct_change().iloc[-1]) * 100
                        if latest_change > 1.0:  # Large recent move
                            market_conditions.append(f"Large move ({latest_change:.2f}%)")
                        
                        # Check RSI for extreme conditions
                        if 'rsi' in df.columns:
                            current_rsi = df['rsi'].iloc[-1]
                            if current_rsi < 20:
                                market_conditions.append(f"Oversold (RSI: {current_rsi:.1f})")
                            elif current_rsi > 80:
                                market_conditions.append(f"Overbought (RSI: {current_rsi:.1f})")
            except Exception as e:
                market_conditions.append("Market data unavailable")
            
            # 4. News/Sentiment analysis
            try:
                if hasattr(self, 'news_manager') and self.news_manager:
                    # Check for recent negative news
                    news_sentiment = getattr(self.news_manager, 'get_latest_sentiment', lambda x: None)(symbol)
                    if news_sentiment and news_sentiment.get('compound', 0) < -0.3:
                        market_conditions.append("Negative news sentiment")
            except Exception:
                pass
            
            # 5. Position sizing risk
            position_size = position.get("quantity", 0)
            if position_size > 0.1:  # Large position
                risk_factors.append("Large position size")
            
            # 6. Stop loss distance analysis
            sl_distance_pct = abs(stop_loss - entry_price) / entry_price * 100
            if sl_distance_pct < 0.5:
                risk_factors.append("Tight stop loss")
            elif sl_distance_pct > 5.0:
                risk_factors.append("Wide stop loss")
            
            # 7. Market session analysis
            current_hour = datetime.now().hour
            if 0 <= current_hour <= 6:  # Asian session low liquidity
                market_conditions.append("Low liquidity session")
            
            # Compile comprehensive reason
            reason_parts = []
            if reasons:
                reason_parts.append(" | ".join(reasons))
            if risk_factors:
                reason_parts.append(f"Risk: {', '.join(risk_factors)}")
            if market_conditions:
                reason_parts.append(f"Market: {', '.join(market_conditions)}")
            
            comprehensive_reason = " | ".join(reason_parts) if reason_parts else "Standard stop loss hit"
            
            # Log detailed analysis
            print(f"🔍 [Emergency Stop Reasoning] {symbol}:")
            print(f"   📊 Loss Analysis: {loss_pct:.2f}% loss, held {hours_held:.1f}h")
            if risk_factors:
                print(f"   ⚠️  Risk Factors: {', '.join(risk_factors)}")
            if market_conditions:
                print(f"   🌍 Market Conditions: {', '.join(market_conditions)}")
            print(f"   📝 Final Reason: {comprehensive_reason}")
            
            # Store in emergency stop history for analysis
            if not hasattr(self, 'emergency_stop_history'):
                self.emergency_stop_history = []
            
            self.emergency_stop_history.append({
                'timestamp': datetime.now(),
                'symbol': symbol,
                'loss_pct': loss_pct,
                'hours_held': hours_held,
                'risk_factors': risk_factors,
                'market_conditions': market_conditions,
                'comprehensive_reason': comprehensive_reason,
                'entry_price': entry_price,
                'exit_price': current_price,
                'stop_loss': stop_loss
            })
            
            # Keep only last 100 emergency stops
            if len(self.emergency_stop_history) > 100:
                self.emergency_stop_history = self.emergency_stop_history[-100:]
            
            return comprehensive_reason
            
        except Exception as e:
            print(f"❌ [Emergency Stop Analysis] Error analyzing {symbol}: {e}")
            return f"Analysis error: {str(e)[:50]}"

    def check_historical_sl_for_all_positions(self, historical_candles=5):
        """
        Check l ch sSL cho allc vthdang m 
         c bi t h u ch khi bot restart sau khi bgin do n trn Colab
        """
        if not ENABLE_HISTORICAL_SL_CHECK:
            return
            
        if not self.open_positions:
            return
            
        print("   [Historical SL] Check lch s SL cho tt c v th...")
        
        for symbol, position in list(self.open_positions.items()):
            try:
                # Check xelevel needs Check l ch sSLnot
                # ChCheck if vthd m> 1 gi(trnh Check lin t c)
                time_since_open = datetime.now(pytz.timezone("Asia/Bangkok")) - position["opened_at"]
                if time_since_open.total_seconds() < 3600:  # < 1 gi 
                    continue
                
                # Check l ch sSL
                sl_hit = self.check_historical_sl_hit(symbol, position, historical_candles)
                if sl_hit:
                    print(f"   [Historical SL] {symbol}: Closing position due to SL hit in history")
                    
            except Exception as e:
                print(f"   [Historical SL] Li Check {symbol}: {e}")
                continue

    def check_existing_positions(self, live_data_cache=None):
        """
        REFACTORED: receive live_data_cache dtrnh fetch data nhi u l n.
        C i money: Tch h p Check l ch sSLngay khi kh i used
        """
        symbols_to_process = list(self.open_positions.keys())
        for symbol in symbols_to_process:
            if symbol not in self.open_positions:
                continue
            try:
                position = self.open_positions[symbol]

                # <<< C I money M I: Check l ch sSL before khi Check gi current >>>
                #  c bi t quan tempty khi bot restart sau khi bgin do n
                print(f"   [Position Check] {symbol}: Bt du Check vth ...")
                
                # Check l ch sSLif vthd m> 30 pht (only if enabled)
                if ENABLE_HISTORICAL_SL_CHECK:
                    time_since_open = datetime.now(pytz.timezone("Asia/Bangkok")) - position["opened_at"]
                    if time_since_open.total_seconds() > 1800:  # > 30 pht
                        print(f"   [Position Check] {symbol}: Check l ch sSL (vthd m{time_since_open})")
                        historical_sl_hit = self.check_historical_sl_hit(symbol, position, historical_candles=5)
                        if historical_sl_hit:
                            print(f"   [Position Check] {symbol}:  used do SL ch min l ch s ")
                            continue  # B qua Check tip theo v d s dng lnh

                # <<< C I money 1: Using CACHED DATA Or FALLBACK >>>
                primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, PRIMARY_TIMEFRAME_DEFAULT)
                
                if live_data_cache and symbol in live_data_cache:
                    # Using data tcache
                    recent_data = live_data_cache[symbol]
                    print(f"[{symbol}] Using cached data cho position check")
                else:
                    # Fallback: fetch data if not c cache
                    recent_data = self.data_manager.fetch_multi_timeframe_data(symbol, count=50).get(primary_tf)
                    print(f"[{symbol}]  Fallback: fetch data cho position check")

                if recent_data is None or recent_data.empty:
                    print(f"[{symbol}]  not has datali u g n duc dCheck vth , bqua.")
                    continue

                # Using gi real-time thay v gi used of n n
                current_price = self.data_manager.get_current_price(symbol)
                if current_price is None:
                    # Fallback vgi used if not l y d gi real-time
                    current_price = recent_data['close'].iloc[-1]
                    print(f"   [SL Check] {symbol}: Fallback vgi usedn n: {current_price:.5f}")
                else:
                    print(f"   [SL Check] {symbol}: Using real-time price: {current_price:.5f}")

                # 1. Check TP/SL before (Logic old original)
                closed = False
                print(f"   [SL Check] {symbol}: Current price={current_price:.5f}, SL={position['sl']:.5f}, TP={position['tp']:.5f}")
                
                if position["signal"] == "BUY":
                    if current_price >= position["tp"]:
                        print(f"   [SL Check] {symbol}: Hit TP! {current_price:.5f} >= {position['tp']:.5f}")
                        self.close_position_enhanced(symbol, "Hit Take Profit", current_price)
                        closed = True
                    elif current_price <= position["sl"]:
                        print(f"   [SL Check] {symbol}: Hit SL! {current_price:.5f} <= {position['sl']:.5f}")
                        self.close_position_enhanced(symbol, "Hit Stop Loss", current_price)
                        closed = True
                else:  # SELL
                    if current_price <= position["tp"]:
                        print(f"   [SL Check] {symbol}: Hit TP! {current_price:.5f} <= {position['tp']:.5f}")
                        self.close_position_enhanced(symbol, "Hit Take Profit", current_price)
                        closed = True
                    elif current_price >= position["sl"]:
                        print(f"   [SL Check] {symbol}: Hit SL! {current_price:.5f} >= {position['sl']:.5f}")
                        self.close_position_enhanced(symbol, "Hit Stop Loss", current_price)
                        closed = True
                if closed:
                    continue

                # 2. Check "TH I GIAN N H N" (Grace Period) with timezone management
                grace_period_candles = ML_CONFIG.get("CONFIDENCE_CHECK_GRACE_PERIOD_CANDLES", 0)
                current_time = get_current_time(TIMEZONE_CONFIG["DEFAULT_TIMEZONE"])
                time_since_open = (current_time - position["opened_at"])

                minutes_per_candle = 60 # Default H1
                tf_string = primary_tf.upper()
                if "M" in tf_string: minutes_per_candle = int(tf_string.replace("M", ""))
                elif "H" in tf_string: minutes_per_candle = int(tf_string.replace("H", "")) * 60
                elif "D" in tf_string: minutes_per_candle = 1440
                grace_period_minutes = grace_period_candles * minutes_per_candle
                is_in_grace_period = (time_since_open.total_seconds() / 60) < grace_period_minutes

                # 3. Check NG L NH (Chkhi d qua th i gian n h n)
                if not is_in_grace_period:
                    _, current_confidence, _ = self.get_enhanced_signal(symbol, for_open_position_check=True)
                    close_threshold = ML_CONFIG.get("CLOSE_ON_CONFIDENCE_DROP_THRESHOLD", 0)

                    if current_confidence > 0 and close_threshold > 0 and current_confidence < close_threshold:
                        # <<< C I money 2: TFunction I U KI N Xcreceive KTHU T >>>
                        ema_period = ML_CONFIG.get("EMA_SHORT_TERM_PERIOD_FOR_CLOSE", 20)
                        latest_ema_short = EMAIndicator(recent_data["close"], window=ema_period).ema_indicator().iloc[-1]
                        current_price = self.data_manager.get_current_price(symbol)

                        technical_confirmation = False
                        if position["signal"] == "BUY" and current_price < latest_ema_short:
                            technical_confirmation = True
                            logging.info(f"[{symbol}] [CloseConfirm] Current price ({current_price:.5f}) reached below EMA{ema_period} ({latest_ema_short:.5f}).")
                        elif position["signal"] == "SELL" and current_price > latest_ema_short:
                            technical_confirmation = True
                            logging.info(f"[{symbol}] [CloseConfirm] Current price ({current_price:.5f}) reached above EMA{ema_period} ({latest_ema_short:.5f}).")

                        # <<< C I money 3: LOGIpublic L NH K T H P >>>
                        if technical_confirmation:
                            is_in_profit = (position["signal"] == "BUY" and current_price > position["entry_price"]) or \
                                           (position["signal"] == "SELL" and current_price < position["entry_price"])

                            reason = f"ML Conf gi m ({current_confidence:.2%}) V gi ph vEMA{ema_period}"
                            reason += " -> Ch t l i s m" if is_in_profit else " -> from l nh s m"

                            print(f"[{symbol}] Closing position due to: {reason}")
                            self.close_position_enhanced(symbol, reason, current_price)
                            continue
                        else:
                            print(f"[{symbol}]  Confidence gi m ({current_confidence:.2%}) nhung gi v n gic u trc. from model th i gil nh.")

                # 4. UPDATE TRAILING STOP - DISABLED (Master Agent handles all trailing stops now)
                # self.update_trailing_stop(symbol, current_price)  # Disabled - Master Agent only

            except Exception as e:
                import traceback
                print(f"Li Check vth{symbol}: {e}\n{traceback.format_exc()}")
                pass
    def close_all_non_crypto_positions_for_weekend(self):
            """
            L p qua allc vthdang mv used receiveg vthnot must l crypto.
            """
            positions_to_close = []
            for symbol, position in self.open_positions.items():
                if not is_crypto_symbol(symbol):
                    positions_to_close.append(symbol)

            if not positions_to_close:
                print("[Weekendlose] not c l receiveo needs used vo cu tu n.")
                return

            alert_message = "🚨 **[NOTICE] WEEKEND POSITION CLOSURE**\n"
            alert_message += "Closing all non-crypto positions for weekend risk management:\n"

            print(f"[Weekendlose] ang used {len(positions_to_close)} vth ...")

            closed_count = 0
            for symbol in positions_to_close:
                current_price = self.data_manager.get_current_price(symbol)
                if current_price:
                    reason = "ng l nh tused cu tu n"
                    signal = self.open_positions[symbol]['signal']
                    alert_message += f"-  used {signal} **{symbol}** @ `{current_price:.5f}`\n"
                    self.close_position_enhanced(symbol, reason, current_price)
                    closed_count += 1

            if closed_count > 0:
                self.send_discord_alert(alert_message)

    def check_and_execute_weekend_close(self):
            """
            Check xelevel must thi dim used l nh cu tu n not v t hnh dng if needs.
            """
            if not WEEKEND_CLOSE_CONFIG["ENABLED"]:
                return

            now_utc = datetime.utcnow()

            # Reset cvo ngy Th2
            if now_utc.weekday() == 0 and self.weekend_close_executed:
                print("[Weekendlose] Reset cused l nh cho tu n mi.")
                self.weekend_close_executed = False

            # Check conditions used l nh
            is_close_day = (now_utc.weekday() == WEEKEND_CLOSE_CONFIG["CLOSE_DAY_UTC"])
            is_past_close_time = (now_utc.hour >= WEEKEND_CLOSE_CONFIG["CLOSE_HOUR_UTC"] and
                                  now_utc.minute >= WEEKEND_CLOSE_CONFIG["CLOSE_MINUTE_UTC"])

            if is_close_day and is_past_close_time and not self.weekend_close_executed:
                print("[Weekendlose]  dn thi dim used l nh cu tu n!")
                self.close_all_non_crypto_positions_for_weekend()
                self.weekend_close_executed = True # nhn d liu d thc hin d lߦ+p lߦi
    def update_trailing_stop(self, symbol, current_price):
        """
        DEPRECATED: This method is no longer used. 
        Master Agent now handles ALL trailing stops via _apply_master_agent_trailing_stops()
        """
        if RISK_MANAGEMENT.get("TRAILING_STOP_MULTIPLIER", 0) <= 0:
            return
        if symbol not in self.open_positions:
            return

        position = self.open_positions[symbol]
        initial_sl_value = position.get("initial_sl", position["sl"])
        trail_distance_price = 0

        # --- NEW LOGIC: TNH TRAILING STOP D A TRN ATR ---
        try:
            # L y data g n duc dtnh ATR (100 n n dd m b o ddata)
            multi_tf_data = self.data_manager.fetch_multi_timeframe_data(symbol, count=100)
            if not multi_tf_data:
                raise ValueError("not has datali u multi-timeframe")
                
            # Using timeframe chnh of symbol
            primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, PRIMARY_TIMEFRAME)
            df_recent = multi_tf_data.get(primary_tf)
            
            if df_recent is None or df_recent.empty or len(df_recent) < 20:
                raise ValueError(f"Insufficient data to calculate ATR (has {len(df_recent) if df_recent is not None else 0} candles)")

            # Calculate ATR on 14 recent candles (standard parameter)
            atr_indicator = AverageTrueRange(
                df_recent["high"], df_recent["low"], df_recent["close"], window=14
            )
            atr_series = atr_indicator.average_true_range().dropna()
            
            if len(atr_series) == 0:
                raise ValueError("ATR series empty sau khi dropna")
                
            current_atr = atr_series.iloc[-1]

            # L y hsnhn tc configuration RISK_MANAGEMENT
            atr_multiplier = RISK_MANAGEMENT.get(
                "TRAILING_STOP_MULTIPLIER", 3.0
            )  # mc dnh data nh l 3.0 if not c

            trail_distance_price = current_atr * atr_multiplier
            print(f" ATR-based trail distance for {symbol}: {trail_distance_price:.5f} (ATR: {current_atr:.5f})")

        except Exception as e:
            # Fallback: if not tnh daTR, used l i logic old with pip cdnh
            print(
                f" Cannot calculate ATR for {symbol}, using fixed pip value. Error: {e}"
            )
            base_trail_pips = 35  # Gi trdphng
            pip_value = self.calculate_pip_value(symbol)
            trail_distance_price = base_trail_pips * pip_value
        # --- K T THC NEW LOGIC ---

        if trail_distance_price == 0:
            return  # not lm g if kho ng allh equal 0

        new_sl = position["sl"]
        old_sl_for_notification = position.get("last_sl_notified", initial_sl_value)

        if position["signal"] == "BUY":
            potential_new_sl = current_price - trail_distance_price
            if (
                potential_new_sl > initial_sl_value
                and potential_new_sl > position["sl"]
            ):
                new_sl = potential_new_sl
        else:  # SELL
            potential_new_sl = current_price + trail_distance_price
            if (
                potential_new_sl < initial_sl_value
                and potential_new_sl < position["sl"]
            ):
                new_sl = potential_new_sl

        if new_sl != position["sl"]:
            pip_value = self.calculate_pip_value(
                symbol
            )  # Recalculate pip_value for notification
            sl_change_pips = (
                abs(new_sl - old_sl_for_notification) / pip_value if pip_value else 0
            )

            if sl_change_pips >= self.SL_CHANGE_NOTIFICATION_THRESHOLD_PIPS:
                alert_message = (
                    f"🔄 **TRAILING STOP UPDATED for {position['signal']} {symbol}**\n"
                    f"- Old SL: {old_sl_for_notification:.5f}\n"
                    f"- New SL: {new_sl:.5f}\n"
                    f"- Current price: {current_price:.5f}"
                )
                self.send_discord_alert(alert_message)
                position["last_sl_notified"] = new_sl

            position["sl"] = new_sl
            save_open_positions(self.open_positions)
            # print(f" Trailing Stop cho {symbol}: SL {old_sl_for_notification:.5f} -> {new_sl:.5f}")

    # EnhancedTradingBofrom modelethods

    # EnhancedTradingBofrom modelethods

    # TM V THAY THTON BFunction run_enhanced_bot equal O N CODE this

    # TM V THAY THTON BFunction run_enhanced_bot equal O N CODE DU I Y

    # TM V THAY THTON BFunction run_enhanced_bot equal O N CODE this

    async def run_enhanced_bot(self):
        """Main bot execution loop - refactored for better maintainability"""
        logger = logging.getLogger('BotAnalysis')
        logger.info("Starting Enhanced Trading Bot")
        print(" Advanced Trading Bot is starting!")
        
        if not self.check_api_connection():
            logger.error("Failed to connect to OANDA API")
            print(" Cannot connect to OANDA API. Please check the error.")
            return

        is_first_run = True
        logger.info("Bot initialization completed successfully")
        
        # Send startup notifications
        print(" [Startup] Sending startup notifications...")
        self.resend_signals_on_startup()
        
        # Send enhanced startup notification
        startup_data = {
            "Status": "Online",
            "Features": "LLMs + RL + Auto-Retrain",
            "Active Symbols": len(self.active_symbols),
            "API Status": "Connected"
        }
        self.send_discord_alert(" **Advanced Bot Started Successfully!**", "SUCCESS", "NORMAL", startup_data)

        while True:
            try:
                # Execute main bot cycle
                await self._execute_bot_cycle(is_first_run)
                is_first_run = False
                
            except KeyboardInterrupt:
                logger.info("Bot stopped by user")
                print("\n Bot d duc dng bi ngui dng.")
                self.send_discord_alert(" **Bot Stopped by User**", "INFO", "NORMAL")
                if self.conn: 
                    self.conn.close()
                break
            except Exception as e:
                logger.error(f"Critical error in bot main loop: {e}")
                print(f" Critical error: {e}")
                
                # Send critical error notification
                error_data = {
                    "Error Type": "Critical",
                    "Error Message": str(e),
                    "Action": "Bot will retry in 60 seconds"
                }
                self.send_discord_alert(f" **Critical Bot Error**\n{str(e)}", "CRITICAL", "HIGH", error_data)
                await asyncio.sleep(60)  # Wait before retry

    async def _execute_bot_cycle(self, is_first_run):
        """Execute one complete bot cycle"""
        # 1. System health checks
        await self._perform_system_health_checks()
        
        # 2. Wait for proper timing
        await self._handle_timing_logic(is_first_run)
        
        # 3. Model management
        await self._handle_model_management()
        
        # 4. Data management
        live_data_cache = await self._handle_data_management()
        
        # 5. Position management
        await self._handle_position_management(live_data_cache)
        
        # 5.1. Master Agent trailing stop management (ONLY system for trailing stops)
        self._apply_master_agent_trailing_stops()
        
        # 6. Trading strategy execution
        await self._handle_trading_strategy(live_data_cache)
        
        # 7. Portfolio summary
        self._display_portfolio_summary()

    async def _perform_system_health_checks(self):
        """Perform system health checks"""
        # Check for scheduled retraining
        if hasattr(self, 'auto_retrain_manager'):
            self.auto_retrain_manager.check_scheduled_retrain()
        
        # Check API health
        if hasattr(self, 'api_monitor'):
            api_report = self.api_monitor.get_api_status_report()
            if api_report["overall_health"] == "degraded":
                print(" [API Monitor] API health degraded - check report")
                for recommendation in api_report["recommendations"]:
                    print(f"   - {recommendation}")

    async def _handle_timing_logic(self, is_first_run):
        """Handle timing logic for bot execution"""
        if not is_first_run:
            await self.wait_until_top_of_the_hour()
        else:
            print(" Performing initial analysis run immediately...")

        current_time_vn = datetime.now(pytz.timezone("Asia/Bangkok"))
        print(f"\n----- Analysis cycle: {current_time_vn.strftime('%Y-%m-%d %H:%M:%S')} (VN) -----")

    async def _handle_model_management(self):
        """Handle model loading and training"""
        # Load or train models
        self.load_or_train_models()

        # Check and adjust performance (every 6 hours)
        current_hour = datetime.now(pytz.timezone("Asia/Bangkok")).hour
        if current_hour % 6 == 0:  # Check every 6 hours (0, 6, 12, 18)
            print(" [Performance Check] Performing periodic performance check...")
            self.check_and_adjust_performance()

        # Circuit breaker logic
        if not self.active_symbols:
            self.consecutive_data_failures += 1
            if self.consecutive_data_failures >= 3:
                print(" Circuit breaker activated - too many data failures")
                await asyncio.sleep(3600)
                return
        else:
            self.consecutive_data_failures = 0

    async def _handle_data_management(self):
        """Handle data fetching and caching"""
        live_data_cache = {}
        
        # Fetch data for all symbols (including symbols with positions)
        all_symbols = set(self.active_symbols) | set(self.open_positions.keys())
        
        if all_symbols:
            print(f" [Data Cache] ang fetch data cho {len(all_symbols)} symbols...")
            for symbol in all_symbols:
                try:
                    df_features = self.data_manager.create_enhanced_features(symbol)
                    if df_features is not None and len(df_features) >= 50:  # At least 50 candles for position check
                        live_data_cache[symbol] = df_features
                        print(f"   [Data Cache] {symbol}: {len(df_features)} candles")
                    else:
                        print(f"   [Data Cache]  {symbol}: data khng d")
                except Exception as e:
                    print(f"   [Data Cache]  {symbol}: Li fetch data - {e}")
                    continue
        
        return live_data_cache

    async def _handle_position_management(self, live_data_cache):
        """Handle position management tasks"""
        # Using cached data for position management
        self.check_existing_positions(live_data_cache)
        self.check_and_execute_weekend_close()
        self.check_and_close_for_major_events()
        
        # Check SL/TP more frequently (every 30 seconds)
        if hasattr(self, '_last_sl_check_time'):
            time_since_last_check = (datetime.now() - self._last_sl_check_time).total_seconds()
            if time_since_last_check >= 30:  # Check every 30 seconds
                print(" [SL Monitor] Check SL/TP thung xuyn...")
                self.check_existing_positions()  # Use fresh data
                self._last_sl_check_time = datetime.now()
        else:
            self._last_sl_check_time = datetime.now()
        
        # Check emergency SL positions
        self.check_emergency_sl_positions()
        
        # Check historical SL for all positions
        self.check_historical_sl_for_all_positions()

    async def _handle_trading_strategy(self, live_data_cache):
        """Handle trading strategy execution"""
        if self.active_symbols:
            print(f" [Strategy] Have {len(self.active_symbols)} active symbols, starting analysis...")
            
            # Master Controller: Ensure strategy runs at proper timing
            if self.use_rl:
                print("   [Master Controller]  Running RL strategy...")
                await self.run_portfolio_rl_strategy(live_data_cache)
            else:
                print("   [Master Controller]  Running Ensemble strategy...")
                await self._run_ensemble_strategy(live_data_cache)
        else:
            print("   [Safe Mode] No symbols are active. Waiting to manage all open orders.")

    def _display_portfolio_summary(self):
        """Display portfolio summary"""
        print("\n----- TNG KT portfolio -----")
        for symbol, pos in self.open_positions.items():
            print(f"   - {symbol}: {pos['signal']} @ {pos['entry_price']:.5f}")
        print(f"   - Tng v th : {len(self.open_positions)}/{RISK_MANAGEMENT['MAX_OPEN_POSITIONS']}")
    
    # === MASTER AGENT INTEGRATION METHODS ===
    
    def _get_master_agent_tp_sl_decision(self, symbol, entry_price, direction, market_data):
        """Get TP/SL decision from Master Agent"""
        try:
            print(f"🎯 [Trading Bot] Consulting Master Agent for {symbol} TP/SL decision...")
            
            # Use Master Agent to decide TP/SL levels
            tp_sl_decision = self.master_agent_coordinator.decide_tp_sl_levels(
                symbol, entry_price, direction, market_data
            )
            
            return tp_sl_decision
            
        except Exception as e:
            print(f"❌ [Trading Bot] Error consulting Master Agent: {e}")
            # Fallback to default calculation
            return self._get_fallback_tp_sl_calculation(symbol, entry_price, direction)
    
    def _get_master_agent_trailing_decision(self, symbol, current_price, entry_price, direction, market_data):
        """Get trailing stop decision from Master Agent"""
        try:
            print(f"🎯 [Trading Bot] Consulting Master Agent for {symbol} trailing stop...")
            
            # Use Master Agent to decide trailing stop activation
            trailing_decision = self.master_agent_coordinator.decide_trailing_stop_activation(
                symbol, current_price, entry_price, direction, market_data
            )
            
            return trailing_decision
            
        except Exception as e:
            print(f"❌ [Trading Bot] Error consulting Master Agent for trailing: {e}")
            return {
                'should_activate': False,
                'reasons': ['Master Agent error'],
                'current_profit_pct': 0,
                'recommended_distance': current_price * 0.01
            }
    
    def _get_fallback_tp_sl_calculation(self, symbol, entry_price, direction):
        """Fallback TP/SL calculation when Master Agent fails"""
        # Get symbol configuration
        symbol_config = ENTRY_TP_SL_CONFIG.get(symbol, ENTRY_TP_SL_CONFIG.get("EURUSD", {}))
        
        # Default multipliers
        tp_mult = symbol_config.get('atr_multiplier_tp', 3.0)
        sl_mult = symbol_config.get('atr_multiplier_sl', 2.0)
        
        # Simple percentage-based calculation
        if direction.upper() == 'BUY':
            tp_distance = entry_price * (tp_mult * 0.01)  # Convert to percentage
            sl_distance = entry_price * (sl_mult * 0.01)
            
            return {
                'take_profit': entry_price + tp_distance,
                'stop_loss': entry_price - sl_distance,
                'tp_distance': tp_distance,
                'sl_distance': sl_distance,
                'risk_reward_ratio': tp_mult / sl_mult,
                'reasoning': {'fallback': True, 'method': 'config_based'}
            }
        else:
            tp_distance = entry_price * (tp_mult * 0.01)
            sl_distance = entry_price * (sl_mult * 0.01)
            
            return {
                'take_profit': entry_price - tp_distance,
                'stop_loss': entry_price + sl_distance,
                'tp_distance': tp_distance,
                'sl_distance': sl_distance,
                'risk_reward_ratio': tp_mult / sl_mult,
                'reasoning': {'fallback': True, 'method': 'config_based'}
            }
    
    def _apply_master_agent_trailing_stops(self):
        """🎯 Master Agent Trailing Stop System - Automatically monitors and activates trailing stops for ALL symbols"""
        try:
            if not self.open_positions:
                return
            
            print("🎯 [Master Agent Trailing Stop] Analyzing ALL open positions for trailing stop opportunities...")
            print(f"   📊 Processing {len(self.open_positions)} open positions with ENHANCED Master Agent system")
            
            trailing_activations = []
            
            for symbol, position in self.open_positions.items():
                try:
                    # Skip if already has trailing stop active
                    if position.get('trailing_stop', False):
                        continue
                    
                    # Get current market data
                    current_data = self.data_manager.fetch_multi_timeframe_data(symbol, count=100)
                    if not current_data:
                        print(f"⚠️ [Master Agent] No market data for {symbol}")
                        continue
                    
                    primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, PRIMARY_TIMEFRAME)
                    df = current_data.get(primary_tf)
                    if df is None or df.empty:
                        print(f"⚠️ [Master Agent] Empty data for {symbol} on {primary_tf}")
                        continue
                    
                    current_price = df['close'].iloc[-1]
                    entry_price = position.get('entry_price', current_price)
                    direction = position.get('signal', 'BUY')
                    
                    # Calculate current profit for quick check
                    if direction.upper() == 'BUY':
                        profit_pct = (current_price - entry_price) / entry_price
                    else:
                        profit_pct = (entry_price - current_price) / entry_price
                    
                    print(f"📈 [Master Agent] {symbol}: Current profit {profit_pct:.2%} ({'✅ Above threshold' if profit_pct > 0.015 else '⏳ Below threshold'})")
                    
                    # Prepare comprehensive market data for Master Agent
                    market_data = {
                        'price_data': df,
                        'symbol': symbol,
                        'timeframe': primary_tf,
                        'multi_tf_data': current_data
                    }
                    
                    # Get comprehensive trailing stop decision from Master Agent
                    trailing_decision = self._get_enhanced_master_agent_trailing_decision(
                        symbol, current_price, entry_price, direction, market_data, position
                    )
                    
                    # Apply trailing stop if recommended by Master Agent
                    if trailing_decision['should_activate']:
                        activation_info = self._activate_trailing_stop_for_position(
                            symbol, position, current_price, entry_price, direction, trailing_decision
                        )
                        
                        if activation_info:
                            trailing_activations.append(activation_info)
                            print(f"✅ [Master Agent] Trailing stop ACTIVATED for {symbol}")
                        else:
                            print(f"❌ [Master Agent] Failed to activate trailing stop for {symbol}")
                    else:
                        print(f"⏳ [Master Agent] Trailing stop NOT activated for {symbol}: {', '.join(trailing_decision['reasons'])}")
                    
                except Exception as e:
                    print(f"❌ [Master Agent] Error processing {symbol}: {e}")
                    continue
            
            # Send summary notification if any trailing stops were activated
            if trailing_activations:
                self._send_trailing_stop_summary_notification(trailing_activations)
                self._log_trailing_stop_activations(trailing_activations)
                print(f"🎯 [Master Agent] Successfully activated {len(trailing_activations)} trailing stops")
            else:
                print("⏳ [Master Agent] No trailing stops activated in this cycle")
                self._log_trailing_stop_analysis_summary()
            
        except Exception as e:
            print(f"❌ [Master Agent Integration] Critical error in trailing stop system: {e}")
            logging.error(f"Master Agent trailing stop system error: {e}")

    def _get_enhanced_master_agent_trailing_decision(self, symbol, current_price, entry_price, direction, market_data, position):
        """Enhanced Master Agent decision system for trailing stops with comprehensive analysis"""
        try:
            print(f"🧠 [Master Agent] Deep analysis for {symbol} trailing stop decision...")
            
            # Use the existing Master Agent decision system
            base_decision = self._get_master_agent_trailing_decision(
                symbol, current_price, entry_price, direction, market_data
            )
            
            # Enhanced factors specific to this implementation
            enhanced_factors = self._calculate_enhanced_trailing_factors(
                symbol, current_price, entry_price, direction, market_data, position
            )
            
            # Combine decisions with enhanced logic
            final_decision = self._combine_trailing_decisions(base_decision, enhanced_factors, symbol)
            
            return final_decision
            
        except Exception as e:
            print(f"❌ [Master Agent] Error in enhanced trailing decision for {symbol}: {e}")
            return {
                'should_activate': False,
                'reasons': ['Analysis error'],
                'current_profit_pct': 0,
                'recommended_distance': current_price * 0.01,
                'confidence': 0.1
            }

    def _calculate_enhanced_trailing_factors(self, symbol, current_price, entry_price, direction, market_data, position):
        """Calculate additional factors for trailing stop decision"""
        try:
            df = market_data.get('price_data')
            factors = {}
            
            # 1. Position age factor (older positions more likely to trail)
            entry_time = position.get('entry_time', datetime.now())
            if isinstance(entry_time, str):
                entry_time = datetime.fromisoformat(entry_time.replace('Z', '+00:00'))
            position_age_hours = (datetime.now() - entry_time).total_seconds() / 3600
            factors['position_age_score'] = min(1.0, position_age_hours / 24)  # Max score after 24 hours
            
            # 2. Volatility adaptation
            atr = df['close'].rolling(14).apply(lambda x: np.mean(np.abs(np.diff(x)))).iloc[-1]
            atr_pct = (atr / current_price) * 100
            factors['volatility_adapted'] = atr_pct > 0.5  # Higher volatility = more suitable for trailing
            
            # 3. Trend consistency
            sma_20 = df['close'].rolling(20).mean().iloc[-1]
            sma_50 = df['close'].rolling(50).mean().iloc[-1] if len(df) >= 50 else sma_20
            trend_alignment = (current_price > sma_20 > sma_50) if direction.upper() == 'BUY' else (current_price < sma_20 < sma_50)
            factors['trend_consistency'] = trend_alignment
            
            # 4. Recent momentum
            recent_returns = df['close'].pct_change().tail(5).mean()
            momentum_favorable = (recent_returns > 0) if direction.upper() == 'BUY' else (recent_returns < 0)
            factors['momentum_favorable'] = momentum_favorable
            
            # 5. Symbol-specific thresholds
            symbol_config = ENTRY_TP_SL_CONFIG.get(symbol, {})
            min_profit_threshold = symbol_config.get('min_trailing_profit', 0.015)  # Default 1.5%
            
            if direction.upper() == 'BUY':
                current_profit_pct = (current_price - entry_price) / entry_price
            else:
                current_profit_pct = (entry_price - current_price) / entry_price
                
            factors['meets_symbol_threshold'] = current_profit_pct >= min_profit_threshold
            factors['current_profit_pct'] = current_profit_pct
            
            return factors
            
        except Exception as e:
            print(f"❌ [Master Agent] Error calculating enhanced factors for {symbol}: {e}")
            return {'error': str(e)}

    def _combine_trailing_decisions(self, base_decision, enhanced_factors, symbol):
        """Combine base Master Agent decision with enhanced factors"""
        try:
            # Start with base decision
            should_activate = base_decision.get('should_activate', False)
            reasons = list(base_decision.get('reasons', []))
            confidence = base_decision.get('confidence', 0.5)
            
            # Apply enhanced factors
            if enhanced_factors.get('error'):
                return base_decision
            
            enhancement_score = 0
            max_enhancements = 5
            
            # Factor 1: Position age
            if enhanced_factors.get('position_age_score', 0) > 0.3:
                enhancement_score += 1
                reasons.append("Position matured for trailing")
            
            # Factor 2: Volatility
            if enhanced_factors.get('volatility_adapted', False):
                enhancement_score += 1
                reasons.append("Volatility favorable for trailing")
            
            # Factor 3: Trend consistency
            if enhanced_factors.get('trend_consistency', False):
                enhancement_score += 1
                reasons.append("Strong trend alignment")
            
            # Factor 4: Momentum
            if enhanced_factors.get('momentum_favorable', False):
                enhancement_score += 1
                reasons.append("Recent momentum supports direction")
            
            # Factor 5: Symbol threshold
            if enhanced_factors.get('meets_symbol_threshold', False):
                enhancement_score += 1
                reasons.append("Meets symbol-specific profit threshold")
            
            # Enhanced decision logic
            enhancement_ratio = enhancement_score / max_enhancements
            
            # If base decision was positive, enhance confidence
            if should_activate and enhancement_ratio >= 0.4:
                confidence = min(0.95, confidence + (enhancement_ratio * 0.2))
                reasons.append("Enhanced confidence via multiple factors")
            
            # If base decision was negative but enhancements are strong, override
            elif not should_activate and enhancement_ratio >= 0.6 and enhanced_factors.get('meets_symbol_threshold', False):
                should_activate = True
                confidence = 0.7
                reasons = ["Override: Strong enhancement factors"] + reasons[-3:]  # Keep last 3 reasons
            
            return {
                'should_activate': should_activate,
                'reasons': reasons,
                'current_profit_pct': enhanced_factors.get('current_profit_pct', 0),
                'recommended_distance': base_decision.get('recommended_distance', 0),
                'confidence': confidence,
                'enhancement_score': enhancement_score,
                'enhancement_ratio': enhancement_ratio
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error combining decisions for {symbol}: {e}")
            return base_decision

    def _activate_trailing_stop_for_position(self, symbol, position, current_price, entry_price, direction, trailing_decision):
        """Activate trailing stop for a specific position with comprehensive tracking"""
        try:
            print(f"🎯 [Master Agent] Activating trailing stop for {symbol}...")
            
            # Calculate TP progress for display
            tp_target = position.get('tp', 0)
            tp_progress_pct = 0
            
            if tp_target > 0:
                if direction.upper() == 'BUY':
                    total_distance_to_tp = tp_target - entry_price
                    current_distance = current_price - entry_price
                else:
                    total_distance_to_tp = entry_price - tp_target
                    current_distance = entry_price - current_price
                
                if total_distance_to_tp != 0:
                    tp_progress_pct = max(0, min(100, (current_distance / total_distance_to_tp) * 100))
            
            # Update position with trailing stop information
            position['trailing_stop'] = True
            position['trailing_distance'] = trailing_decision['recommended_distance']
            position['trailing_stop_price'] = self._calculate_trailing_stop_price(
                current_price, direction, trailing_decision['recommended_distance']
            )
            position['trailing_activated_at'] = datetime.now().isoformat()
            position['trailing_activation_price'] = current_price
            position['trailing_confidence'] = trailing_decision.get('confidence', 0.5)
            
            # Store original SL for comparison
            original_sl = position.get('initial_sl', position.get('sl', 0))
            if not position.get('initial_sl'):
                position['initial_sl'] = original_sl
            
            # Update the actual stop loss to the trailing stop price
            position['sl'] = position['trailing_stop_price']
            
            # Save updated positions
            save_open_positions(self.open_positions)
            
            # Format values for display
            if symbol.startswith('BTC') or symbol.startswith('ETH') or 'USD' in symbol:
                original_sl_formatted = f"{original_sl:.0f}"
                new_trailing_sl_formatted = f"{position['trailing_stop_price']:.0f}"
                trailing_distance_formatted = f"{trailing_decision['recommended_distance']:.0f}"
            else:
                original_sl_formatted = f"{original_sl:.5f}"
                new_trailing_sl_formatted = f"{position['trailing_stop_price']:.5f}"
                trailing_distance_formatted = f"{trailing_decision['recommended_distance']:.5f}"
            
            # Console output
            print(f"   💰 Current Price: {current_price:.5f}")
            print(f"   📈 Profit: {trailing_decision['current_profit_pct']:.2%} ({tp_progress_pct:.1f}% to TP)")
            print(f"   📏 Trailing Distance: {trailing_distance_formatted}")
            print(f"   🎯 SL Range: {original_sl_formatted} → {new_trailing_sl_formatted}")
            print(f"   🔍 Confidence: {trailing_decision.get('confidence', 0.5):.1%}")
            print(f"   ✅ Reasons: {', '.join(trailing_decision['reasons'])}")
            
            # Return activation info for summary
            return {
                'symbol': symbol,
                'direction': direction,
                'current_profit_pct': trailing_decision['current_profit_pct'],
                'tp_progress_pct': tp_progress_pct,
                'trailing_distance': trailing_decision['recommended_distance'],
                'sl_range': f"{original_sl_formatted} → {new_trailing_sl_formatted}",
                'reasons': trailing_decision['reasons'],
                'confidence': trailing_decision.get('confidence', 0.5),
                'current_price': current_price
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error activating trailing stop for {symbol}: {e}")
            return None

    def _send_trailing_stop_summary_notification(self, activations):
        """Send comprehensive summary of all trailing stop activations"""
        try:
            if not activations:
                return
            
            # Create detailed summary message
            message = f"🎯 **Master Agent Trailing Stop Activated**\n\n"
            message += f"📊 **Activated {len(activations)} trailing stop{'s' if len(activations) > 1 else ''}:**\n\n"
            
            for i, activation in enumerate(activations, 1):
                message += f"**{i}. {activation['symbol']}**\n"
                message += f"**Direction:** {activation['direction']}\n"
                message += f"**Current Profit:** {activation['current_profit_pct']:.2%} ({activation['tp_progress_pct']:.1f}% to TP)\n"
                message += f"**Trailing Distance:** {activation['trailing_distance']:.5f}\n"
                message += f"**SL Range:** {activation['sl_range']}\n"
                message += f"**Confidence:** {activation['confidence']:.1%}\n"
                message += f"**Reasons:** {', '.join(activation['reasons'][:2])}\n\n"  # Show top 2 reasons
            
            # Add summary statistics
            avg_profit = sum(a['current_profit_pct'] for a in activations) / len(activations)
            avg_confidence = sum(a['confidence'] for a in activations) / len(activations)
            
            message += f"📈 **Summary:**\n"
            message += f"Average Profit: {avg_profit:.2%}\n"
            message += f"Average Confidence: {avg_confidence:.1%}\n"
            message += f"🤖 Master Agent System Active"
            
            # Send the notification
            self.send_discord_alert(message)
            
        except Exception as e:
            print(f"❌ [Master Agent] Error sending summary notification: {e}")
            # Send individual notifications as fallback
            for activation in activations:
                try:
                    self.send_discord_alert(
                        f"🎯 **Master Agent Trailing Stop Activated**\n\n"
                        f"**Symbol:** {activation['symbol']}\n"
                        f"**Direction:** {activation['direction']}\n"
                        f"**Current Profit:** {activation['current_profit_pct']:.2%} ({activation['tp_progress_pct']:.1f}% to TP)\n"
                        f"**Trailing Distance:** {activation['trailing_distance']:.5f}\n"
                        f"**SL Range:** {activation['sl_range']}\n"
                        f"**Reasons:** {', '.join(activation['reasons'][:3])}"
                    )
                except:
                    continue

    def _log_trailing_stop_activations(self, activations):
        """Log comprehensive trailing stop activation details"""
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # Log to main log file
            logging.info(f"🎯 [Master Agent Trailing] {len(activations)} trailing stops activated at {timestamp}")
            
            for activation in activations:
                logging.info(
                    f"   {activation['symbol']}: {activation['direction']} | "
                    f"Profit: {activation['current_profit_pct']:.2%} | "
                    f"Distance: {activation['trailing_distance']:.5f} | "
                    f"SL: {activation['sl_range']} | "
                    f"Confidence: {activation['confidence']:.1%}"
                )
            
            # Create detailed log entry for analysis
            log_entry = {
                'timestamp': timestamp,
                'event_type': 'trailing_stop_activation',
                'activations': activations,
                'total_count': len(activations),
                'avg_profit': sum(a['current_profit_pct'] for a in activations) / len(activations),
                'avg_confidence': sum(a['confidence'] for a in activations) / len(activations)
            }
            
            # Store in memory for performance tracking
            if not hasattr(self, 'trailing_stop_history'):
                self.trailing_stop_history = []
            
            self.trailing_stop_history.append(log_entry)
            
            # Keep only last 100 entries to manage memory
            if len(self.trailing_stop_history) > 100:
                self.trailing_stop_history = self.trailing_stop_history[-100:]
                
            print(f"📝 [Master Agent] Logged {len(activations)} trailing stop activations")
            
        except Exception as e:
            print(f"❌ [Master Agent] Error logging trailing stop activations: {e}")

    def _log_trailing_stop_analysis_summary(self):
        """Log summary when no trailing stops are activated"""
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            if not self.open_positions:
                logging.info(f"⏳ [Master Agent Trailing] No open positions to analyze at {timestamp}")
                return
            
            # Analyze why no trailing stops were activated
            analysis_summary = {
                'total_positions': len(self.open_positions),
                'positions_with_trailing': sum(1 for pos in self.open_positions.values() if pos.get('trailing_stop', False)),
                'positions_analyzed': 0,
                'profit_threshold_failures': 0
            }
            
            for symbol, position in self.open_positions.items():
                if position.get('trailing_stop', False):
                    continue  # Skip positions that already have trailing
                    
                analysis_summary['positions_analyzed'] += 1
                
                # Quick analysis of why trailing wasn't activated
                entry_price = position.get('entry_price', 0)
                direction = position.get('signal', 'BUY')
                
                if entry_price > 0:
                    # Simulate current price check (simplified)
                    try:
                        current_data = self.data_manager.fetch_multi_timeframe_data(symbol, count=5)
                        if current_data:
                            primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, PRIMARY_TIMEFRAME)
                            df = current_data.get(primary_tf)
                            if df is not None and not df.empty:
                                current_price = df['close'].iloc[-1]
                                
                                if direction.upper() == 'BUY':
                                    profit_pct = (current_price - entry_price) / entry_price
                                else:
                                    profit_pct = (entry_price - current_price) / entry_price
                                
                                symbol_config = ENTRY_TP_SL_CONFIG.get(symbol, {})
                                min_profit_threshold = symbol_config.get('min_trailing_profit', 0.015)
                                
                                if profit_pct < min_profit_threshold:
                                    analysis_summary['profit_threshold_failures'] += 1
                    except:
                        pass
            
            logging.info(
                f"⏳ [Master Agent Trailing] Analysis summary at {timestamp}: "
                f"{analysis_summary['positions_analyzed']} positions analyzed, "
                f"{analysis_summary['profit_threshold_failures']} below profit threshold, "
                f"{analysis_summary['positions_with_trailing']} already have trailing"
            )
            
            print(f"📊 [Master Agent] Analysis complete: {analysis_summary['positions_analyzed']} positions checked")
            
        except Exception as e:
            print(f"❌ [Master Agent] Error logging analysis summary: {e}")

    def get_trailing_stop_performance_stats(self):
        """Get comprehensive trailing stop performance statistics"""
        try:
            if not hasattr(self, 'trailing_stop_history') or not self.trailing_stop_history:
                return {
                    'total_activations': 0,
                    'avg_profit_at_activation': 0,
                    'avg_confidence': 0,
                    'symbols_analysis': {},
                    'recent_activity': 'No trailing stop history available'
                }
            
            # Calculate comprehensive statistics
            total_activations = sum(entry['total_count'] for entry in self.trailing_stop_history)
            
            all_activations = []
            for entry in self.trailing_stop_history:
                all_activations.extend(entry['activations'])
            
            if not all_activations:
                return {'total_activations': 0, 'message': 'No activation data available'}
            
            # Symbol-wise analysis
            symbol_stats = {}
            for activation in all_activations:
                symbol = activation['symbol']
                if symbol not in symbol_stats:
                    symbol_stats[symbol] = {
                        'count': 0,
                        'avg_profit': 0,
                        'avg_confidence': 0,
                        'profits': [],
                        'confidences': []
                    }
                
                symbol_stats[symbol]['count'] += 1
                symbol_stats[symbol]['profits'].append(activation['current_profit_pct'])
                symbol_stats[symbol]['confidences'].append(activation['confidence'])
            
            # Calculate averages for each symbol
            for symbol, stats in symbol_stats.items():
                stats['avg_profit'] = sum(stats['profits']) / len(stats['profits'])
                stats['avg_confidence'] = sum(stats['confidences']) / len(stats['confidences'])
                del stats['profits']  # Clean up raw data
                del stats['confidences']
            
            recent_entry = self.trailing_stop_history[-1] if self.trailing_stop_history else None
            
            return {
                'total_activations': total_activations,
                'total_sessions': len(self.trailing_stop_history),
                'avg_profit_at_activation': sum(a['current_profit_pct'] for a in all_activations) / len(all_activations),
                'avg_confidence': sum(a['confidence'] for a in all_activations) / len(all_activations),
                'symbols_analysis': symbol_stats,
                'recent_activity': recent_entry['timestamp'] if recent_entry else 'No recent activity',
                'most_active_symbol': max(symbol_stats.keys(), key=lambda x: symbol_stats[x]['count']) if symbol_stats else 'None'
            }
            
        except Exception as e:
            print(f"❌ [Master Agent] Error getting performance stats: {e}")
            return {'error': str(e)}
    
    def _calculate_trailing_stop_price(self, current_price, direction, trailing_distance):
        """Calculate trailing stop price based on direction and distance"""
        if direction.upper() == 'BUY':
            return current_price - trailing_distance
        else:
            return current_price + trailing_distance

    def calculate_price_action_score(self, df_features):
        """
        Calculate a comprehensive score BASED ON MULTIPLE Price Action factors.
        Returns score from -1.0 (extremely bearish) to +1.0 (extremely bullish).
        """
        if df_features.empty:
            return 0.0

        latest = df_features.iloc[-1]
        score = 0.0
        weights = {
            "sd_zone": 0.35,
            "engulfing_pattern": 0.25,
            "trend_structure": 0.20,
            "volume_confirmation": 0.10,
            "rsi_convergence": 0.10,
        }

        # 1. i m tVng cung/C u (quan tempty nh t)
        sd_score = 0.0
        if latest.get("in_demand_reversal"):
            sd_score += 1.0
        if latest.get("in_demand_continuation"):
            sd_score += 0.5
        if latest.get("in_supply_reversal"):
            sd_score -= 1.0
        if latest.get("in_supply_continuation"):
            sd_score -= 0.5
        score += sd_score * weights["sd_zone"]

        # 2. i m tlevelonfigurationn n receive engulfing (Engulfing)
        engulfing_score = 0.0
        if latest.get("bullish_engulfing"):
            engulfing_score += 1.0
        if latest.get("bearish_engulfing"):
            engulfing_score -= 1.0
        score += engulfing_score * weights["engulfing_pattern"]

        # 3. i m tC u trfromhtru ng (Higher Highs / Lower Lows)
        trend_strength = latest.get("trend_strength", 0)  # T-5 dn 5
        score += (trend_strength / 5.0) * weights["trend_structure"]

        # 4. Xcreceive b i Volume
        volume_ratio = latest.get("volume_ratio", 1.0)
        if (engulfing_score > 0 or sd_score > 0) and volume_ratio > 1.5:
            score += 1.0 * weights["volume_confirmation"]
        if (engulfing_score < 0 or sd_score < 0) and volume_ratio > 1.5:
            score -= 1.0 * weights["volume_confirmation"]

        # 5. H i twith RSI
        rsi = latest.get(f"rsi_{PRIMARY_TIMEFRAME}", 50)
        # if t+n hi+u mua v RSI v a thot kh i vng qu bn
        if (sd_score > 0 or engulfing_score > 0) and (20 < rsi < 40):
            score += 1.0 * weights["rsi_convergence"]
        # if t+n hi+u bn v RSI v a thot kh i vng qu mua
        if (sd_score < 0 or engulfing_score < 0) and (60 < rsi < 80):
            score -= 1.0 * weights["rsi_convergence"]

        return max(-1.0, min(1.0, score))

    def _get_currency_exposures(self, symbol, signal):
        """
        analysis m t giao dh thnh mc dnh datati p xc with total used money.
        V d : SELL EURUSD -> {'EUR': 'SHORT', 'USD': 'LONG'}
        """
        if not isinstance(symbol, str) or len(symbol) < 6:
            return {}

        base, quote = symbol[:3].upper(), symbol[3:].upper()

        if signal.upper() == "BUY":
            return {base: "LONG", quote: "SHORT"}
        elif signal.upper() == "SELL":
            return {base: "SHORT", quote: "LONG"}
        return {}

    def _execute_sideways_strategy(self, symbol):
        """
        Execute sideways trading strategy.
        Logic: Buy near support, sell near resistance.
        """
        print(
            f"[{symbol}] [Sideway Strategy]  Analyzing with sideways strategy..."
        )

        df_features = self.data_manager.create_enhanced_features(symbol)
        if df_features is None or df_features.empty:
            return

        latest = df_features.iloc[-1]
        signal = None
        confidence = 0.0

        # diu kin MUA: G n htrv RSI v a di ln tvng qu bn
        is_near_support = latest.get("near_support", False)
        rsi_m15 = latest.get(f"rsi_{PRIMARY_TIMEFRAME}", 50)

        if is_near_support and 30 < rsi_m15 < 45:
            signal = "BUY"
            # Confidence fromhd a vo vi c gi g n htrdn mneedso
            confidence = 0.75 + (0.10 * (1 - latest.get("close_position_20", 1)))

        # diu kin BN: G n not cv RSI v a di xu ng tvng qu mua
        is_near_resistance = latest.get("near_resistance", False)
        if is_near_resistance and 55 < rsi_m15 < 70:
            signal = "SELL"
            confidence = 0.75 + (0.10 * latest.get("close_position_20", 0))

        if signal and confidence >= ML_CONFIG["MIN_CONFIDENCE_TRADE"]:
            print(
                f"[{symbol}] [Sideway Strategy] T+n hi+u h+p l+ : {signal} with Confidence {confidence:.2%}"
            )
            self.handle_position_logic(symbol, signal, confidence)
        else:
            print(f"[{symbol}] [Sideway Strategy]  not t+n hi+u h+p l+ .")
    # TM V THAY THFunction this in L P EnhancedTradingBot

    # EnhancedTradingBofrom modelethods

    async def gather_trade_reasoning(self, symbol, signal, confidence):
        """
        NNG C P: L y lu n di m giao dh bt du bv Using used Function l y tin t c.
        """
        try:
            # L y data features (Function this not needs async)
            df_features = self.data_manager.create_enhanced_features(symbol)
            if df_features is None or df_features.empty:
                return {"Error": "Cannot get feature data."}

            latest = df_features.iloc[-1]
            current_regime = latest.get('market_regime', 0)

            # L y  nh hu ng of feature from model suitable
            feature_influence_text = "not x l data nh"
            model_data = self.trending_models.get(symbol) if current_regime != 0 else self.ranging_models.get(symbol)
            if model_data:
                ensemble_model = model_data.get('ensemble')
                if ensemble_model and hasattr(ensemble_model, 'get_base_model_feature_influence'):
                    feature_influence_text = ensemble_model.get_base_model_feature_influence()

            # L y tm t t xu hu ng tcfromF cao hon
            trend_summary_parts = []
            for htf in ["H4", "D1"]:
                if f"trend_{htf}" in latest:
                    trend = "Tang" if latest[f'trend_{htf}'] == 1 else "Gi m"
                    trend_summary_parts.append(f"{htf}: {trend}")
            trend_summary = ", ".join(trend_summary_parts) if trend_summary_parts else "not x l data nh"

            # *** PH N fix L I CHNH ***
            # G i Function get_aggregated_news bt du bdl y tin t c
            news_items = await self.news_manager.get_aggregated_news(symbol)

            reasoning = {
                "Main Signal": f"{signal} with Confidence {confidence:.2%}",
                "Main Technical Factors": feature_influence_text,
                "Trend Analysis": trend_summary,
                "Related News": f"Found {len(news_items)} latest news articles."
            }
            return reasoning

        except Exception as e:
            import traceback
            print(f"Li in gather_trade_reasoning: {e}\n{traceback.format_exc()}")
            return {"Li": str(e)}
  # (TFunction Function this vo in l p EnhancedTradingBot)
    # TM V THAY THFunction this in L P EnhancedTradingBot

    async def consult_master_agent(self, symbol, signal, reasoning_data, news_items):
        """
        NNG C P: G p 2 nhi m v(analysis sentiment + ph duy t) vo 1 API call.
         chuy n sang async with logging chi ti t.
        """
        print(f"\n [Master Agent] ===== Analysis for {signal} {symbol} =====")
        
        # Extract confidence from reasoning data
        confidence = 0.5  # Default confidence
        if "Main Signal" in reasoning_data:
            confidence_text = reasoning_data["Main Signal"]
            if "Confidence" in confidence_text:
                try:
                    confidence = float(confidence_text.split("Confidence ")[1].split("%")[0]) / 100
                except:
                    confidence = 0.5
        
        # Check LLM analyzer
        if not self.news_manager.llm_analyzer or not self.news_manager.llm_analyzer.model:
            print(f" [Master Agent] LLM Analyzer not khused!")
            print(f"   - news_manager.llm_analyzer: {self.news_manager.llm_analyzer is not None}")
            if self.news_manager.llm_analyzer:
                print(f"   - llm_analyzer.model: {self.news_manager.llm_analyzer.model is not None}")
            
            # Fallback logic based on confidence
            if confidence > 0.5:
                print(f"[Master Agent] Fallback: Auto-approve due to sin technical signal (confidence: {confidence:.2%})")
                return {"decision": "APPROVE", "justification": "Sin technical signal, auto-approved (LLM notoperational)", "sentiment_score": 0.0}
            else:
                print(f" [Master Agent] Fallback: Auto-reject due to weak technical signal (confidence: {confidence:.2%})")
                return {"decision": "REJECT", "justification": "Weak technical signal, auto-rejected (LLM notoperational)", "sentiment_score": 0.0}

        print(f"[Master Agent] LLM Analyzer ready, starting analysis...")

        # Format data d u vo
        formatted_news = "\n".join([f"- {item.get('title', '')}" for item in news_items[:5]])
        if not formatted_news: 
            formatted_news = "No relevant news found."
            print(f" [Master Agent] News: {len(news_items)} articles")
        else:
            print(f" [Master Agent] News: {len(news_items)} articles")
            print(f"   - Top headline: {news_items[0].get('title', 'N/A')[:50]}...")
        
        reasoning_text = "\n".join([f"- {key}: {value}" for key, value in reasoning_data.items()])
        print(f" [Master Agent] Technical arguments:")
        for key, value in list(reasoning_data.items())[:3]:  # Show first 3 items
            print(f"   - {key}: {value}")
        
        # Tạo prompt chi tiết với logic cân bằng hơn
        prompt = f"""
Bạn là Giám đốc Quản lý Rủi ro (CRO) của một quỹ đầu tư.
Một hệ thống AI cấp dưới đã xuất tín hiệu giao dịch: {signal} {symbol}.

NHIỆM VỤ 1: PHÂN TÍCH TIN TỨC
Related News:
---
{formatted_news}
---

NHIỆM VỤ 2: XEM XÉT LUẬN ĐIỂM KỸ THUẬT
{reasoning_text}

NHIỆM VỤ 3: RA QUYẾT ĐỊNH CUỐI CÙNG
Dựa trên phân tích, hãy đưa ra quyết định cuối cùng.

**QUAN TRỌNG:** 
- Nếu không có tin tức tiêu cực và tín hiệu kỹ thuật mạnh (confidence > 50%), nên APPROVE
- Chỉ REJECT khi có tin tức tiêu cực rõ ràng hoặc tín hiệu kỹ thuật yếu
- Crypto markets thường giao dịch dựa trên technical analysis khi không có news

Chỉ trả về duy nhất một khối JSON với định dạng sau:
{{
  "sentiment_score": <số float từ -1.0 đến 1.0 dựa trên tin tức>,
  "decision": "APPROVE" hoặc "REJECT",
  "justification": "<Lý do ngắn gọn cho quyết định của bạn>"
}}
"""

        print(f" [Master Agent] Calling analysis request to LLM...")
        
        for attempt in range(3):
            try:
                print(f"    [Master Agent] Attempt {attempt + 1}/3...")
                
                # Using generate_content_async cho hoạt động bất đồng bộ 
                response = await self.news_manager.llm_analyzer.model.generate_content_async(prompt)
                json_text = response.text.strip().replace('```json', '').replace('```', '')
                
                print(f"    [Master Agent] Response from LLM: {json_text[:100]}...")
                
                # Kiểm tra JSON text trước khi parse
                if not json_text or json_text.strip() == "":
                    print(f"    [Master Agent] Empty JSON response, retrying...")
                    await asyncio.sleep(2)
                    continue
                
                # Th parse JSON vi error handling tt hon
                try:
                    decision_data = json.loads(json_text)
                except json.JSONDecodeError as json_error:
                    print(f"    [Master Agent] Invalid JSON format: {json_error}")
                    print(f"    [Master Agent] Raw response: {json_text[:200]}...")
                    await asyncio.sleep(2)
                    continue
                
                print(f"[Master Agent] Analysis completed!")
                print(f"   - Decision: {decision_data.get('decision')}")
                print(f"   - Sentiment Score: {decision_data.get('sentiment_score', 0):.2f}")
                print(f"   - Reason: {decision_data.get('justification', 'N/A')}")
                
                # Ensure sentiment_score is properly formatted
                sentiment_score = decision_data.get('sentiment_score', 0.0)
                if isinstance(sentiment_score, str):
                    try:
                        sentiment_score = float(sentiment_score)
                    except:
                        sentiment_score = 0.0
                decision_data['sentiment_score'] = sentiment_score
                
                return decision_data
                
            except json.JSONDecodeError as e:
                print(f"    [Master Agent] Li parse JSON (Ln {attempt+1}/3): {e}")
                print(f"    [Master Agent] Response text: {response.text[:200]}...")
                await asyncio.sleep(2)
                
            except Exception as e:
                print(f"    [Master Agent] Li (Ln {attempt+1}/3): {e}")
                await asyncio.sleep(2)

        print(f" [Master Agent] Cannot get valid response from LLM after 3 attempts")
        print(f" [Master Agent] Tused ph duy t l nh do l i processing")
        
        # Fallback logic: Auto-approve if technical signalfixre sin
        if confidence > 0.5:  # High confidence technical signal
            print(f"[Master Agent] Fallback: Auto-approve due to sin technical signal (confidence: {confidence:.2%})")
            return {"decision": "APPROVE", "justification": "Sin technical signal, auto-approved due to LLM error", "sentiment_score": 0.0}
        else:
            print(f" [Master Agent] Fallback: Auto-reject due to weak technical signal (confidence: {confidence:.2%})")
            return {"decision": "REJECT", "justification": "Weak technical signal, auto-rejected due to LLM error", "sentiment_score": 0.0}
    # TFunction Function M I this VO BN in L P EnhancedTradingBot

    async def wait_until_top_of_the_hour(self):
        """
        Calculate and wait until exactly XX:00:00 of the next hour.
        """
        # Using mi giVi t Nam dlog cho dhi u
        now = datetime.now(pytz.timezone("Asia/Bangkok"))

        # Calculate next hour start time
        next_hour = (now + timedelta(hours=1)).replace(minute=0, second=0, microsecond=0)

        wait_seconds = (next_hour - now).total_seconds()

        #  m b o lun engulfing t kho ng th i gian duong
        if wait_seconds <= 0:
            wait_seconds += 3600

        print(f"Đang báo. Còn {int(wait_seconds // 60)} phút {int(wait_seconds % 60)} giây cho đến {next_hour.strftime('%H:%M:%S')}...")
        await asyncio.sleep(wait_seconds)
class DriftMonitor:
    """
    Theo di sthay d i (drift) in phn must data gi a t p training
    v data live equal allh Using Population Stability Index (PSI).
    """
    def __init__(self, X_train_reference):
        """
        Initialize with DataFrame used as reference (training data).
        """
        self.reference_data = X_train_reference
        self.reference_bins = {}
        self.top_features = X_train_reference.columns.tolist()

        # T o all "bin" (ngu ng phn chia) cho total feature d a trn data thalevelhi u
        for col in self.top_features:
            # Using quantiles dchia data thnh 10 ph n equal nhau
            self.reference_bins[col] = pd.qcut(self.reference_data[col], 10, labels=False, duplicates='drop')

    def calculate_psi(self, feature_name, live_data_column):
        """
        Calculate PSI index for a specific feature.
        """
        # L y all bin d dufromnh ton trufromdata thalevelhi u
        reference_dist = self.reference_bins[feature_name].value_counts(normalize=True).sort_index()

        # p used all bin this ln data live
        live_bins = pd.qcut(live_data_column, bins=pd.qcut(self.reference_data[feature_name], 10, retbins=True, duplicates='drop')[1], labels=False)
        live_dist = live_bins.value_counts(normalize=True).sort_index()

        # K t h p hai phn must dd m b o chng c cng all bin
        psi_df = pd.DataFrame({'reference': reference_dist, 'live': live_dist}).fillna(0.0001) # i n 0.0001 dtrnh l i chia cho 0

        # Calculate PSI
        psi_df['psi'] = (psi_df['live'] - psi_df['reference']) * np.log(psi_df['live'] / psi_df['reference'])

        return psi_df['psi'].sum()

    def check_drift(self, X_live, psi_threshold=0.25):
        """
        Check drift trn ton ball feature quan tempty.
        Trvdanh sch all feature d b"drift".
        """
        drifted_features = {}
        if len(X_live) < 20: # needs ddata live dso snh
            return None

        for feature in self.top_features:
            if feature in X_live.columns:
                psi_score = self.calculate_psi(feature, X_live[feature])

                # Quy t c chung:
                # PSI < 0.1: not fromhay d i used k 
                # 0.1 <= PSI < 0.25: Thay d i nh 
                # PSI >= 0.25: Thay d i l n, needs xem xt l i model
                if psi_score >= psi_threshold:
                    drifted_features[feature] = psi_score

        return drifted_features
# <<< TFunction L P M I this VO FILE BOT >>>

class AdvancedRiskManager:
    """Enhanced risk management with asset class specific configurations"""

    def __init__(self):
        # Risk limits
        self.daily_var_limit = PORTFOLIO_RISK_LIMITS["daily_var_limit"]
        self.weekly_var_limit = PORTFOLIO_RISK_LIMITS["weekly_var_limit"]

        # Correlation matrix for portfolio risk
        self.correlation_matrix = {}

        # Position sizing limits
        self.max_position_size_percent = 0.1  # 10% max per position
        self.max_portfolio_exposure = 0.8  # 80% max total exposure
    def _initialize_symbol_risk_configs(self):
        """Initialize risk configurations for each symbol"""
        configs = {}
        for symbol in SYMBOLS:
            metadata = SYMBOL_METADATA.get(symbol, {})
            asset_class = metadata.get("asset_class", "equity_index")
            base_config = RISK_CONFIG_BY_ASSET_CLASS.get(asset_class, RISK_CONFIG_BY_ASSET_CLASS["equity_index"])

            # Symbol-specifiofdjustments
            symbol_config = base_config.copy()

            # Adjust for volatility profile
            volatility_profile = metadata.get("volatility_profile", "medium")
            if volatility_profile == "very_high":
                symbol_config["max_position_size"] *= 0.7
                symbol_config["var_multiplier"] *= 1.3
                symbol_config["stop_loss_atr"] *= 1.2
            elif volatility_profile == "high":
                symbol_config["max_position_size"] *= 0.8
                symbol_config["var_multiplier"] *= 1.1
            elif volatility_profile == "low":
                symbol_config["max_position_size"] *= 1.2
                symbol_config["var_multiplier"] *= 0.9
                symbol_config["stop_loss_atr"] *= 0.8

            # Adjust for region/session
            region = metadata.get("region", "US")
            if region in ["Asia", "Europe"]:
                symbol_config["session_risk_adjustment"] = True

            configs[symbol] = symbol_config

        return configs

    def _get_symbol_risk_config(self, symbol):
        """Get risk configuration for specific symbol"""
        return self.symbol_risk_configs.get(symbol, RISK_CONFIG_BY_ASSET_CLASS["equity_index"])

    def calculate_asset_class_exposure(self, positions):
        """Calculate exposure by asset class"""
        exposure = {}
        for position in positions:
            symbol = position['symbol']
            metadata = SYMBOL_METADATA.get(symbol, {})
            asset_class = metadata.get("asset_class", "equity_index")

            if asset_class not in exposure:
                exposure[asset_class] = 0
            exposure[asset_class] += position.get('weight', 0)

        self.asset_class_exposure = exposure
        return exposure

    def check_asset_class_limits(self, symbol, existing_positions):
        """Check if adding position would violate asset class limits"""
        metadata = SYMBOL_METADATA.get(symbol, {})
        asset_class = metadata.get("asset_class", "equity_index")

        # Calculate current exposure
        current_exposure = self.calculate_asset_class_exposure(existing_positions)
        current_asset_exposure = current_exposure.get(asset_class, 0)

        # Get limits
        max_asset_class_exposure = PORTFOLIO_RISK_LIMITS["max_asset_class_exposure"]

        violations = []
        if current_asset_exposure >= max_asset_class_exposure:
            violations.append({
                'type': 'asset_class_limit',
                'asset_class': asset_class,
                'current_exposure': current_asset_exposure,
                'limit': max_asset_class_exposure
            })

        return len(violations) == 0, violations

    def calculate_symbol_specific_var(self, symbol, position_size, returns_history):
        """Calculate VaR specififromo symbol characteristics"""
        config = self._get_symbol_risk_config(symbol)

        if symbol not in returns_history or len(returns_history[symbol]) < 30:
            return 0.0

        symbol_returns = np.array(returns_history[symbol])

        # Apply asset class specific VaR multiplier
        var_multiplier = config["var_multiplier"]

        # Calculate VaR
        var = np.percentile(symbol_returns, 5) * var_multiplier * position_size

        return abs(var)

    def calculate_portfolio_var(self, positions, returns_history, confidence_level=0.05):
        """Calculate Portfolio Value at Risk using historical simulation with asset clasfixdjustments"""
        if len(returns_history) < 30:
            return 0.0

        portfolio_returns = []
        for position in positions:
            symbol = position['symbol']
            symbol_returns = returns_history.get(symbol, [])
            if len(symbol_returns) > 0:
                # Apply symbol-specific risk adjustments
                config = self._get_symbol_risk_config(symbol)
                adjusted_returns = np.array(symbol_returns) * position['weight'] * config["var_multiplier"]
                portfolio_returns.append(adjusted_returns)

        if not portfolio_returns:
            return 0.0

        # Sum portfolio returns
        total_portfolio_returns = np.sum(portfolio_returns, axis=0)

        # Calculate VaR at specifiedonfidence level
        var = np.percentile(total_portfolio_returns, confidence_level * 100)
        return abs(var)

    def calculate_correlation_matrix(self, returns_data):
        """Calculate correlation matrix with asset class grouping"""
        if len(returns_data) < 2:
            return {}

        symbols = list(returns_data.keys())
        n_symbols = len(symbols)
        correlation_matrix = np.zeros((n_symbols, n_symbols))

        for i, symbol1 in enumerate(symbols):
            for j, symbol2 in enumerate(symbols):
                if i == j:
                    correlation_matrix[i, j] = 1.0
                else:
                    returns1 = np.array(returns_data[symbol1])
                    returns2 = np.array(returns_data[symbol2])

                    if len(returns1) > 10 and len(returns2) > 10:
                        # Align lengths
                        min_len = min(len(returns1), len(returns2))
                        corr = np.corrcoef(returns1[-min_len:], returns2[-min_len:])[0, 1]
                        correlation_matrix[i, j] = corr if not np.isnan(corr) else 0.0

        # Store as dictionary for easy access
        correlation_dict = {}
        for i, symbol1 in enumerate(symbols):
            correlation_dict[symbol1] = {}
            for j, symbol2 in enumerate(symbols):
                correlation_dict[symbol1][symbol2] = correlation_matrix[i, j]

        self.correlation_matrix = correlation_dict
        return correlation_dict

    def check_correlation_limits(self, new_symbol, existing_positions):
        """Check if adding new position would violate correlation limits"""
        if new_symbol not in self.correlation_matrix:
            return True, []  # Allow if no correlation data

        config = self._get_symbol_risk_config(new_symbol)
        correlation_threshold = config["correlation_threshold"]

        violations = []
        for position in existing_positions:
            existing_symbol = position['symbol']
            if existing_symbol in self.correlation_matrix[new_symbol]:
                correlation = abs(self.correlation_matrix[new_symbol][existing_symbol])
                if correlation > correlation_threshold:
                    violations.append({
                        'symbol_pair': (new_symbol, existing_symbol),
                        'correlation': correlation,
                        'limit': correlation_threshold
                    })

        return len(violations) == 0, violations

    def calculate_optimal_position_size(self, symbol, account_balance, atr_value, confidence):
        """Calculate optimal position size based on symbol characteristics"""
        config = self._get_symbol_risk_config(symbol)
        metadata = SYMBOL_METADATA.get(symbol, {})

        # Base position size from risk config
        max_position_size = config["max_position_size"]

        # Adjust for confidence
        confidence_multiplier = min(confidence / 0.7, 1.0)  # Scale confidence to 0-1

        # Adjust for volatility profile
        volatility_profile = metadata.get("volatility_profile", "medium")
        if volatility_profile == "very_high":
            volatility_multiplier = 0.6
        elif volatility_profile == "high":
            volatility_multiplier = 0.8
        elif volatility_profile == "low":
            volatility_multiplier = 1.2
        else:
            volatility_multiplier = 1.0

        # Calculate position size
        base_size = account_balance * max_position_size * confidence_multiplier * volatility_multiplier

        # Apply ATR-based sizing
        pip_value = metadata.get("pip_value", 1.0)
        stop_loss_atr = config["stop_loss_atr"]
        stop_loss_pips = (atr_value * stop_loss_atr) / pip_value

        # Risk per trade (2% of account)
        risk_per_trade = account_balance * 0.02
        atr_based_size = risk_per_trade / stop_loss_pips

        # Use the smaller of the two
        optimal_size = min(base_size, atr_based_size)

        return optimal_size

    async def validate_trade(self, symbol, size, existing_positions, returns_history, account_balance):
        """Comprehensive trade validation with asset class specific checks"""
        validation_result = {
            'approved': True,
            'violations': [],
            'risk_metrics': {},
            'recommended_size': size
        }

        # Create hypothetical new positions list
        new_positions = existing_positions.copy()
        new_positions.append({'symbol': symbol, 'size': size, 'weight': size / account_balance})

        # 1. Asset class exposure check
        asset_class_ok, asset_class_violations = self.check_asset_class_limits(symbol, existing_positions)
        if not asset_class_ok:
            validation_result['approved'] = False
            validation_result['violations'].extend(asset_class_violations)

        # 2. Correlation check
        corr_ok, corr_violations = self.check_correlation_limits(symbol, existing_positions)
        if not corr_ok:
            validation_result['approved'] = False
            validation_result['violations'].extend(corr_violations)

        # 3. VaR check
        portfolio_var = self.calculate_portfolio_var(new_positions, returns_history)
        validation_result['risk_metrics']['portfolio_var'] = portfolio_var

        daily_var_limit = PORTFOLIO_RISK_LIMITS["daily_var_limit"]
        if portfolio_var > daily_var_limit:
            validation_result['approved'] = False
            validation_result['violations'].append({
                'type': 'var_limit',
                'var': portfolio_var,
                'limit': daily_var_limit
            })

        # 4. Position size check
        config = self._get_symbol_risk_config(symbol)
        max_position_size = config["max_position_size"]
        if size / account_balance > max_position_size:
            validation_result['approved'] = False
            validation_result['violations'].append({
                'type': 'position_size_limit',
                'size': size / account_balance,
                'limit': max_position_size
            })

        # 5. Send alerts for violations
        if not validation_result['approved']:
            violation_msg = f"Trade validation failed for {symbol}:\n"
            for violation in validation_result['violations']:
                violation_msg += f"- {violation}\n"
            await self.observability.send_discord_alert(violation_msg, "WARNING")

        return validation_result

class PortfolioRiskManager:
    """
    Portfolio risk management based on asset correlation.
    """
    def __init__(self, symbols, data_manager):
        self.symbols = symbols
        self.data_manager = data_manager
        self.correlation_matrix = None
        self.last_update_time = None

    def update_correlation_matrix(self, force_update=False):
        """
        Calculate and save correlation matrix based on daily returns.
        """
        now = datetime.now()

    # === Hybridorrelation: long (D1) + short (H1/H4) ===
    def _compute_corr(self, timeframe='D1', lookback=250):
        all_returns = {}
        for symbol in self.symbols:
            df_tf = self.data_manager.fetch_multi_timeframe_data(symbol, count=int(lookback), timeframes_to_use=[timeframe]).get(timeframe)
            if df_tf is not None and not df_tf.empty:
                all_returns[symbol] = df_tf['close'].pct_change().dropna()
        if not all_returns:
            return pd.DataFrame()
        # Align to common index
        ret_df = pd.DataFrame(all_returns).dropna(how='all').fillna(0)
        return ret_df.corr().clip(-1.0, 1.0)

    def update_correlation_hybrid(self, tf_long='D1', tf_short='H1', lb_long=250, lb_short=500, method='max', w_long=0.6):
        print(f"[Risk Manager] Hybridorr long={tf_long}/{lb_long}, short={tf_short}/{lb_short}, method={method}")
        corr_long = self._compute_corr(tf_long, lb_long)
        corr_short = self._compute_corr(tf_short, lb_short)
        # Ensure same axes
        symbols = sorted(set(corr_long.index).union(set(corr_short.index)))
        corr_long = corr_long.reindex(index=symbols, columns=symbols).fillna(0)
        corr_short = corr_short.reindex(index=symbols, columns=symbols).fillna(0)
        if method == 'max':
            self.corr_matrix = np.maximum(corr_long.values, corr_short.values)
            self.corr_symbols = symbols
        else:
            wl = float(w_long)
            ws = 1.0 - wl
            self.corr_matrix = (wl * corr_long.values) + (ws * corr_short.values)
            self.corr_symbols = symbols
        print("   [Hybrid] Corr matrix updated.")

    def get_corr_pair(self, a, b):
        try:
            i = self.corr_symbols.index(a)
            j = self.corr_symbols.index(b)
            return float(self.corr_matrix[i, j])
        except Exception:
            return 0.0

        # Chc p nh t 1 l n mi 24 gidti t ki m moneyguyn
        if not force_update and self.last_update_time and (now - self.last_update_time).total_seconds() < 24 * 3600:
            return

        print("[Risk Manager] C p nh from modela tr n tuong quan...")
        all_returns = {}
        for symbol in self.symbols:
            # L y data D1 dtnh tuong quan di h n
            df_d1 = self.data_manager.fetch_multi_timeframe_data(symbol, count=100, timeframes_to_use=["D1"]).get("D1")
            if df_d1 is not None and not df_d1.empty:
                all_returns[symbol] = df_d1['close'].pct_change().dropna()

        if not all_returns:
            print("[Risk Manager]  not has datali u dtnh tuong quan.")
            return

        returns_df = pd.DataFrame(all_returns).dropna()
        self.correlation_matrix = returns_df.corr()
        self.last_update_time = now
        print("[Risk Manager] Ma tr n tuong quan has d c p nh t.")
        # print(self.correlation_matrix) # Bcomment dxem ma tr n

    def get_adjusted_risk_factor(self, new_symbol, new_signal, open_positions, threshold=0.6):
        """
        Calculate risk adjustment factor for a new order.
        Factor < 1.0 if high correlation.
        """
        if self.correlation_matrix is None:
            return 1.0 # not yet c ma tr n, not adjustment

        risk_factor = 1.0
        max_correlation_impact = 0.0

        for open_pos in open_positions.values():
            open_symbol = open_pos['symbol']
            open_signal = open_pos['signal']

            if new_symbol == open_symbol: continue

            try:
                correlation = self.correlation_matrix.loc[new_symbol, open_symbol]
            except KeyError:
                continue

            # Check xem 2 l nh c cng hu ng hay not (v d : cng BUY Or cng SELL)
            # Hopublicu c hu ng if tuong quan l m
            is_same_direction_bet = (new_signal == open_signal and correlation > 0) or \
                                     (new_signal != open_signal and correlation < 0)

            if abs(correlation) > threshold and is_same_direction_bet:
                # Ghi receive mc dnh data nh hu ng of tuong quan m receiveh t
                max_correlation_impact = max(max_correlation_impact, abs(correlation))

        if max_correlation_impact > 0:
            # Gi m r i ro d a trn mc dnh datatuong quan
            # V d : corr=0.8 -> gi m 40% r i ro (1 - 0.8*0.5)
            # V d : corr=0.9 -> gi m 45% r i ro
            risk_factor = 1.0 - (max_correlation_impact * 0.5)
            print(f"[Risk Manager] Tuong quan cao d pht hi n ({max_correlation_impact:.2f}). Gi m r i ro l nh mi xu ng {risk_factor:.2f} l n.")

        return risk_factor
# <<< THAY THTON BFunction this equal PHIN B N  fix L I >>>

# Fixed version functions

def calculate_rsi_divergence_vectorized(df, window=14, lookback=60):
    """
    Xhas data nh phn kRSI equal phuong php vector ha, nhanh hon vng l p.
    """
    df['bearish_divergence'] = 0
    df['bullish_divergence'] = 0

    # Chprocessing trn `lookback` n n g n nh t dti t ki m th i gian
    df_slice = df.iloc[-lookback:].copy()

    # Tm allhas data nh v duc c c b 
    price_peaks_idx = argrelextrema(df_slice['high'].to_numpy(), np.greater_equal, order=5)[0]
    price_troughs_idx = argrelextrema(df_slice['low'].to_numpy(), np.less_equal, order=5)[0]
    rsi_peaks_idx = argrelextrema(df_slice['rsi_14'].to_numpy(), np.greater_equal, order=5)[0]

    # <<<  fix L I: Xa m t ch'l' bth a >>>
    rsi_troughs_idx = argrelextrema(df_slice['rsi_14'].to_numpy(), np.less_equal, order=5)[0]

    if len(price_peaks_idx) < 2 or len(rsi_peaks_idx) < 2 or \
       len(price_troughs_idx) < 2 or len(rsi_troughs_idx) < 2:
        return df # not ddnh/duc dso snh

    # -- Check phn kgi m (Bearish Divergence) --
    # Gi to dnh cao hon (Higher High)
    if df_slice['high'].iloc[price_peaks_idx[-1]] > df_slice['high'].iloc[price_peaks_idx[-2]]:
        # RSI to dnh th p hon (Lower High)
        if df_slice['rsi_14'].iloc[rsi_peaks_idx[-1]] < df_slice['rsi_14'].iloc[rsi_peaks_idx[-2]]:
            # nhn d liu t i v tr of dnh gi cu cng
            df.loc[df_slice.index[price_peaks_idx[-1]], 'bearish_divergence'] = 1

    # -- Check phn k (Bullish Divergence) --
    # Gi to duc th p hon (Lower Low)
    if df_slice['low'].iloc[price_troughs_idx[-1]] < df_slice['low'].iloc[price_troughs_idx[-2]]:
        # RSI to duc cao hon (Higher Low)
        if df_slice['rsi_14'].iloc[rsi_troughs_idx[-1]] > df_slice['rsi_14'].iloc[rsi_troughs_idx[-2]]:
            # nhn d liu t i v tr of duc gi cu cng
            df.loc[df_slice.index[price_troughs_idx[-1]], 'bullish_divergence'] = 1

    return df
# 1. fix Function fetch data thnh Function `async`
async def fetch_symbol_data_async(session, symbol, data_manager):
    """Function bt du bdfetch data cho 1 symbol."""
    print(f"   -> Bt du fetch {symbol}...")
    # Logic fetch of b n sd chuy n d i dused `await session.get(...)`
    # y chl v d , not must code ch y dngay
    # multi_tf_data = await data_manager.fetch_multi_timeframe_data_async(session, symbol)
    # df_features = data_manager.feature_engineer.create_all_features(multi_tf_data)
    # return symbol, df_features
    await asyncio.sleep(2) # Gil p th i gian ch 
    print(f"   <- Hon thnh fetch {symbol}.")
    return symbol, pd.DataFrame() # TrvDataFrame empty

# 2. Vi t l i vng l p chnh dch y bt du b 
async def main_async_loop(bot):
    async with aiohttp.ClientSession() as session:
        while True:
            print("\n----- Bt du chu kanalysis bt du b-----")

            # Check for config updates from experiment system
            bot.check_for_config_updates()

            # T o cfromc vfetch data cho t t csymbol
            tasks = [fetch_symbol_data_async(session, symbol, bot.data_manager) for symbol in SYMBOLS]

            # Chy tt c song song v di chng hon thnh
            results = await asyncio.gather(*tasks)

            # processing k t qusau khi d dli u of t t csymbols
            live_data_all_symbols = {symbol: df for symbol, df in results}

            # ... G i Function run_portfolio_rl_strategy with data has d fetch song song ...
            # bot.run_portfolio_rl_strategy(live_data_all_symbols)

            print("----- Kt thc chu kanalysis -----")
            await asyncio.sleep(60) # Ch60 giy



# ==== RL PRODUCTION EXECUTION ADAPTER (Appended by Assistant) ====
import json, math, requests

def _decode_actions_to_vector(action, n_symbols, action_space):
    """Return vector length N with values in {0,1,2} (0=HOLD,1=BUY,2=SELL)."""
    import numpy as _np
    arr = _np.asarray(action).reshape(-1)

    # MultiDiscrete 
    try:
        if hasattr(action_space, "nvec") and int(getattr(action_space.nvec, "size", 0)) == n_symbols:
            vec = arr[:n_symbols].astype(int).tolist()
            return [v if v in (0,1,2) else 0 for v in vec]
    except Exception:
        pass

    # Discrete fallback
    try:
        n_total = int(getattr(action_space, "n", n_symbols * 3))
    except Exception:
        n_total = n_symbols * 3
    A = 3 if n_total % max(1, n_symbols) in (0,3) else max(1, n_total // max(1, n_symbols))
    raw = int(arr[0]) if arr.size else 0
    sym_idx = int(_np.clip(raw // A, 0, max(0, n_symbols-1)))
    act = int(raw % A)
    vec = [0]*n_symbols
    vec[sym_idx] = act if act in (0,1,2) else 0
    return vec

def _to_oanda_instrument(sym: str) -> str:
    m = {"XAUUSD":"XAU_USD","XAGUSD":"XAG_USD","BTCUSD":"BTC_USD","ETHUSD":"ETH_USD"}
    if sym in m: return m[sym]
    if len(sym)==6 and sym.isalpha(): return f"{sym[:3]}_{sym[3:]}"
    return sym

def _place_market_order(symbol: str, side: str, units: int,
                        sl_price=None, tp_price=None):
    """
    Send marketorder to OANDA v20. If OANDA_ACCOUNT_ID not defined, DRY-RUN logs only.
    """
    import logging
    account_id = globals().get("OANDA_ACCOUNT_ID", None)
    api_key = globals().get("OANDA_API_KEY", None)
    base_url = globals().get("OANDA_URL", "https://api-fxtrade.oanda.com/v3")
    if not account_id or not api_key:
        logging.warning("[DRY-RUN] Missing OANDA_ACCOUNT_ID/API_KEY; skip live order.")
        return {"dry_run": True, "symbol": symbol, "side": side, "units": units}

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    instrument = _to_oanda_instrument(symbol)
    u = int(units) if side.lower()=="buy" else -int(units)
    u = 1 if u==0 else u

    payload = {
        "order": {
            "type": "MARKET",
            "instrument": instrument,
            "units": str(u),
            "timeInForce": "FOK",
            "positionFill": "DEFAULT",
        }
    }
    if sl_price:
        payload["order"]["stopLossOnFill"] = {"timeInForce": "GTC", "price": f"{sl_price:.5f}"}
    if tp_price:
        payload["order"]["takeProfitOnFill"] = {"timeInForce": "GTC", "price": f"{tp_price:.5f}"}

    url = f"{base_url}/accounts/{account_id}/orders"
    r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=10)
    if r.status_code >= 300:
        raise RuntimeError(f"OANDA error {r.status_code}: {r.text}")
    return r.json()

def _calc_position_size(balance: float, atr: float, pip_value: float,
                        risk_per_trade: float, sl_atr_mult: float, conf: float):
    import numpy as _np
    if atr is None or atr <= 0 or pip_value <= 0:
        return max(1, int(balance*0.0001)), None
    risk_amount = max(1.0, balance * risk_per_trade * max(0.2, float(conf)))
    sl_dist = atr * sl_atr_mult
    units = int(max(1, risk_amount / (sl_dist * pip_value)))
    return units, sl_dist

def rl_execute_production(model, symbols, observation, features_map, get_balance_func,
                          rl_deterministic=True,
                          min_conf_threshold=None,
                          risk_per_trade=0.0075,
                          sl_atr_mult=2.0,
                          logger_name="ProdBot"):
    """
    Plug-and-play executor:
      - model: SB3 model (PPO/A2C/...) d load
      - symbols: list[str] thtusednhu env
      - observation: obs theo policy of model
      - features_map: dict[symbol] -> {'confidence': float, 'atr': float, ...}
      - get_balance_func: callable -> float
    """
    import logging, numpy as _np
    logger = logging.getLogger(logger_name)

    action, _ = model.predict(observation, deterministic=bool(rl_deterministic))

    try:
        env = model.get_env()
        env_syms = env.get_attr("symbols")[0]
        if env_syms and len(env_syms)==len(symbols):
            symbols = env_syms
    except Exception:
        pass

    vec = _decode_actions_to_vector(action, len(symbols), getattr(model, "action_space", None))
    logger.info(f"[RL Decision] vector={vec} (0=HOLD,1=BUY,2=SELL)")
    logger.info(f"[Trace] Mapping: {list(enumerate(symbols))}")

    balance = float(get_balance_func())
    any_action = False

    # thresholds
    conf_gate = 0.0 if min_conf_threshold is None else float(min_conf_threshold)

    for idx, a in enumerate(vec):
        if a not in (1,2):
            continue
        sym = symbols[idx]
        feats = features_map.get(sym, {})
        conf = float(feats.get("confidence", 1.0))
        if conf < conf_gate:
            logger.info(f"[Gate] Skip {sym} due to confidence {conf:.2f} < {conf_gate:.2f}")
            continue

        atr = feats.get("atr", None)
        pip_val = feats.get("pip_value", 0.0001)

        units, sl_dist = _calc_position_size(balance, atr, pip_val, risk_per_trade, sl_atr_mult, conf)
        side = "buy" if a==1 else "sell"

        # Optional SL/TP derivation hook (user to implement via feats)
        sl_price = feats.get("sl_price")
        tp_price = feats.get("tp_price")

        try:
            resp = _place_market_order(sym, side, units, sl_price, tp_price)
            txid = resp.get("lastTransactionID") or resp.get("orderCreateTransaction", {}).get("id") or "dry-run"
            logger.info(f"[OPENED] {sym} {side.upper()} units={units} tx={txid}")
            any_action = True
        except Exception as e:
            logger.error(f"[ORDER ERROR] {sym}: {e}")

    if not any_action:
        logger.info("[RL Decision] HOLD (portfolio)")
    return any_action

class QualityGateValidator:
    """Advanced quality gates for model validation"""

    def __init__(self, config=None):
        self.config = config or {
            'min_sharpe_ratio': MIN_SHARPE_RATIO,
            'max_drawdown': MAX_DRAWDOWN_THRESHOLD,
            'min_calmar_ratio': MIN_CALMAR_RATIO,
            'min_information_ratio': MIN_INFORMATION_RATIO,
            'min_samples': MIN_SAMPLES_GATE,
            'min_f1_score': MIN_F1_SCORE_GATE,
            'max_std_f1': MAX_STD_F1_GATE
        }

    def calculate_deflated_sharpe(self, returns, benengulfingark_returns=None):
        """Calculate Deflated Sharpe Ratio to aallount for multiple testing bias"""
        if len(returns) < 30:
            return 0.0

        sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)

        # Deflation factor based on number of trialfixnd skewness/kurtosis
        n_trials = 100  # Assume 100 different strategies tested
        skewness = self._calculate_skewness(returns)
        kurtosis = self._calculate_kurtosis(returns)

        # Deflation adjustment
        variance_adjustment = (1 + (skewness**2)/6 + (kurtosis**2)/24) / len(returns)
        deflated_sharpe = sharpe / np.sqrt(1 + variance_adjustment * np.log(n_trials))

        return deflated_sharpe

    def _calculate_skewness(self, returns):
        """Calculate skewness of returns"""
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        if std_return == 0:
            return 0
        return np.mean(((returns - mean_return) / std_return) ** 3)

    def _calculate_kurtosis(self, returns):
        """Calculate excess kurtosis of returns"""
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        if std_return == 0:
            return 0
        return np.mean(((returns - mean_return) / std_return) ** 4) - 3

    def calculate_maximum_drawdown(self, returns):
        """Calculate maximum drawdown from returns series"""
        cumulative = np.cumprod(1 + returns)
        running_max = np.maximum.accumulate(cumulative)
        drawdown = (cumulative - running_max) / running_max
        return abs(np.min(drawdown))

    def calculate_calmar_ratio(self, returns):
        """Calculate Calmar ratio (annual return / max drawdown)"""
        annual_return = np.mean(returns) * 252
        max_dd = self.calculate_maximum_drawdown(returns)
        return annual_return / max_dd if max_dd > 0 else 0

    def validate_model_quality(self, model_results, returns=None):
        """Comprehensive quality gate validation"""
        validation_results = {
            'passed': True,
            'failures': [],
            'metrics': {}
        }

        # Statistical validation
        if 'cv_scores' in model_results:
            cv_scores = model_results['cv_scores']
            mean_score = np.mean(cv_scores)
            std_score = np.std(cv_scores)

            validation_results['metrics']['mean_cv_score'] = mean_score
            validation_results['metrics']['std_cv_score'] = std_score

            if mean_score < self.config['min_f1_score']:
                validation_results['passed'] = False
                validation_results['failures'].append(f"Mean CV score {mean_score:.3f} < {self.config['min_f1_score']}")

            if std_score > self.config['max_std_f1']:
                validation_results['passed'] = False
                validation_results['failures'].append(f"CV score std {std_score:.3f} > {self.config['max_std_f1']}")

        # Financial metrics validation
        if returns is not None and len(returns) > 0:
            deflated_sharpe = self.calculate_deflated_sharpe(returns)
            max_dd = self.calculate_maximum_drawdown(returns)
            calmar = self.calculate_calmar_ratio(returns)

            validation_results['metrics']['deflated_sharpe'] = deflated_sharpe
            validation_results['metrics']['max_drawdown'] = max_dd
            validation_results['metrics']['calmar_ratio'] = calmar

            if deflated_sharpe < self.config['min_sharpe_ratio']:
                validation_results['passed'] = False
                validation_results['failures'].append(f"Deflated Sharpe {deflated_sharpe:.3f} < {self.config['min_sharpe_ratio']}")

            if max_dd > self.config['max_drawdown']:
                validation_results['passed'] = False
                validation_results['failures'].append(f"Max drawdown {max_dd:.3f} > {self.config['max_drawdown']}")

            if calmar < self.config['min_calmar_ratio']:
                validation_results['passed'] = False
                validation_results['failures'].append(f"Calmar ratio {calmar:.3f} < {self.config['min_calmar_ratio']}")

        return validation_results

class TimeframeOptimizer:
    """T i uu ha giao dh theo total timeframe"""

    def __init__(self):
        self.timeframe_configs = TIMEFRAME_ENTRY_CONFIG
        self.model_configs = TIMEFRAME_MODEL_CONFIG
        self.risk_configs = TIMEFRAME_RISK_CONFIG

    def get_optimal_entry_config(self, symbol, timeframe):
        """L y config t i uu cho entry theo timeframe"""
        primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, "H4")

        # Uu tin timeframe d chdnh, if not fromh used primary
        target_tf = timeframe if timeframe in self.timeframe_configs else primary_tf

        config = self.timeframe_configs.get(target_tf, self.timeframe_configs["H4"])

        # i u ch nh theo d di m symbol
        symbol_metadata = SYMBOL_METADATA.get(symbol, {})
        asset_class = symbol_metadata.get("asset_class", "forex")

        if asset_class == "crypto":
            config["atr_multiplier_sl"] *= 1.5  # Wider SL for crypto
            config["atr_multiplier_tp"] *= 1.2   # Slightly wider TP
        elif asset_class == "equity_index":
            config["volume_confirmation"] = True  # Always use volume for indices
            config["session_filter"] = True      # Always filter by session

        return config

    def get_optimal_model_config(self, symbol, timeframe):
        """L y config t i uu cho model training theo timeframe"""
        primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, "H4")
        target_tf = timeframe if timeframe in self.model_configs else primary_tf

        config = self.model_configs.get(target_tf, self.model_configs["H4"])

        # i u ch nh theo d di m symbol
        symbol_metadata = SYMBOL_METADATA.get(symbol, {})
        volatility_profile = symbol_metadata.get("volatility_profile", "medium")

        if volatility_profile == "high":
            config["validation_split"] = min(0.4, config["validation_split"] + 0.05)
            config["early_stopping_rounds"] = int(config["early_stopping_rounds"] * 0.8)
        elif volatility_profile == "low":
            config["validation_split"] = max(0.15, config["validation_split"] - 0.05)
            config["early_stopping_rounds"] = int(config["early_stopping_rounds"] * 1.2)

        return config

    def get_optimal_risk_config(self, symbol, timeframe):
        """L y config t i uu cho risk management theo timeframe"""
        primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, "H4")
        target_tf = timeframe if timeframe in self.risk_configs else primary_tf

        config = self.risk_configs.get(target_tf, self.risk_configs["H4"])

        # i u ch nh theo allocation of symbol
        symbol_allocation = SYMBOL_ALLOCATION.get(symbol, {})
        weight = symbol_allocation.get("weight", 0.05)
        risk_multiplier = symbol_allocation.get("risk_multiplier", 1.0)

        # i u ch nh position size theo weight v risk multiplier
        config["max_position_size"] = min(0.1, config["max_position_size"] * weight * 20 * risk_multiplier)
        config["max_daily_loss"] = min(0.2, config["max_daily_loss"] * risk_multiplier)

        return config

    def calculate_timeframe_specific_features(self, symbol, timeframe, market_data):
        """Calculate features specififromo each timeframe"""
        config = self.get_optimal_entry_config(symbol, timeframe)
        features = config.get("features", [])

        timeframe_features = {}

        for feature in features:
            if feature == "momentum":
                timeframe_features["momentum"] = self._calculate_momentum_features(market_data, timeframe)
            elif feature == "volatility":
                timeframe_features["volatility"] = self._calculate_volatility_features(market_data, timeframe)
            elif feature == "trend":
                timeframe_features["trend"] = self._calculate_trend_features(market_data, timeframe)
            elif feature == "support_resistance":
                timeframe_features["support_resistance"] = self._calculate_sr_features(market_data, timeframe)
            elif feature == "session":
                timeframe_features["session"] = self._calculate_session_features(market_data, timeframe)
            elif feature == "volume":
                timeframe_features["volume"] = self._calculate_volume_features(market_data, timeframe)
            elif feature == "fundamental":
                timeframe_features["fundamental"] = self._calculate_fundamental_features(symbol, timeframe)
            elif feature == "seasonality":
                timeframe_features["seasonality"] = self._calculate_seasonality_features(market_data, timeframe)
            elif feature == "macro":
                timeframe_features["macro"] = self._calculate_macro_features(symbol, timeframe)
            elif feature == "long_term_trend":
                timeframe_features["long_term_trend"] = self._calculate_long_term_trend_features(market_data, timeframe)

        return timeframe_features

    def _calculate_momentum_features(self, market_data, timeframe):
        """Calculate momentum features by timeframe"""
        if len(market_data) < 20:
            return {"rsi": 50, "macd_signal": 0, "momentum": 0}

        # RSI calculation
        closes = market_data['close'].values
        rsi = self._calculate_rsi(closes, 14)

        # MACD calculation
        macd_line, signal_line, histogram = self._calculate_macd(closes)
        macd_signal = histogram[-1] if len(histogram) > 0 else 0

        # Price momentum
        momentum = (closes[-1] - closes[-20]) / closes[-20] if len(closes) >= 20 else 0

        return {
            "rsi": rsi[-1] if len(rsi) > 0 else 50,
            "macd_signal": macd_signal,
            "momentum": momentum
        }

    def _calculate_volatility_features(self, market_data, timeframe):
        """Calculate volatility features by timeframe"""
        if len(market_data) < 20:
            return {"atr": 0.001, "bb_position": 0.5, "volatility": 0}

        highs = market_data['high'].values
        lows = market_data['low'].values
        closes = market_data['close'].values

        # ATR calculation
        atr = self._calculate_atr(highs, lows, closes, 14)

        # Bollinger Bands position
        bb_position = self._calculate_bb_position(closes, 20, 2)

        # Volatility (standard deviation of returns)
        returns = np.diff(closes) / closes[:-1]
        volatility = np.std(returns) if len(returns) > 0 else 0

        return {
            "atr": atr[-1] if len(atr) > 0 else 0.001,
            "bb_position": bb_position,
            "volatility": volatility
        }

    def _calculate_trend_features(self, market_data, timeframe):
        """Calculate trend features by timeframe"""
        if len(market_data) < 50:
            return {"ema_20": 0, "ema_50": 0, "trend_direction": 0}

        closes = market_data['close'].values

        # EMA calculations
        ema_20 = self._calculate_ema(closes, 20)
        ema_50 = self._calculate_ema(closes, 50)

        # Trend direction
        trend_direction = 1 if ema_20[-1] > ema_50[-1] else -1

        return {
            "ema_20": ema_20[-1] if len(ema_20) > 0 else closes[-1],
            "ema_50": ema_50[-1] if len(ema_50) > 0 else closes[-1],
            "trend_direction": trend_direction
        }

    def _calculate_sr_features(self, market_data, timeframe):
        """Calculate support/resistance features"""
        if len(market_data) < 20:
            return {"support_level": 0, "resistance_level": 0}

        highs = market_data['high'].values
        lows = market_data['low'].values

        # Simple support/resistance calculation
        recent_highs = highs[-20:]
        recent_lows = lows[-20:]

        resistance_level = np.max(recent_highs)
        support_level = np.min(recent_lows)

        return {
            "support_level": support_level,
            "resistance_level": resistance_level
        }

    def _calculate_session_features(self, market_data, timeframe):
        """Calculate session features"""
        current_hour = datetime.now().hour

        # Determine current session
        if 0 <= current_hour < 8:
            session = "asian"
        elif 8 <= current_hour < 16:
            session = "london"
        elif 16 <= current_hour < 24:
            session = "new_york"
        else:
            session = "overlap"

        return {
            "current_session": session,
            "session_volatility": 1.0,  # Placeholder
            "session_momentum": 0.0      # Placeholder
        }

    def _calculate_volume_features(self, market_data, timeframe):
        """Calculate volume features"""
        if 'volume' not in market_data.columns or len(market_data) < 20:
            return {"volume_spike": 1.0, "volume_trend": 0}

        volumes = market_data['volume'].values
        avg_volume = np.mean(volumes[-20:])
        current_volume = volumes[-1] if len(volumes) > 0 else avg_volume

        volume_spike = current_volume / avg_volume if avg_volume > 0 else 1.0
        volume_trend = (volumes[-1] - volumes[-20]) / volumes[-20] if len(volumes) >= 20 else 0

        return {
            "volume_spike": volume_spike,
            "volume_trend": volume_trend
        }

    def _calculate_fundamental_features(self, symbol, timeframe):
        """Calculate fundamental features"""
        # Placeholder for fundamental analysis
        return {
            "fundamental_score": 0.0,
            "fundamental_outlook": 0.0
        }

    def _calculate_seasonality_features(self, market_data, timeframe):
        """Calculate seasonality features"""
        # Placeholder for seasonality analysis
        return {
            "seasonality_factor": 1.0,
            "seasonal_pattern": 0.0
        }

    def _calculate_macro_features(self, symbol, timeframe):
        """Calculate macro features"""
        # Placeholder for macro analysis
        return {
            "macro_trend": 0.0,
            "macro_score": 0.0
        }

    def _calculate_long_term_trend_features(self, market_data, timeframe):
        """Calculate long-term trend features"""
        if len(market_data) < 100:
            return {"long_term_trend": 0.0}

        closes = market_data['close'].values
        long_term_trend = (closes[-1] - closes[-100]) / closes[-100] if len(closes) >= 100 else 0

        return {
            "long_term_trend": long_term_trend
        }

    # Helper methods for technical calculations
    def _calculate_rsi(self, prices, period=14):
        """Calculate RSI"""
        if len(prices) < period + 1:
            return np.array([50])

        deltas = np.diff(prices)
        gains = np.where(deltas > 0, deltas, 0)
        losses = np.where(deltas < 0, -deltas, 0)

        avg_gains = np.mean(gains[:period])
        avg_losses = np.mean(losses[:period])

        rsi_values = []
        for i in range(period, len(prices)):
            if avg_losses == 0:
                rsi = 100
            else:
                rs = avg_gains / avg_losses
                rsi = 100 - (100 / (1 + rs))
            rsi_values.append(rsi)

            if i < len(prices) - 1:
                avg_gains = (avg_gains * (period - 1) + gains[i]) / period
                avg_losses = (avg_losses * (period - 1) + losses[i]) / period

        return np.array(rsi_values)

    def _calculate_macd(self, prices, fast=12, slow=26, signal=9):
        """Calculate MACD"""
        if len(prices) < slow:
            return np.array([0]), np.array([0]), np.array([0])

        ema_fast = self._calculate_ema(prices, fast)
        ema_slow = self._calculate_ema(prices, slow)

        macd_line = ema_fast - ema_slow
        signal_line = self._calculate_ema(macd_line, signal)
        histogram = macd_line - signal_line

        return macd_line, signal_line, histogram

    def _calculate_ema(self, prices, period):
        """Calculate EMA"""
        if len(prices) < period:
            return np.array([prices[-1]])

        alpha = 2 / (period + 1)
        ema = np.zeros_like(prices)
        ema[0] = prices[0]

        for i in range(1, len(prices)):
            ema[i] = alpha * prices[i] + (1 - alpha) * ema[i-1]

        return ema[period-1:]

    def _calculate_atr(self, highs, lows, closes, period=14):
        """Calculate ATR"""
        if len(highs) < period + 1:
            return np.array([0.001])

        tr = np.maximum(
            highs[1:] - lows[1:],
            np.maximum(
                np.abs(highs[1:] - closes[:-1]),
                np.abs(lows[1:] - closes[:-1])
            )
        )

        atr = np.zeros_like(tr)
        atr[0] = np.mean(tr[:period])

        for i in range(1, len(tr)):
            atr[i] = (atr[i-1] * (period - 1) + tr[i]) / period

        return atr

    def _calculate_bb_position(self, prices, period=20, std_dev=2):
        """Calculate Bollinger Bands position"""
        if len(prices) < period:
            return 0.5

        sma = np.mean(prices[-period:])
        std = np.std(prices[-period:])

        upper_band = sma + (std_dev * std)
        lower_band = sma - (std_dev * std)

        current_price = prices[-1]

        if upper_band == lower_band:
            return 0.5

        bb_position = (current_price - lower_band) / (upper_band - lower_band)
        return max(0, min(1, bb_position))


class AdvancedEntryTPSLCalculator:
    """Advanced Entry, Take Profit, and Stop Loss calculator for all symbols"""

    def __init__(self):
        self.validation_threshold = 0.02  # Ultra-strict: from test results
        self.early_stopping_patience = 3   # Very early stopping
        self.regularization_strength = 0.15  # Very sin regularization from test results
        self.dropout_rate = 0.7  # Very high dropout from test results
        self.batch_normalization = True
        self.data_augmentation = True
        self.cross_validation_folds = 10  # More folds from test results
        self.out_of_sample_testing = True
        self.max_depth_limit = 4  # Very shallow trees from test results
        self.min_samples_split_limit = 50  # Very high minimum from test results
        self.min_samples_leaf_limit = 25   # Very high minimum from test results
        self.max_features_limit = 0.3  # Very few features from test results
        self.timeframe_optimizer = TimeframeOptimizer()  # Add timeframe optimizer
        self.cpu_threshold = 80  # CPU optimization from test results
        self.memory_threshold = 85  # Memory optimization from test results
        self.alert_limit = 5  # Reduced alert limit from test results
        self.overfitting_detection = True  # Enable overfitting detection
        self.validation_requirements = "STRICT"  # Strict validation
    def calculate_optimal_entry_tpsl(self, symbol, signal, current_price, technical_data, market_data, timeframe=None):
        """Calculate optimal entry, TP, and SL for this symbol with timeframe optimization"""

        # Get primary timeframe if not specified
        if timeframe is None:
            timeframe = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, "H4")

        # Get timeframe-specific configuration
        tf_config = self.timeframe_optimizer.get_optimal_entry_config(symbol, timeframe)

        # Get symbol-specific configuration as fallback
        symbol_config = ENTRY_TP_SL_CONFIG.get(symbol, ENTRY_TP_SL_CONFIG["EURUSD"])

        # Merge configurations (timeframe config takes priority)
        config = {**symbol_config, **tf_config}

        # Calculate timeframe-specific features
        tf_features = self.timeframe_optimizer.calculate_timeframe_specific_features(symbol, timeframe, market_data)

        # Merge technical data with timeframe features
        enhanced_technical_data = {**technical_data, **tf_features}

        # Calculate ATR for dynamic SL/TP
        atr = self._calculate_atr_from_data(market_data)

        # Apply volatility adjustment if enabled
        if config.get("volatility_adjustment", True):
            volatility_multiplier = tf_features.get("volatility", {}).get("volatility", 1.0)
            config["atr_multiplier_sl"] *= (1 + volatility_multiplier * 0.5)
            config["atr_multiplier_tp"] *= (1 + volatility_multiplier * 0.3)

        # Base SL and TP calculations
        if signal == "BUY":
            base_sl = current_price - (atr * config["atr_multiplier_sl"])
            base_tp = current_price + (atr * config["atr_multiplier_tp"])
        else:  # SELL
            base_sl = current_price + (atr * config["atr_multiplier_sl"])
            base_tp = current_price - (atr * config["atr_multiplier_tp"])

        # Apply symbol-specific optimizations with timeframe features
        optimized_entry, optimized_sl, optimized_tp = self._apply_symbol_optimizations(
            symbol, signal, current_price, base_sl, base_tp, enhanced_technical_data, market_data, config
        )

        # Calculate Risk/Reward ratio
        if signal == "BUY":
            risk = abs(optimized_entry - optimized_sl)
            reward = abs(optimized_tp - optimized_entry)
        else:  # SELL
            risk = abs(optimized_sl - optimized_entry)
            reward = abs(optimized_entry - optimized_tp)

        rr_ratio = reward / risk if risk > 0 else 0

        # Validate R/R ratio
        if rr_ratio < config["min_rr_ratio"]:
            # Adjust TP to meefrom modelinimum R/R
            if signal == "BUY":
                optimized_tp = optimized_entry + (risk * config["min_rr_ratio"])
            else:
                optimized_tp = optimized_entry - (risk * config["min_rr_ratio"])
            rr_ratio = config["min_rr_ratio"]

        # Apply session filter if enabled
        if config.get("session_filter", False):
            session_features = tf_features.get("session", {})
            current_session = session_features.get("current_session", "unknown")
            session_multiplier = TIMEFRAME_RISK_CONFIG.get(timeframe, {}).get("session_risk_multiplier", {}).get(current_session, 1.0)

            # Adjust position size based on session
            optimized_entry *= session_multiplier
            optimized_sl *= session_multiplier
            optimized_tp *= session_multiplier

        return {
            "entry_price": optimized_entry,
            "stop_loss": optimized_sl,
            "take_profit": optimized_tp,
            "rr_ratio": rr_ratio,
            "atr": atr,
            "timeframe": timeframe,
            "timeframe_config": tf_config,
            "confidence": self._calculate_entry_confidence(symbol, signal, optimized_entry, enhanced_technical_data)
        }

    def _apply_symbol_optimizations(self, symbol, signal, current_price, base_sl, base_tp,
                                   technical_data, market_data, config):
        """Apply symbol-specific optimizations to entry, SL, and TP"""

        # Get primary timeframe for this symbol
        primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, "H4")

        # Apply timeframe-specific optimization first
        if primary_tf in TIMEFRAME_ENTRY_CONFIG:
            tf_config = TIMEFRAME_ENTRY_CONFIG[primary_tf]
            entry_method = tf_config["entry_method"]

            if entry_method == "scalping_momentum":
                return self._optimize_scalping_momentum_entry(symbol, signal, current_price, base_sl, base_tp, technical_data)
            elif entry_method == "swing_trading":
                return self._optimize_swing_trading_entry(symbol, signal, current_price, base_sl, base_tp, technical_data)
            elif entry_method == "position_trading":
                return self._optimize_position_trading_entry(symbol, signal, current_price, base_sl, base_tp, technical_data)
            elif entry_method == "long_term_trend":
                return self._optimize_long_term_trend_entry(symbol, signal, current_price, base_sl, base_tp, technical_data)

        # Fallback to symbol-specific methods
        if config["entry_method"] == "breakout_confirmation":
            return self._optimize_breakout_entry(symbol, signal, current_price, base_sl, base_tp, technical_data)

        elif config["entry_method"] == "trend_following":
            return self._optimize_trend_following_entry(symbol, signal, current_price, base_sl, base_tp, technical_data)

        elif config["entry_method"] == "fibonacci_confluence":
            return self._optimize_fibonacci_entry(symbol, signal, current_price, base_sl, base_tp, technical_data)

        elif config["entry_method"] == "carry_trade_momentum":
            return self._optimize_carry_trade_entry(symbol, signal, current_price, base_sl, base_tp, technical_data)

        elif config["entry_method"] == "volatility_breakout":
            return self._optimize_volatility_breakout_entry(symbol, signal, current_price, base_sl, base_tp, technical_data)

        elif config["entry_method"] == "momentum_trending":
            return self._optimize_momentum_entry(symbol, signal, current_price, base_sl, base_tp, technical_data)

        elif config["entry_method"] == "asian_session_breakout":
            return self._optimize_asian_session_entry(symbol, signal, current_price, base_sl, base_tp, technical_data)

        else:
            return current_price, base_sl, base_tp

    def _optimize_breakout_entry(self, symbol, signal, current_price, base_sl, base_tp, technical_data):
        """Optimize entry for breakout confirmation (SPX500)"""
        # Wait for breakout confirmation
        resistance_level = technical_data.get('resistance_level', current_price * 1.002)
        support_level = technical_data.get('support_level', current_price * 0.998)

        if signal == "BUY":
            # Enter slightly above resistance for confirmation
            entry_price = resistance_level * 1.0005
            # Adjust SL to be below recent support
            optimized_sl = support_level * 0.9995
        else:  # SELL
            # Enter slightly below support for confirmation
            entry_price = support_level * 0.9995
            # Adjust SL to be above recent resistance
            optimized_sl = resistance_level * 1.0005

        return entry_price, optimized_sl, base_tp

    def _optimize_trend_following_entry(self, symbol, signal, current_price, base_sl, base_tp, technical_data):
        """Optimize entry for trend following (DE40)"""
        ema_20 = technical_data.get('ema_20', current_price)
        ema_50 = technical_data.get('ema_50', current_price)

        if signal == "BUY":
            # Enter on pullback to EMA 20
            entry_price = min(current_price, ema_20 * 1.0002)
            # SL below EMA 50
            optimized_sl = ema_50 * 0.9995
        else:  # SELL
            # Enter on pullback to EMA 20
            entry_price = max(current_price, ema_20 * 0.9998)
            # SL above EMA 50
            optimized_sl = ema_50 * 1.0005

        return entry_price, optimized_sl, base_tp

    def _optimize_fibonacci_entry(self, symbol, signal, current_price, base_sl, base_tp, technical_data):
        """Optimize entry for Fibonaalli confluence (XAUUSD)"""
        fib_levels = technical_data.get('fibonacci_levels', {})

        if signal == "BUY":
            # Findnearest Fibonaalli support level
            fib_support = fib_levels.get('fib_38.2', current_price * 0.998)
            entry_price = fib_support * 1.0003
            # SL below next Fibonaalli level
            optimized_sl = fib_levels.get('fib_23.6', current_price * 0.995)
        else:  # SELL
            # Findnearest Fibonaalli resistance level
            fib_resistance = fib_levels.get('fib_61.8', current_price * 1.002)
            entry_price = fib_resistance * 0.9997
            # SL above next Fibonaalli level
            optimized_sl = fib_levels.get('fib_78.6', current_price * 1.005)

        return entry_price, optimized_sl, base_tp

    def _optimize_carry_trade_entry(self, symbol, signal, current_price, base_sl, base_tp, technical_data):
        """Optimize entry for carry trade momentum (AUDNZD)"""
        # Use interest rate differential and momentum
        momentum = technical_data.get('momentum', 0)

        if signal == "BUY":
            # Enter on momentulevelonfirmation
            entry_price = current_price * (1 + momentum * 0.1)
            # Tighter SL for carry trades
            optimized_sl = current_price * 0.999
        else:  # SELL
            # Enter on momentulevelonfirmation
            entry_price = current_price * (1 - momentum * 0.1)
            # Tighter SL for carry trades
            optimized_sl = current_price * 1.001

        return entry_price, optimized_sl, base_tp

    def _optimize_volatility_breakout_entry(self, symbol, signal, current_price, base_sl, base_tp, technical_data):
        """Optimize entry for volatility breakout (BTCUSD)"""
        volatility = technical_data.get('volatility', 0)
        bollinger_position = technical_data.get('bb_position', 0.5)

        if signal == "BUY":
            # Enter on volatility expansion
            entry_price = current_price * (1 + volatility * 0.05)
            # Wider SL for crypto volatility
            optimized_sl = current_price * (1 - volatility * 0.15)
        else:  # SELL
            # Enter on volatility expansion
            entry_price = current_price * (1 - volatility * 0.05)
            # Wider SL for crypto volatility
            optimized_sl = current_price * (1 + volatility * 0.15)

        return entry_price, optimized_sl, base_tp

    def _optimize_momentum_entry(self, symbol, signal, current_price, base_sl, base_tp, technical_data):
        """Optimize entry for momentum trending (ETHUSD)"""
        macd_signal = technical_data.get('macd_signal', 0)
        rsi = technical_data.get('rsi', 50)

        if signal == "BUY":
            # Enter on MACD momentum
            entry_price = current_price * (1 + macd_signal * 0.02)
            # Adjust SL based on RSI
            rsi_factor = max(0.5, (100 - rsi) / 100)
            optimized_sl = current_price * (1 - rsi_factor * 0.02)
        else:  # SELL
            # Enter on MACD momentum
            entry_price = current_price * (1 - macd_signal * 0.02)
            # Adjust SL based on RSI
            rsi_factor = max(0.5, rsi / 100)
            optimized_sl = current_price * (1 + rsi_factor * 0.02)

        return entry_price, optimized_sl, base_tp

    def _optimize_asian_session_entry(self, symbol, signal, current_price, base_sl, base_tp, technical_data):
        """Optimize entry for Asian session breakout (JP225)"""
        session_volatility = technical_data.get('asian_session_volatility', 0)

        if signal == "BUY":
            # Enter on Asian session momentum
            entry_price = current_price * (1 + session_volatility * 0.03)
            # Tighter SL for Asian markets
            optimized_sl = current_price * 0.998
        else:  # SELL
            # Enter on Asian session momentum
            entry_price = current_price * (1 - session_volatility * 0.03)
            # Tighter SL for Asian markets
            optimized_sl = current_price * 1.002

        return entry_price, optimized_sl, base_tp

    # === TIMEFRAME-SPECIFIC ENTRY OPTIMIZATION METHODS ===

    def _optimize_scalping_momentum_entry(self, symbol, signal, current_price, base_sl, base_tp, technical_data):
        """Optimize entry for H1 scalping momentum"""
        rsi = technical_data.get('rsi', 50)
        macd_signal = technical_data.get('macd_signal', 0)
        volume_spike = technical_data.get('volume_spike', 1.0)

        if signal == "BUY":
            # Enter on momentum with volume confirmation
            momentum_factor = max(0.0001, macd_signal * 0.01)
            volume_factor = min(1.0005, volume_spike * 0.0001)
            entry_price = current_price * (1 + momentum_factor * volume_factor)
            # Tight SL for scalping
            optimized_sl = current_price * 0.9995
        else:  # SELL
            momentum_factor = max(0.0001, -macd_signal * 0.01)
            volume_factor = min(1.0005, volume_spike * 0.0001)
            entry_price = current_price * (1 - momentum_factor * volume_factor)
            optimized_sl = current_price * 1.0005

        return entry_price, optimized_sl, base_tp

    def _optimize_swing_trading_entry(self, symbol, signal, current_price, base_sl, base_tp, technical_data):
        """Optimize entry for H4 swing trading"""
        ema_20 = technical_data.get('ema_20', current_price)
        ema_50 = technical_data.get('ema_50', current_price)
        support_resistance = technical_data.get('support_resistance_level', current_price)

        if signal == "BUY":
            # Enter on pullback to supportor EMA
            support_level = min(support_resistance, ema_20)
            entry_price = support_level * 1.0003
            # SL below EMA 50
            optimized_sl = ema_50 * 0.9995
        else:  # SELL
            # Enter on pullback to resistance or EMA
            resistance_level = max(support_resistance, ema_20)
            entry_price = resistance_level * 0.9997
            # SL above EMA 50
            optimized_sl = ema_50 * 1.0005

        return entry_price, optimized_sl, base_tp

    def _optimize_position_trading_entry(self, symbol, signal, current_price, base_sl, base_tp, technical_data):
        """Optimize entry for D1 position trading"""
        trend_direction = technical_data.get('trend_direction', 0)
        fundamental_score = technical_data.get('fundamental_score', 0)
        seasonality_factor = technical_data.get('seasonality_factor', 1.0)

        if signal == "BUY":
            # Enter on trendonfirmation with fundamental support
            trend_factor = max(0.0005, trend_direction * 0.002)
            fundamental_factor = max(0.0002, fundamental_score * 0.001)
            entry_price = current_price * (1 + trend_factor + fundamental_factor) * seasonality_factor
            # Wider SL for position trading
            optimized_sl = current_price * 0.998
        else:  # SELL
            trend_factor = max(0.0005, -trend_direction * 0.002)
            fundamental_factor = max(0.0002, -fundamental_score * 0.001)
            entry_price = current_price * (1 - trend_factor - fundamental_factor) * seasonality_factor
            optimized_sl = current_price * 1.002

        return entry_price, optimized_sl, base_tp

    def _optimize_long_term_trend_entry(self, symbol, signal, current_price, base_sl, base_tp, technical_data):
        """Optimize entry for W1 long-term trend"""
        macro_trend = technical_data.get('macro_trend', 0)
        seasonal_pattern = technical_data.get('seasonal_pattern', 0)
        fundamental_outlook = technical_data.get('fundamental_outlook', 0)

        if signal == "BUY":
            # Enter on macro trend with seasonal and fundamental confirmation
            macro_factor = max(0.001, macro_trend * 0.005)
            seasonal_factor = max(0.0005, seasonal_pattern * 0.002)
            fundamental_factor = max(0.0005, fundamental_outlook * 0.002)
            entry_price = current_price * (1 + macro_factor + seasonal_factor + fundamental_factor)
            # Very wide SL for long-term trading
            optimized_sl = current_price * 0.995
        else:  # SELL
            macro_factor = max(0.001, -macro_trend * 0.005)
            seasonal_factor = max(0.0005, -seasonal_pattern * 0.002)
            fundamental_factor = max(0.0005, -fundamental_outlook * 0.002)
            entry_price = current_price * (1 - macro_factor - seasonal_factor - fundamental_factor)
            optimized_sl = current_price * 1.005

        return entry_price, optimized_sl, base_tp

    def _calculate_atr_from_data(self, market_data):
        """Calculate ATR from market data if not provided"""
        try:
            if market_data is None:
                return 0.001  # Default ATR
            
            # Get price data from market_data
            price_data = market_data.get('price_data')
            if price_data is None or len(price_data) < 14:
                return 0.001  # Default ATR
            
            # Calculate ATR with safe window size
            atr_window = min(14, len(price_data) - 1)
            if atr_window < 2:
                return 0.001
                
            atr_indicator = AverageTrueRange(
                price_data["high"], 
                price_data["low"], 
                price_data["close"], 
                window=atr_window
            )
            atr_series = atr_indicator.average_true_range().dropna()
            
            if len(atr_series) > 0:
                return atr_series.iloc[-1]
            else:
                return 0.001  # Default ATR
        except Exception as e:
            print(f"❌ Error calculating ATR from data: {e}")
            return 0.001  # Default ATR

    def _calculate_entry_confidence(self, symbol, signal, entry_price, technical_data):
        """Calculate confidence score for the entry"""
        confidence_factors = []

        # RSI factor
        rsi = technical_data.get('rsi', 50)
        if signal == "BUY":
            rsi_factor = max(0, (30 - rsi) / 30) if rsi < 30 else max(0, (70 - rsi) / 40)
        else:
            rsi_factor = max(0, (rsi - 70) / 30) if rsi > 70 else max(0, (rsi - 30) / 40)
        confidence_factors.append(rsi_factor)

        # MACD factor
        macd = technical_data.get('macd', 0)
        macd_signal = technical_data.get('macd_signal', 0)
        macd_factor = abs(macd - macd_signal) / max(abs(macd), 0.001)
        confidence_factors.append(min(macd_factor, 1.0))

        # Trend factor
        trend_strength = technical_data.get('trend_strength', 0)
        confidence_factors.append(trend_strength)

        # Volume factor (if available)
        volume_factor = technical_data.get('volume_factor', 0.5)
        confidence_factors.append(volume_factor)

        # Calculate weighted average
        weights = [0.3, 0.25, 0.25, 0.2]
        confidence = sum(f * w for f, w in zip(confidence_factors, weights))

        return min(max(confidence, 0), 1)  # Clamp between 0 and 1


class EnhancedDataFreshnessManager:
    """Enhanced data freshness management with intelligent thresholdfixnd recovery"""

    def __init__(self):
        # Dynamifromhresholds based on market conditions
        self.base_thresholds = {
            "H1": 1440,   # 24 hours
            "H4": 2880,   # 48 hours
            "D1": 10080,  # 7 days
            "W1": 20160   # 14 days
        }

        # Weekend multipliers (more lenienton weekends)
        self.weekend_multipliers = {
            "H1": 2.0,    # 48 hours on weekends
            "H4": 1.5,    # 72 hours on weekends
            "D1": 1.2,    # 8.4 days on weekends
            "W1": 1.0     # No change for weekly
        }

        # Market session adjustments
        self.session_adjustments = {
            "asian": 1.0,
            "london": 1.0,
            "new_york": 1.0,
            "closed": 2.0  # Double threshold when markets closed
        }

        self.freshness_history = {}
        self.recovery_attempts = {}

    def get_dynamic_threshold(self, symbol: str, timeframe: str) -> float:
        """Get dynamic threshold based on market conditions"""
        base_threshold = self.base_thresholds.get(timeframe.upper(), 1440)

        # Apply weekend multiplier
        if is_weekend():
            multiplier = self.weekend_multipliers.get(timeframe.upper(), 1.0)
            base_threshold *= multiplier

        # Apply market session adjustment
        current_session = self._get_current_session()
        session_multiplier = self.session_adjustments.get(current_session, 1.0)
        base_threshold *= session_multiplier

        # Apply symbol-specifiofdjustments
        if is_equity_index_symbol(symbol):
            # Equity indices have longer thresholds on weekends
            if is_weekend():
                base_threshold *= 1.5
        elif is_crypto_symbol(symbol):
            # Crypto has shorter thresholds (24/7 trading)
            base_threshold *= 0.8

        return base_threshold

    def _get_current_session(self) -> str:
        """Get current trading session"""
        from datetime import datetime
        current_hour = datetime.now().hour

        if 0 <= current_hour < 8:
            return "asian"
        elif 8 <= current_hour < 16:
            return "london"
        elif 16 <= current_hour < 24:
            return "new_york"
        else:
            return "closed"

    def validate_with_recovery(self, multi_tf_data, primary_tf: str, symbol: str) -> bool:
        """Validate data freshness with automatic recovery attempts"""
        if not multi_tf_data or primary_tf not in multi_tf_data:
            return False

        # Get dynamic thresholds
        thresholds = {}
        for tf in multi_tf_data.keys():
            thresholds[tf] = self.get_dynamic_threshold(symbol, tf)

        # Use enhanced validation with dynamic thresholds
        return validate_dataframe_freshness(
            multi_tf_data,
            primary_tf,
            symbol=symbol,
            per_tf_override=thresholds
        )
    def log_freshness_status(self, symbol: str, multi_tf_data: dict):
        """Log detailed freshness status for all timeframes"""
        logging.info(f" [Freshness Status] {symbol}:")

        for tf, df in multi_tf_data.items():
            if df is None or df.empty:
                logging.warning(f"  {tf}: No data")
                continue

            threshold = self.get_dynamic_threshold(symbol, tf)

            # Calculate staleness
            from datetime import datetime, timezone
            now_utc = datetime.now(timezone.utc)
            idx = df.index

            if getattr(idx, "tz", None) is None:
                last_ts = idx[-1].to_pydatetime().replace(tzinfo=timezone.utc)
            else:
                last_ts = idx[-1].tz_convert("UTC").to_pydatetime()

            minutes_diff = (now_utc - last_ts).total_seconds() / 60.0
            hours_diff = minutes_diff / 60.0

            status = " " if minutes_diff <= threshold else "  "
            logging.info(f"  {tf}: {status} {hours_diff:.1f}h ago (threshold: {threshold/60:.1f}h)")


# Global instance
data_freshness_manager = EnhancedDataFreshnessManager()

class AdvancedObservability:
    """Enhanced observability with modern Discord alertfixnd performance tracking"""

    def __init__(self, discord_webhook=None):
        self.discord_webhook = discord_webhook or DISCORD_WEBHOOK
        self.last_alert_time = {}
        self.performance_history = deque(maxlen=1000)
        self.alert_templates = self._init_alert_templates()
        self.performance_metrics = {}
        self.alert_queue = deque(maxlen=100)

    def _init_alert_templates(self):
        """Initialize modern Discord alert templates"""
        templates = {
            "trade_signal": {
                "title": " Trading Signal Detected",
                "color": 3066993,  # Green
                "thumbnail": "https://cdn.discordapp.com/emojis/1234567890123456789.png"
            },
            "position_opened": {
                "title": " Position Opened",
                "color": 3066993,  # Green
                "thumbnail": "https://cdn.discordapp.com/emojis/1234567890123456789.png"
            },
            "position_closed": {
                "title": " Position Closed",
                "color": 15158332,  # Red
                "thumbnail": "https://cdn.discordapp.com/emojis/1234567890123456789.png"
            },
            "risk_warning": {
                "title": " Risk Warning",
                "color": 15105570,  # Orange
                "thumbnail": "https://cdn.discordapp.com/emojis/1234567890123456789.png"
            },
            "performance_report": {
                "title": " Performance Report",
                "color": 3447003,  # Blue
                "thumbnail": "https://cdn.discordapp.com/emojis/1234567890123456789.png"
            },
            "system_status": {
                "title": " System Status",
                "color": 3447003,  # Blue
                "thumbnail": "https://cdn.discordapp.com/emojis/1234567890123456789.png"
            }
        }

        return templates

    async def send_discord_alert(self, message, alert_type="INFO", force=False, data=None):
        """Send modern Discord alert with rich embedfixnd rate limiting"""
        # Validate inputs
        if not message or not message.strip():
            logging.warning(" Discord alert skipped: Empty message")
            return False

        # Validate webhook URL format
        if not self.discord_webhook or "webhook" not in self.discord_webhook.lower():
            logging.warning(" Discord webhook not configured")
            return False

        # Additional validation for webhook URL
        if not self.discord_webhook.startswith("https://discord.com/api/webhooks/"):
            logging.warning(" Invalid Discord webhook URL format")
            return False

        current_time = datetime.now()
        alert_key = f"{alert_type}_{hash(message) % 1000}"

        # Rate limiting
        if not force and alert_key in self.last_alert_time:
            time_diff = (current_time - self.last_alert_time[alert_key]).total_seconds()
            if time_diff < DISCORD_RATE_LIMIT_SECONDS:
                logging.debug(f"Discord alert rate limited: {alert_type}")
                return False

        self.last_alert_time[alert_key] = current_time

        # Get template for alert type - use trade_signal as default instead of system_status
        template = self.alert_templates.get(alert_type.lower(), self.alert_templates["trade_signal"])

        # Ensure message is not too long (Discord limit: 2000 chars for description)
        if len(message) > 1900:
            message = message[:1900] + "..."

        # Create rich embed with GMT+7 timezone
        try:
            current_time_str = get_current_time(TIMEZONE_CONFIG["DEFAULT_TIMEZONE"]).isoformat()
        except:
            current_time_str = datetime.now().isoformat()

        embed = {
            "title": template.get("title", " Trading Bot Alert"),
            "description": message,
            "color": template.get("color", 3447003),  # Default blue color
            "timestamp": current_time_str,
            "footer": {
                "text": f" AI Trading Bot | GMT+7 ({TIMEZONE_CONFIG.get('DEFAULT_TIMEZONE', 'Asia/Bangkok')})"
            },
            "author": {
                "name": "Trading Bot System"
            }
        }

        # Add fields based on data provided
        if data:
            fields = self._create_embed_fields(data, alert_type)
            if fields:
                embed["fields"] = fields

        # Add thumbnail if available
        if template.get("thumbnail"):
            embed["thumbnail"] = {"url": template["thumbnail"]}

        # Validate embed before creating payload
        if not embed.get("title") or not embed.get("description"):
            logging.warning(" Discord embed validation failed: missing title or description")
            return await self._send_fallback_discord_alert(message)

        payload = {"embeds": [embed]}

        # Additional payload validation
        if not payload.get("embeds") or len(payload["embeds"]) == 0:
            logging.warning(" Discord payload validation failed: empty embeds")
            return await self._send_fallback_discord_alert(message)

        # Validate embedontent
        embed = payload["embeds"][0]
        if not embed.get("title") and not embed.get("description") and not embed.get("fields"):
            logging.warning(" Discord payload validation failed: empty embedontent")
            return await self._send_fallback_discord_alert(message)

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(self.discord_webhook, json=payload, timeout=10) as response:
                    response_text = await response.text()
                    if response.status in [200, 204]:
                        logging.info(f"Discord alert sent successfully: {alert_type}")
                        return True
                    else:
                        logging.error(f"Discord alert failed: {response.status} - {response_text}")
                        return False
        except Exception as e:
            logging.error(f"Discord alert error: {e}")
            # Try fallback
            try:
                return await self._send_fallback_discord_alert(message)
            except Exception as fallback_error:
                logging.error(f"Fallback Discord alert also failed: {fallback_error}")
                # Print to console as last resort
                print(f" Bot Alert: {message}")
                return False

    async def _send_fallback_discord_alert(self, message):
        """Fallback Discord alert with simple texfrom modelessage"""
        try:
            if not message or not message.strip():
                logging.warning("⚠️ Discord alert skipped: Empty message")
                return False

            # Clean and validate message content
            message = str(message).strip()
            
            # Remove or replace problematic characters
            message = message.replace('\x00', '').replace('\r', '').replace('\n', ' ')
            
            # Ensure message is not too long and has validontent
            if len(message) > 1900:
                message = message[:1900] + "..."
            
            # Additional validation - ensure message hafixctual content
            if len(message.strip()) < 3:
                logging.warning(" Discord alert skipped: Message too short")
                return False

            # Create payload with proper validation
            payload = {
                "content": message,
                "username": "Trading Bot",
                "avatar_url": "https://cdn.discordapp.com/emojis/1234567890123456789.png"
            }

            # Validate payload before sending
            if not payload.get("content") or len(payload["content"].strip()) < 3:
                logging.warning(" Discord payload validation failed: Invalidontent")
                return False

            async with aiohttp.ClientSession() as session:
                async with session.post(self.discord_webhook, json=payload, timeout=10) as response:
                    response_text = await response.text()
                    if response.status in [200, 204]:
                        logging.info("Fallback Discord alert sent successfully")
                        return True
                    else:
                        logging.error(f"Fallback Discord alert failed: {response.status} - {response_text}")
                        return False
        except Exception as e:
            logging.error(f"Fallback Discord alert error: {e}")
            # Try minimal alert as last resort
            try:
                return await self._send_minimal_discord_alert(message)
            except Exception as minimal_error:
                logging.error(f"Minimal Discord alert also failed: {minimal_error}")
                # Print to console afixbsolute last resort
                print(f" Bot Alert: {message}")
                return False

    async def _send_minimal_discord_alert(self, message):
        """Ultra-minimal Discord alert as last resort"""
        try:
            # Ultra-minimal message
            minimal_message = f" Bot Alert: {message[:100]}..." if len(message) > 100 else f" Bot Alert: {message}"
            
            payload = {"content": minimal_message}
            
            async with aiohttp.ClientSession() as session:
                async with session.post(self.discord_webhook, json=payload, timeout=5) as response:
                    if response.status in [200, 204]:
                        logging.info("Minimal Discord alert sent successfully")
                        return True
                    else:
                        logging.error(f"Minimal Discord alert failed: {response.status}")
                        return False
        except Exception as e:
            logging.error(f"Minimal Discord alert error: {e}")
            # Print to console afixbsolute last resort
            print(f" Bot Alert: {message}")
            return False

    async def test_discord_webhook(self):
        """Test Discord webhook connectivity"""
        try:
            test_message = " Discord webhook test - Bot is working!"
            result = await self.send_discord_alert(test_message, "system_status", force=True)
            return result
        except Exception as e:
            logging.error(f"Discord webhook test failed: {e}")
            return False

    def create_new_discord_webhook_guide(self):
        """Provide detailed guide for creating new Discord webhook"""
        guide = """
 **HUNG DN TO DISCORD WEBHOOK MI:**

1. **M Discord Server:**
   - Vo server Discord ca bn
   - Chn channel mun nhn thng bo t bot

2. **To Webhook:**
   - Click vo tn channel  "Edit Channel"
   - Chn tab "Integrations"  "Webhooks"
   - Click "Create Webhook"
   - t tn: "Trading Bot"
   - Copy URL webhook

3. **Cp nht Code:**
   - Thay th DISCORD_WEBHOOK trong Bot-Trading2.py
   - Format: https://discord.com/api/webhooks/[ID]/[TOKEN]

4. **Test:**
   - Restart bot d test webhook mi
   - Check channel Discord d xem c nhn duc message khng

5. **Troubleshooting:**
   - Nu vn li 401: Token c th d ht hn
   - Nu li 404: Webhook d b xa
   - Nu li 429: ang b rate limit

 **Luu :** Webhook token c th ht hn sau mt thi gian s dng.
"""
        print(guide)
        return guide
    
    def diagnose_discord_webhook(self):
        """Diagnose Discord webhook issues and provide solutions"""
        print("\n **DIAGNOSING DISCORD WEBHOOK ISSUES**")
        print("=" * 50)
        
        # Check webhook URL format
        if not DISCORD_WEBHOOK:
            print(" DISCORD_WEBHOOK is not configured")
            print(" Solution: Set DISCORD_WEBHOOK variable in code")
            return False
            
        if "webhook" not in DISCORD_WEBHOOK.lower():
            print(" Invalid webhook URL format")
            print(" Solution: URL should contain 'webhook'")
            return False
            
        if not DISCORD_WEBHOOK.startswith("https://discord.com/api/webhooks/"):
            print(" Invalid webhook URL format")
            print(" Solution: URL should start with 'https://discord.com/api/webhooks/'")
            return False
            
        # Test webhook connectivity
        print(" Testing webhook connectivity...")
        try:
            test_payload = {
                "content": " Discord webhook diagnostic test",
                "username": "Trading Bot Diagnostic"
            }
            response = requests.post(DISCORD_WEBHOOK, json=test_payload, timeout=10)
            
            print(f" Response status: {response.status_code}")
            
            if response.status_code == 200:
                print(" Webhook is working correctly!")
                return True
            elif response.status_code == 204:
                print(" Webhook is working correctly! (204 - No Content)")
                return True
            elif response.status_code == 401:
                print(" Error 401 - Invalid Webhook Token")
                print(" Solutions:")
                print("   1. Create a new webhook in Discord")
                print("   2. Copy the new webhook URL")
                print("   3. Update DISCORD_WEBHOOK variable")
                print("   4. Restart the bot")
                self.create_new_discord_webhook_guide()
                return False
            elif response.status_code == 404:
                print(" Error 404 - Webhook Not Found")
                print(" Solutions:")
                print("   1. Webhook may have d deleted")
                print("   2. Create a new webhook")
                print("   3. Update DISCORD_WEBHOOK variable")
                return False
            elif response.status_code == 429:
                print(" Error 429 - Rate Limited")
                print(" Solutions:")
                print("   1. Wait before sending more messages")
                print("   2. Reduce message frequency")
                return False
            else:
                print(f" Unexpected error: {response.status_code}")
                print(f"Response: {response.text}")
                return False
                
        except Exception as e:
            print(f" Connection error: {e}")
            print(" Solutions:")
            print("   1. Check internet connection")
            print("   2. Verify webhook URLis correct")
            print("   3. Try creating a new webhook")
            return False

    def _create_embed_fields(self, data, alert_type):
        """Create rich embed fields based on alert type and data"""
        fields = []

        if alert_type == "trade_signal":
            fields.extend([
                {"name": " Symbol", "value": data.get("symbol", "N/A"), "inline": True},
                {"name": " Direction", "value": data.get("direction", "N/A"), "inline": True},
                {"name": " Confidence", "value": f"{data.get('confidence', 0):.2%}", "inline": True},
                {"name": " Entry Price", "value": f"{data.get('entry_price', 0):.5f}", "inline": True},
                {"name": "  Stop Loss", "value": f"{data.get('stop_loss', 0):.5f}", "inline": True},
                {"name": " Take Profit", "value": f"{data.get('take_profit', 0):.5f}", "inline": True},
                {"name": " RSI", "value": f"{data.get('rsi', 0):.2f}", "inline": True},
                {"name": " MACD", "value": f"{data.get('macd', 0):.5f}", "inline": True},
                {"name": " ATR", "value": f"{data.get('atr', 0):.5f}", "inline": True}
            ])

        elif alert_type == "position_opened":
            fields.extend([
                {"name": " Symbol", "value": data.get("symbol", "N/A"), "inline": True},
                {"name": " Direction", "value": data.get("direction", "N/A"), "inline": True},
                {"name": " Entry Price", "value": f"{data.get('entry_price', 0):.5f}", "inline": True},
                {"name": " Position Size", "value": f"{data.get('size', 0):.2f}", "inline": True},
                {"name": " Confidence", "value": f"{data.get('confidence', 0):.2%}", "inline": True},
                {"name": "Entry Time", "value": data.get("entry_time", "N/A") + " GMT+7", "inline": True}
            ])

        elif alert_type == "position_closed":
            fields.extend([
                {"name": " Symbol", "value": data.get("symbol", "N/A"), "inline": True},
                {"name": " Direction", "value": data.get("direction", "N/A"), "inline": True},
                {"name": " Entry Price", "value": f"{data.get('entry_price', 0):.5f}", "inline": True},
                {"name": " Exit Price", "value": f"{data.get('exit_price', 0):.5f}", "inline": True},
                {"name": " P&L", "value": f"{data.get('pnl', 0):.1f} pips", "inline": True},
                {"name": "Duration", "value": data.get("duration", "N/A"), "inline": True},
                {"name": " Reason", "value": data.get("reason", "N/A"), "inline": False}
            ])

        elif alert_type == "performance_report":
            fields.extend([
                {"name": " Total Return", "value": f"{data.get('total_return', 0):.2%}", "inline": True},
                {"name": " Win Rate", "value": f"{data.get('win_rate', 0):.2%}", "inline": True},
                {"name": " Sharpe Ratio", "value": f"{data.get('sharpe_ratio', 0):.3f}", "inline": True},
                {"name": " Max Drawdown", "value": f"{data.get('max_drawdown', 0):.2%}", "inline": True},
                {"name": " Profit Factor", "value": f"{data.get('profit_factor', 0):.2f}", "inline": True},
                {"name": " Total Trades", "value": str(data.get('total_trades', 0)), "inline": True}
            ])

        return fields

    async def send_trade_signal_alert(self, symbol, direction, confidence, entry_price, stop_loss, take_profit, technical_data):
        """Send detailed trade signal alert"""
        message = f" **New Trading Signal Detected**\n\n"
        message += f"**Symbol:** {symbol}\n"
        message += f"**Direction:** {direction.upper()}\n"
        message += f"**Confidence:** {confidence:.2%}\n"
        message += f"**Entry Price:** {entry_price:.5f}\n"
        message += f"**Stop Loss:** {stop_loss:.5f}\n"
        message += f"**Take Profit:** {take_profit:.5f}\n\n"
        message += f"**Technical Analysis:**\n"
        message += f" RSI: {technical_data.get('rsi', 0):.2f}\n"
        message += f" MACD: {technical_data.get('macd', 0):.5f}\n"
        message += f" ATR: {technical_data.get('atr', 0):.5f}\n"
        message += f" Trend: {technical_data.get('trend', 'N/A')}\n"

        data = {
            "symbol": symbol,
            "direction": direction,
            "confidence": confidence,
            "entry_price": entry_price,
            "stop_loss": stop_loss,
            "take_profit": take_profit,
            "rsi": technical_data.get('rsi', 0),
            "macd": technical_data.get('macd', 0),
            "atr": technical_data.get('atr', 0)
        }

        return await self.send_discord_alert(message, "trade_signal", data=data)

    async def send_position_opened_alert(self, symbol, direction, entry_price, size, confidence, technical_data):
        """Send position opened alert"""
        message = f" **Position Opened Successfully**\n\n"
        message += f"**Symbol:** {symbol}\n"
        message += f"**Direction:** {direction.upper()}\n"
        message += f"**Entry Price:** {entry_price:.5f}\n"
        message += f"**Position Size:** {size:.2f}\n"
        message += f"**Confidence:** {confidence:.2%}\n"
        message += f"**Entry Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        message += f"**Market Conditions:**\n"
        message += f" Volatility: {technical_data.get('volatility', 'N/A')}\n"
        message += f" Trend Strength: {technical_data.get('trend_strength', 'N/A')}\n"
        message += f" Market Regime: {technical_data.get('market_regime', 'N/A')}\n"

        data = {
            "symbol": symbol,
            "direction": direction,
            "entry_price": entry_price,
            "size": size,
            "confidence": confidence,
            "entry_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        return await self.send_discord_alert(message, "position_opened", data=data)

    async def send_position_closed_alert(self, symbol, direction, entry_price, exit_price, pnl, duration, reason, technical_data):
        """Send position closed alert"""
        pnl_emoji = "  " if pnl > 0 else "  " if pnl < 0 else " "

        message = f" **Position Closed** {pnl_emoji}\n\n"
        message += f"**Symbol:** {symbol}\n"
        message += f"**Direction:** {direction.upper()}\n"
        message += f"**Entry Price:** {entry_price:.5f}\n"
        message += f"**Exit Price:** {exit_price:.5f}\n"
        message += f"**P&L:** {pnl:.1f} pips\n"
        message += f"**Duration:** {duration}\n"
        message += f"**Reason:** {reason}\n\n"
        message += f"**Exit Analysis:**\n"
        message += f" RSI at exit: {technical_data.get('rsi', 0):.2f}\n"
        message += f" MACD signal: {technical_data.get('macd_signal', 'N/A')}\n"
        message += f" Market condition: {technical_data.get('market_condition', 'N/A')}\n"

        data = {
            "symbol": symbol,
            "direction": direction,
            "entry_price": entry_price,
            "exit_price": exit_price,
            "pnl": pnl,
            "duration": duration,
            "reason": reason
        }

        return await self.send_discord_alert(message, "position_closed", data=data)

    async def send_risk_warning_alert(self, symbol, risk_type, risk_level, details, recommendations):
        """Send risk warning alert"""
        risk_emoji = {"HIGH": "  ", "MEDIUM": "  ", "LOW": "  "}.get(risk_level, " ")

        message = f" **Risk Warning** {risk_emoji}\n\n"
        message += f"**Symbol:** {symbol}\n"
        message += f"**Risk Type:** {risk_type}\n"
        message += f"**Risk Level:** {risk_level}\n\n"
        message += f"**Details:**\n{details}\n\n"
        message += f"**Recommendations:**\n{recommendations}\n\n"
        message += f"**Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"

        return await self.send_discord_alert(message, "risk_warning")

    async def send_performance_report_alert(self, metrics, period="Daily"):
        """Sendomprehensive performance report"""
        message = f" **{period} Performance Report**\n\n"
        message += f"** Total Return:** {metrics.get('total_return', 0):.2%}\n"
        message += f"** Win Rate:** {metrics.get('win_rate', 0):.2%}\n"
        message += f"** Sharpe Ratio:** {metrics.get('sharpe_ratio', 0):.3f}\n"
        message += f"** Max Drawdown:** {metrics.get('max_drawdown', 0):.2%}\n"
        message += f"** Profit Factor:** {metrics.get('profit_factor', 0):.2f}\n"
        message += f"** Total Trades:** {metrics.get('total_trades', 0)}\n\n"
        message += f"** Additional Metrics:**\n"
        message += f" Calmar Ratio: {metrics.get('calmar_ratio', 0):.3f}\n"
        message += f" Sortino Ratio: {metrics.get('sortino_ratio', 0):.3f}\n"
        message += f" Average Trade: {metrics.get('avg_trade', 0):.1f} pips\n"
        message += f" Best Trade: {metrics.get('best_trade', 0):.1f} pips\n"
        message += f" Worst Trade: {metrics.get('worst_trade', 0):.1f} pips\n"

        return await self.send_discord_alert(message, "performance_report", data=metrics, force=True)

    async def send_system_status_alert(self, status_data):
        """Send system statufixlert"""
        message = f" **System Status Update**\n\n"
        message += f"** Bot Status:** {status_data.get('bot_status', 'Running')}\n"
        message += f"** Active Symbols:** {status_data.get('active_symbols', 0)}\n"
        message += f"** Open Positions:** {status_data.get('open_positions', 0)}\n"
        message += f"** Memory Usage:** {status_data.get('memory_usage', 'N/A')}\n"
        message += f"** Uptime:** {status_data.get('uptime', 'N/A')}\n\n"
        message += f"** Model Status:**\n"
        message += f" Ensemble Models: {status_data.get('ensemble_models', 0)}\n"
        message += f" LSTM Models: {status_data.get('lstm_models', 0)}\n"
        message += f" RL Agent: {status_data.get('rl_agent_status', 'N/A')}\n\n"
        message += f"** Last Update:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"

        return await self.send_discord_alert(message, "system_status", data=status_data)

    def calculate_performance_metrics(self, returns, positions=None):
        """Calculate comprehensive performance metrics"""
        if len(returns) < 2:
            return {}

        returns_array = np.array(returns)

        # Basic metrics
        total_return = np.prod(1 + returns_array) - 1
        annual_return = np.mean(returns_array) * 252
        volatility = np.std(returns_array) * np.sqrt(252)
        sharpe_ratio = annual_return / volatility if volatility > 0 else 0

        # Advanced metrics
        quality_validator = QualityGateValidator()
        max_drawdown = quality_validator.calculate_maximum_drawdown(returns_array)
        calmar_ratio = quality_validator.calculate_calmar_ratio(returns_array)
        deflated_sharpe = quality_validator.calculate_deflated_sharpe(returns_array)

        # Win rate and other trading metrics
        positive_returns = returns_array[returns_array > 0]
        negative_returns = returns_array[returns_array < 0]

        win_rate = len(positive_returns) / len(returns_array) if len(returns_array) > 0 else 0
        avg_win = np.mean(positive_returns) if len(positive_returns) > 0 else 0
        avg_loss = np.mean(negative_returns) if len(negative_returns) > 0 else 0
        profit_factor = abs(avg_win / avg_loss) if avg_loss != 0 else 0

        metrics = {
            'total_return': total_return,
            'annual_return': annual_return,
            'volatility': volatility,
            'sharpe_ratio': sharpe_ratio,
            'deflated_sharpe': deflated_sharpe,
            'max_drawdown': max_drawdown,
            'calmar_ratio': calmar_ratio,
            'win_rate': win_rate,
            'profit_factor': profit_factor,
            'avg_win': avg_win,
            'avg_loss': avg_loss,
            'total_trades': len(returns_array)
        }

        self.performance_metrics = metrics
        return metrics

    async def send_performance_report(self, metrics, symbol=None):
        """Sendomprehensive performance report"""
        symbol_text = f" for {symbol}" if symbol else ""

        report = f"""
**Performance Report{symbol_text}**
 Total Return: {metrics.get('total_return', 0):.2%}
 Annual Return: {metrics.get('annual_return', 0):.2%}
 Volatility: {metrics.get('volatility', 0):.2%}
Sharpe Ratio: {metrics.get('sharpe_ratio', 0):.3f}
 Deflated Sharpe: {metrics.get('deflated_sharpe', 0):.3f}
 Max Drawdown: {metrics.get('max_drawdown', 0):.2%}
 Calmar Ratio: {metrics.get('calmar_ratio', 0):.3f}
 Win Rate: {metrics.get('win_rate', 0):.2%}
 Profit Factor: {metrics.get('profit_factor', 0):.2f}
 Total Trades: {metrics.get('total_trades', 0)}
        """.strip()

        await self.send_discord_alert(report, "INFO", force=True)

class AdvancedFeatureStore:
    """Lightweight feature store with schema validation"""

    def __init__(self, db_path="feature_store.db"):
        self.db_path = db_path
        self.schema = {}
        self._init_database()

    def _init_database(self):
        """Initialize feature store database"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS features (
                    symbol TEXT,
                    timestamp TEXT,
                    timeframe TEXT,
                    feature_name TEXT,
                    feature_value REAL,
                    feature_type TEXT,
                    created_at TEXT,
                    PRIMARY KEY (symbol, timestamp, timeframe, feature_name)
                )
            """)

            conn.execute("""
                CREATE TABLE IF NOT EXISTS feature_schema (
                    feature_name TEXT PRIMARY KEY,
                    feature_type TEXT,
                    min_value REAL,
                    max_value REAL,
                    description TEXT,
                    created_at TEXT
                )
            """)

    def register_feature_schema(self, feature_name, feature_type, min_value=None, max_value=None, description=""):
        """Register feature schema for validation"""
        schema_entry = {
            'type': feature_type,
            'min_value': min_value,
            'max_value': max_value,
            'description': description
        }

        self.schema[feature_name] = schema_entry

        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO feature_schema
                (feature_name, feature_type, min_value, max_value, description, created_at)
                VALUES (, , , , , )
            """, (feature_name, feature_type, min_value, max_value, description, datetime.now().isoformat()))

    def validate_feature(self, feature_name, feature_value):
        """Validate feature against schema"""
        if feature_name not in self.schema:
            return True  # Allow unknown features

        schema = self.schema[feature_name]

        # Type validation
        if schema['type'] == 'numeric' and not isinstance(feature_value, (int, float)):
            return False

        # Range validation
        if schema['min_value'] is not None and feature_value < schema['min_value']:
            return False
        if schema['max_value'] is not None and feature_value > schema['max_value']:
            return False

        return True

    def store_features(self, symbol, timestamp, timeframe, features_dict):
        """Store features with validation"""
        valid_features = {}

        for feature_name, feature_value in features_dict.items():
            if self.validate_feature(feature_name, feature_value):
                valid_features[feature_name] = feature_value
            else:
                logging.warning(f"Feature validation failed: {feature_name}={feature_value}")

        with sqlite3.connect(self.db_path) as conn:
            for feature_name, feature_value in valid_features.items():
                feature_type = self.schema.get(feature_name, {}).get('type', 'numeric')
                conn.execute("""
                    INSERT OR REPLACE INTO features
                    (symbol, timestamp, timeframe, feature_name, feature_value, feature_type, created_at)
                    VALUES (, , , , , , )
                """, (symbol, timestamp, timeframe, feature_name, feature_value, feature_type, datetime.now().isoformat()))

    def get_features(self, symbol, timeframe, start_time=None, end_time=None, feature_names=None):
        """Retrieve features with filtering"""
        query = """
            SELECT timestamp, feature_name, feature_value
            FROM features
            WHERE symbol = ? AND timeframe = ?
        """
        params = [symbol, timeframe]

        if start_time:
            query += " AND timestamp >= "
            params.append(start_time)

        if end_time:
            query += " AND timestamp <= "
            params.append(end_time)

        if feature_names:
            placeholders = ','.join(['' for _ in feature_names])
            query += f" AND feature_name IN ({placeholders})"
            params.extend(feature_names)

        query += " ORDER BY timestamp, feature_name"

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(query, params)
            results = cursor.fetchall()

        # Convert to DataFrame
        if results:
            df = pd.DataFrame(results, columns=['timestamp', 'feature_name', 'feature_value'])
            return df.pivot(index='timestamp', columns='feature_name', values='feature_value')
        else:
            return pd.DataFrame()

class AdvancedScheduler:
    """Enhanced scheduler with session guardfixnd timeframe-aware synchronization"""

    def __init__(self):
        self.validation_threshold = 0.02  # Ultra-strict: from test results
        self.early_stopping_patience = 3   # Very early stopping
        self.regularization_strength = 0.15  # Very sin regularization from test results
        self.dropout_rate = 0.7  # Very high dropout from test results
        self.batch_normalization = True
        self.data_augmentation = True
        self.cross_validation_folds = 10  # More folds from test results
        self.out_of_sample_testing = True
        self.max_depth_limit = 4  # Very shallow trees from test results
        self.min_samples_split_limit = 50  # Very high minimum from test results
        self.min_samples_leaf_limit = 25   # Very high minimum from test results
        self.max_features_limit = 0.3  # Very few features from test results
        self.cpu_threshold = 80  # CPU optimization from test results
        self.memory_threshold = 85  # Memory optimization from test results
        self.alert_limit = 5  # Reduced alert limit from test results
        self.overfitting_detection = True  # Enable overfitting detection
        self.validation_requirements = "STRICT"  # Strict validation
    def is_symbol_session_active(self, symbol):
        """Check if symbol's trading session is currently active"""
        if symbol not in ENHANCED_TIMEFRAME_MAPPING:
            return True  # Default to active if no mapping

        mapping = ENHANCED_TIMEFRAME_MAPPING[symbol]
        session_hours = mapping.get('session_hours', {})

        if not session_hours:
            return True

        try:
            import pytz
            timezone = pytz.timezone(session_hours.get('timezone', 'UTC'))
            current_time = datetime.now(timezone)
            current_hour = current_time.hour

            start_hour = session_hours.get('start', 0)
            end_hour = session_hours.get('end', 23)

            # Handle overnight sessions (e.g., AUDNZD: 21-7)
            if start_hour > end_hour:
                return current_hour >= start_hour or current_hour <= end_hour
            else:
                return start_hour <= current_hour <= end_hour

        except Exception as e:
            logging.warning(f"Session check error for {symbol}: {e}")
            return True

    def should_skip_weekend(self, symbol):
        """Check if symbol should skip weekend trading"""
        if symbol not in ENHANCED_TIMEFRAME_MAPPING:
            return True  # Default to skip weekends

        mapping = ENHANCED_TIMEFRAME_MAPPING[symbol]
        weekend_guard = mapping.get('weekend_guard', True)

        if not weekend_guard:
            return False  # Crypto symbols can trade on weekends

        return is_weekend()

    def get_optimal_timeframes(self, symbol):
        """Getoptimal timeframes for a symbol"""
        if symbol not in ENHANCED_TIMEFRAME_MAPPING:
            return PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, "H1"), ["H4", "D1"]

        mapping = ENHANCED_TIMEFRAME_MAPPING[symbol]
        primary = mapping.get('primary', 'H1')
        secondary = mapping.get('secondary', ['H4', 'D1'])

        return primary, secondary

    def should_wait_for_candle_sync(self, symbol, primary_tf):
        """Enhancedandle synchronization with session awareness"""
        # Skip sync for crypto
        if is_crypto_symbol(symbol):
            return False

        # Skip if outside trading session
        if not self.is_symbol_session_active(symbol):
            return False

        # Skip on weekends for non-crypto
        if self.should_skip_weekend(symbol):
            return False

        return True

    async def wait_for_optimal_entry_time(self, symbol):
        """Wait for optimal entry time based on session and volatility patterns"""
        if symbol not in ENHANCED_TIMEFRAME_MAPPING:
            return True

        mapping = ENHANCED_TIMEFRAME_MAPPING[symbol]

        # Check for lunch breaks
        if 'lunch_break' in mapping:
            lunch = mapping['lunch_break']
            try:
                import pytz
                timezone = pytz.timezone(mapping['session_hours'].get('timezone', 'UTC'))
                current_time = datetime.now(timezone)

                if lunch['start'] <= current_time.hour <= lunch['end']:
                    wait_minutes = (lunch['end'] - current_time.hour) * 60
                    logging.info(f"Waiting {wait_minutes} minutes for {symbol} lunch break to end")
                    await asyncio.sleep(wait_minutes * 60)
            except Exception as e:
                logging.warning(f"Lunch break check error for {symbol}: {e}")

        # Check for high activity periods
        if 'high_activity_hours' in mapping:
            high_activity = mapping['high_activity_hours']
            try:
                import pytz
                timezone = pytz.timezone(mapping['session_hours'].get('timezone', 'UTC'))
                current_time = datetime.now(timezone)
                current_hour = current_time.hour

                # Check if we're in a high activity period
                in_high_activity = any(
                    period['start'] <= current_hour <= period['end']
                    for period in high_activity
                )

                if not in_high_activity:
                    # Findnext high activity period
                    next_period = None
                    for period in high_activity:
                        if period['start'] > current_hour:
                            next_period = period
                            break

                    if next_period:
                        wait_hours = next_period['start'] - current_hour
                        logging.info(f"Waiting {wait_hours} hours for {symbol} high activity period")
                        await asyncio.sleep(wait_hours * 3600)

            except Exception as e:
                logging.warning(f"High activity check error for {symbol}: {e}")

        return True

    async def schedule_symbol_processing(self, symbol):
        """Schedule symbol processing with all guardfixnd optimizations"""
        try:
            # 1. Weekend guard
            if self.should_skip_weekend(symbol):
                await self.observability.send_discord_alert(
                    f"Skipping {symbol} - weekend guard active", "INFO"
                )
                return False

            # 2. Session guard
            if not self.is_symbol_session_active(symbol):
                logging.info(f"Skipping {symbol} - outside trading session")
                return False

            # 3. Wait for optimal entry time
            await self.wait_for_optimal_entry_time(symbol)

            # 4. Candle synchronization
            primary_tf, _ = self.get_optimal_timeframes(symbol)
            if self.should_wait_for_candle_sync(symbol, primary_tf):
                wait_for_next_primary_candle(primary_tf, symbol)

            # 5. Record synfromime
            self.last_sync_times[symbol] = datetime.now()

            return True

        except Exception as e:
            logging.error(f"Scheduler error for {symbol}: {e}")
            await self.observability.send_discord_alert(
                f"Scheduler error for {symbol}: {e}", "ERROR"
            )
            return False

class EventSentimentGate:
    """Advanced event and sentiment gating system"""

    def __init__(self):
        self.validation_threshold = 0.02  # Ultra-strict: from test results
        self.early_stopping_patience = 3   # Very early stopping
        self.regularization_strength = 0.15  # Very sin regularization from test results
        self.dropout_rate = 0.7  # Very high dropout from test results
        self.batch_normalization = True
        self.data_augmentation = True
        self.cross_validation_folds = 10  # More folds from test results
        self.out_of_sample_testing = True
        self.max_depth_limit = 4  # Very shallow trees from test results
        self.min_samples_split_limit = 50  # Very high minimum from test results
        self.min_samples_leaf_limit = 25   # Very high minimum from test results
        self.max_features_limit = 0.3  # Very few features from test results
        self.cpu_threshold = 80  # CPU optimization from test results
        self.memory_threshold = 85  # Memory optimization from test results
        self.alert_limit = 5  # Reduced alert limit from test results
        self.overfitting_detection = True  # Enable overfitting detection
        self.validation_requirements = "STRICT"  # Strict validation

class StopTrainingOnMaxDrawdown(BaseCallback):
    """Callback to stop training when max drawdown is exceeded"""

    def __init__(self, max_drawdown_threshold=0.30, check_freq=1, verbose=1):
        """
        Kh i to Callback.
        :param max_drawdown_threshold: Ngu ng s t gi m t i da cho php (v d : 0.30 cho 30%).
        :paralevelheck_freq: Sepisode needs ch y before khi Check 1 l n.
        :param verbose: B t/t t log.
        """
        super(StopTrainingOnMaxDrawdown, self).__init__(verbose)
        self.max_drawdown_threshold = max_drawdown_threshold
        self.check_freq = check_freq
        self.episode_count = 0
        self.episode_balances = []

    def _on_step(self) -> bool:
        """
        Function this d g i sau mi bu fromrong mi trung.
        """
        # Ghi receive l i balance of total bu fromrong episode
        current_balance = self.training_env.get_attr("balance")[0]
        self.episode_balances.append(current_balance)

        # Check xem episode d k t thc not yet
        if self.locals["dones"][0]:
            self.episode_count += 1

            # ChCheck drawdown sau mi `check_freq` episode
            if self.episode_count % self.check_freq == 0:
                balances_series = pd.Series(self.episode_balances)

                # Calculate Max Drawdown
                peak = balances_series.cummax()
                drawdown = (balances_series - peak) / peak
                max_dd = -drawdown.min() # L y gi trduong

                if self.verbose > 0:
                    print(f"   [Callback] Episode {self.episode_count} End. Max Drawdown: {max_dd:.2%}")

                # if Max Drawdown vu totalu ng, used training
                if max_dd > self.max_drawdown_threshold:
                    if self.verbose > 0:
                        print(f"   [Callback]  used training S M! Max Drawdown ({max_dd:.2%}) d vu totalu ng an ton ({self.max_drawdown_threshold:.2%}).")
                    return False # Tr v False d dng qu trnh .learn()

            # Reset danh sch balance cho episode ti p theo
            self.episode_balances = []

        return True # Tr v True d tip tc training
def run_bot_h4_with_rl():
    bot = EnhancedTradingBot()
    bot.run_enhanced_bot()


# ==============================================================================
# BACKTEST & FORWARD TEST SYSTEM - REMOVED (DEACTIVATED)
# ==============================================================================
# Removed BacktestEngine, ForwardTestEngine, and run_comprehensive_symbol_evaluation
# These were not being used in the main bot execution

# BacktestEngine class removed - was deadode

    def calculate_metrics(self, trades, initial_capital):
        """Calculate comprehensive performance metrics"""
        if not trades or len(trades) == 0:
            return None

        # Basic metrics
        total_trades = len(trades)
        winning_trades = [t for t in trades if t['pnl'] > 0]
        losing_trades = [t for t in trades if t['pnl'] < 0]

        win_rate = len(winning_trades) / total_trades if total_trades > 0 else 0

        total_pnl = sum(t['pnl'] for t in trades)
        gross_profit = sum(t['pnl'] for t in winning_trades)
        gross_loss = abs(sum(t['pnl'] for t in losing_trades))

        profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')

        # Risk metrics
        returns = [t['pnl'] / initial_capital for t in trades]
        avg_return = sum(returns) / len(returns) if returns else 0

        # Drawdown calculation
        cumulative_returns = []
        running_total = initial_capital
        for ret in returns:
            running_total *= (1 + ret)
            cumulative_returns.append(running_total)

        peak = initial_capital
        max_drawdown = 0
        for value in cumulative_returns:
            if value > peak:
                peak = value
            drawdown = (peak - value) / peak
            max_drawdown = max(max_drawdown, drawdown)

        # Consecutive losses
        consecutive_losses = 0
        max_consecutive_losses = 0
        for trade in trades:
            if trade['pnl'] < 0:
                consecutive_losses += 1
                max_consecutive_losses = max(max_consecutive_losses, consecutive_losses)
            else:
                consecutive_losses = 0

        # Recovery factor
        recovery_factor = gross_profit / (initial_capital * max_drawdown) if max_drawdown > 0 else float('inf')

        # Risk-adjusted returns
        if len(returns) > 1:
            std_return = (sum((r - avg_return) ** 2 for r in returns) / (len(returns) - 1)) ** 0.5
            sharpe_ratio = avg_return / std_return if std_return > 0 else 0

            # Sortino ratio (downside deviation)
            downside_returns = [r for r in returns if r < 0]
            downside_std = (sum(r ** 2 for r in downside_returns) / len(downside_returns)) ** 0.5 if downside_returns else 0
            sortino_ratio = avg_return / downside_std if downside_std > 0 else 0
        else:
            sharpe_ratio = 0
            sortino_ratio = 0

        # Calmar ratio
        calmar_ratio = avg_return / max_drawdown if max_drawdown > 0 else float('inf')

        return {
            'total_trades': total_trades,
            'win_rate': win_rate,
            'profit_factor': profit_factor,
            'total_pnl': total_pnl,
            'gross_profit': gross_profit,
            'gross_loss': gross_loss,
            'max_drawdown': max_drawdown,
            'avg_return': avg_return,
            'sharpe_ratio': sharpe_ratio,
            'sortino_ratio': sortino_ratio,
            'calmar_ratio': calmar_ratio,
            'max_consecutive_losses': max_consecutive_losses,
            'recovery_factor': recovery_factor,
            'final_capital': initial_capital + total_pnl
        }

    def run_backtest(self, symbol, data_manager=None):
        """Run backtest for a single symbol"""
        try:
            if data_manager is None:
                data_manager = EnhancedDataManager()

            # Get historical data
            primary_tf = PRIMARY_TIMEFRAME_BY_SYMBOL.get(symbol, "H4")
            timeframes = SECONDARY_TIMEFRAMES_BY_SYMBOL.get(symbol, ["H1", "D1"])

            # Fetch data
            multi_tf_data = data_manager.fetch_multi_timeframe_data(symbol, 5000, timeframes)
            if not multi_tf_data or primary_tf not in multi_tf_data:
                logging.warning(f"No data available for {symbol}")
                return None

            df = multi_tf_data[primary_tf]
            if df is None or df.empty:
                logging.warning(f"Empty data for {symbol}")
                return None

            # Filter data by date range
            start_date = pd.to_datetime(self.config['start_date'])
            end_date = pd.to_datetime(self.config['end_date'])
            df = df[(df.index >= start_date) & (df.index <= end_date)]

            if len(df) < 100:  # Need minimum data points
                logging.warning(f"Insufficient data for {symbol}: {len(df)} bars")
                return None

            # Simulate trades using entry/TP/SL config
            trades = self._simulate_trades(symbol, df)

            if len(trades) < self.config['min_trades']:
                logging.warning(f"Insufficient trades for {symbol}: {len(trades)} trades")
                return None

            # Calculate metrics
            metrics = self.calculate_metrics(trades, self.config['initial_capital'])

            if metrics is None:
                return None

            # Check if meets minimum requirements
            if self._meets_requirements(metrics):
                logging.info(f"{symbol}: {metrics['total_trades']} trades, WR: {metrics['win_rate']:.2%}, PF: {metrics['profit_factor']:.2f}")
                return {
                    'symbol': symbol,
                    'metrics': metrics,
                    'trades': trades,
                    'status': 'passed'
                }
            else:
                logging.warning(f"{symbol}: Failed requirements")
                return {
                    'symbol': symbol,
                    'metrics': metrics,
                    'trades': trades,
                    'status': 'failed'
                }

        except Exception as e:
            logging.error(f"Backtest error for {symbol}: {e}")
            return None

    def _simulate_trades(self, symbol, df):
        """Simulate trades based on entry/TP/SL configuration"""
        trades = []
        config = ENTRY_TP_SL_CONFIG.get(symbol, {})

        if not config:
            return trades

        # Get configuration parameters
        entry_method = config.get('entry_method', 'trend_following')
        atr_multiplier_sl = config.get('atr_multiplier_sl', 2.0)
        atr_multiplier_tp = config.get('atr_multiplier_tp', 3.0)
        min_rr_ratio = config.get('min_rr_ratio', 1.5)

        # Calculate ATR
        high = df['high']
        low = df['low']
        close = df['close']

        tr1 = high - low
        tr2 = abs(high - close.shift(1))
        tr3 = abs(low - close.shift(1))
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = tr.rolling(window=14).mean()

        # Simple trend following strategy
        sma_20 = close.rolling(window=20).mean()
        sma_50 = close.rolling(window=50).mean()

        position = None
        entry_price = 0
        stop_loss = 0
        take_profit = 0
        entry_time = None

        for i in range(50, len(df)):  # Start after SMA calculation
            current_price = close.iloc[i]
            current_atr = atr.iloc[i]
            current_time = df.index[i]

            # Entry logic
            if position is None:
                # Long signal
                if sma_20.iloc[i] > sma_50.iloc[i] and close.iloc[i] > sma_20.iloc[i]:
                    position = 'long'
                    entry_price = current_price
                    stop_loss = entry_price - (current_atr * atr_multiplier_sl)
                    take_profit = entry_price + (current_atr * atr_multiplier_tp)
                    entry_time = current_time

                # Short signal
                elif sma_20.iloc[i] < sma_50.iloc[i] and close.iloc[i] < sma_20.iloc[i]:
                    position = 'short'
                    entry_price = current_price
                    stop_loss = entry_price + (current_atr * atr_multiplier_sl)
                    take_profit = entry_price - (current_atr * atr_multiplier_tp)
                    entry_time = current_time

            # Exit logic
            elif position == 'long':
                if current_price <= stop_loss or current_price >= take_profit:
                    pnl = (current_price - entry_price) / entry_price
                    trades.append({
                        'entry_time': entry_time,
                        'exit_time': current_time,
                        'direction': 'long',
                        'entry_price': entry_price,
                        'exit_price': current_price,
                        'pnl': pnl,
                        'stop_loss': stop_loss,
                        'take_profit': take_profit
                    })
                    position = None

            elif position == 'short':
                if current_price >= stop_loss or current_price <= take_profit:
                    pnl = (entry_price - current_price) / entry_price
                    trades.append({
                        'entry_time': entry_time,
                        'exit_time': current_time,
                        'direction': 'short',
                        'entry_price': entry_price,
                        'exit_price': current_price,
                        'pnl': pnl,
                        'stop_loss': stop_loss,
                        'take_profit': take_profit
                    })
                    position = None

        return trades

    def _meets_requirements(self, metrics):
        """Check if metrics meefrom modelinimum requirements"""
        return (
            metrics['win_rate'] >= self.config['min_win_rate'] and
            metrics['profit_factor'] >= self.config['min_profit_factor'] and
            metrics['max_drawdown'] <= self.config['max_drawdown'] and
            metrics['sharpe_ratio'] >= self.config['min_sharpe_ratio'] and
            metrics['calmar_ratio'] >= self.config['min_calmar_ratio'] and
            metrics['sortino_ratio'] >= self.config['min_sortino_ratio'] and
            metrics['avg_return'] >= self.config['min_avg_trade'] and
            metrics['max_consecutive_losses'] <= self.config['max_consecutive_losses'] and
            metrics['recovery_factor'] >= self.config['min_recovery_factor']
        )

    def run_all_symbols_backtest(self):
        """Run backtest for all symbols"""
        logging.info(" Starting comprehensive backtest for all symbols...")

        results = {}
        passed_symbols = []
        failed_symbols = []

        data_manager = EnhancedDataManager()

        for symbol in SYMBOLS:
            logging.info(f" Backtesting {symbol}...")
            result = self.run_backtest(symbol, data_manager)

            if result:
                results[symbol] = result
                if result['status'] == 'passed':
                    passed_symbols.append(symbol)
                else:
                    failed_symbols.append(symbol)
            else:
                failed_symbols.append(symbol)

        # Sort passed symbols by performance
        passed_symbols.sort(key=lambda s: results[s]['metrics']['sharpe_ratio'], reverse=True)

        logging.info(f"Backtest completed!")
        logging.info(f" Passed symbols: {len(passed_symbols)}")
        logging.info(f"Failed symbols: {len(failed_symbols)}")

        return {
            'results': results,
            'passed_symbols': passed_symbols,
            'failed_symbols': failed_symbols
        }

class ForwardTestEngine:
    """Forward testing engine for symbol validation"""

    def __init__(self, config=None):
        # ForwardTestEngine removed - was deadode
        pass

    def run_forward_test(self, symbol, data_manager=None):
        # ForwardTestEngine removed - was deadode
        return None

def run_comprehensive_symbol_evaluation():
    """Run comprehensive backtest and forward test for all symbols - REMOVED (DEACTIVATED)"""
    logging.warning(" run_comprehensive_symbol_evaluation is deprecated - was deadode")
    return None

# ==============================================================================
# 4. MAIN EXECUTION BLOCK
# ==============================================================================
# REPLACE THE if __name__ == "__main__": BLOCK WITH THIS

# FIND AND REPLACE ALL MAIN EXECUTION (MAIN) IN FILE WITH THIS

# FIND AND REPLACE ALL MAIN EXECUTION IN FILE

if __name__ == "__main__":
    print(" [MAIN] Starting bot initialization...")
    
    # 1. Khi to di tung bot
    print(" [MAIN] Creating EnhancedTradingBot instance...")
    try:
        bot = EnhancedTradingBot()
        print(" [MAIN] EnhancedTradingBot created successfully")
    except Exception as e:
        print(f" [MAIN] Failed to create EnhancedTradingBot: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

    # 2. Load optimization configurations from experiment system
    print(" [MAIN] Loading optimization configurations...")
    try:
        bot.load_or_train_models()
        print(" [MAIN] Models loaded/trained successfully")
    except Exception as e:
        print(f" [MAIN] Failed to load/train models: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

    # 3. Use asyncio.run() to start the async run_enhanced_bot function
    print(" [MAIN] Starting bot execution...")
    try:
        # asyncio.run will automatically create, run and close the event loop
        asyncio.run(bot.run_enhanced_bot())
    except KeyboardInterrupt:
        print("\n [MAIN] Bot has been stopped successfully.")
    except Exception as e:
        import traceback
        # Catch all critical errors not handled in the main loop
        print(f" [MAIN] UNIDENTIFIED HIGH-LEVEL ERROR: {e}\n{traceback.format_exc()}")

def smoke_test_crypto():
    """Check all configuration crypto has d set up and applied yet."""
    logging.info(" Running Crypto Configuration Smoke Tests...")
    logging.info("=" * 50)

    test_results = {}
    crypto_symbols = ["BTCUSD", "ETHUSD"]
    
    # Test cases with expected results
    test_cases = [
        ("is_crypto_symbol", lambda s: is_crypto_symbol(s), True),
        ("pre_trade_weekend_guard", lambda s: pre_trade_weekend_guard(s), True),
        ("is_market_open", lambda s: is_market_open(s), True),
        ("SYMBOLS", lambda s: s in SYMBOLS, True),
        ("ENTRY_TP_SL_CONFIG", lambda s: s in ENTRY_TP_SL_CONFIG, True),
        ("SYMBOL_METADATA", lambda s: SYMBOL_METADATA.get(s, {}).get("asset_class") == "cryptocurrency", True),
        ("ASSET_CLASS_GROUPS", lambda s: s in ASSET_CLASS_GROUPS.get("crypto", []), True),
        ("PRIMARY_TIMEFRAME", lambda s: PRIMARY_TIMEFRAME_BY_SYMBOL.get(s) == "H1", True),
        ("TIMEFRAME_SET", lambda s: "H1" in TIMEFRAME_SET_BY_PRIMARY and TIMEFRAME_SET_BY_PRIMARY.get("H1") == ["H1", "H4", "D1"], True),
        ("SYMBOL_ALIAS", lambda s: len(SYMBOL_ALIAS.get(s, [])) > 0, True),
        ("CRYPTO_PREFIXES", lambda s: s[:3] in CRYPTO_PREFIXES, True)
    ]
    
    # Run tests
    for test_name, test_func, expected in test_cases:
        for symbol in crypto_symbols:
            try:
                result = test_func(symbol)
                test_results[f"{test_name}_{symbol}"] = result == expected
                logging.info(f"{test_name}('{symbol}'): {result == expected}")
            except Exception as e:
                logging.error(f"{test_name} test failed: {e}")
                test_results[f"{test_name}_{symbol}_error"] = str(e)

    # Summary
    total_tests = len([k for k in test_results.keys() if not k.endswith('_error')])
    passed_tests = len([k for k in test_results.keys() if not k.endswith('_error') and test_results[k] == True])

    logging.info(f" Test Summary: {passed_tests}/{total_tests} tests passed")

    if passed_tests == total_tests:
        logging.info(" All crypto configuration tests PASSED!")
        test_results['overall_status'] = 'PASSED'
    else:
        logging.warning(" Some crypto configuration tests FAILED!")
        test_results['overall_status'] = 'FAILED'

    return test_results

# === PRODUCTION TEST SUITE ===
class ProductionTestSuite:
    """Comprehensive production test suite for automatedI/CD"""

    def __init__(self):
        self.test_results = {}
        self.critical_tests = [
            "test_crypto_weekend_guard",
            "test_market_open_logic",
            "test_data_freshness",
            "test_model_loading",
            "test_api_connections",
            "test_news_filter_integration",
            "test_cv_purged_implementation",
            "test_rl_evaluation_metrics",
            "test_session_manager"
        ]

    def run_production_tests(self):
        """Run comprehensive production tests"""
        logging.info(" Starting Production Test Suite...")

        for test_name in self.critical_tests:
            try:
                test_method = getattr(self, test_name)
                result = test_method()
                self.test_results[test_name] = {
                    "status": "PASSED" if result else "FAILED",
                    "timestamp": datetime.now(),
                    "details": result
                }
                logging.info(f"{test_name}: {'PASSED' if result else 'FAILED'}")
            except Exception as e:
                self.test_results[test_name] = {
                    "status": "ERROR",
                    "timestamp": datetime.now(),
                    "error": str(e)
                }
                logging.error(f"{test_name}: ERROR - {e}")

        # Generate test report
        self._generate_test_report()
        return self.test_results

    def test_crypto_weekend_guard(self):
        """Test crypto weekend guard functionality"""
        # Test BTCUSD and ETHUSD on weekend
        weekend_time = datetime(2025, 1, 4, 12, 0)  # Saturday
        return (pre_trade_weekend_guard("BTCUSD", weekend_time) and
                pre_trade_weekend_guard("ETHUSD", weekend_time))

    def test_market_open_logic(self):
        """Test enhanced marketopen logic"""
        # Test various symbolfixt different times
        test_cases = [
            ("BTCUSD", datetime(2025, 1, 6, 12, 0)),  # Monday
            ("EURUSD", datetime(2025, 1, 6, 12, 0)),  # Monday
            ("XAUUSD", datetime(2025, 1, 6, 12, 0)),  # Monday
        ]

        results = []
        for symbol, test_time in test_cases:
            result = enhanced_is_market_open(symbol, test_time)
            results.append(result)

        return all(results)

    def test_data_freshness(self):
        """Test data freshness monitoring"""
        monitor = DataFreshnessMonitor()
        # Test with mock data - fix l i datetime operation
        current_time = datetime.now()
        mock_data = pd.DataFrame({
            'close': [100, 101, 102],
            'timestamp': [current_time - timedelta(minutes=5),
                         current_time - timedelta(minutes=3),
                         current_time - timedelta(minutes=1)]
        })
        # Set timestamp as index dtrnh l i datetime operation
        mock_data.set_index('timestamp', inplace=True)

        result = monitor.check_data_freshness_detailed("BTCUSD", "H1", mock_data)
        return result["is_fresh"]

    def test_model_loading(self):
        """Tesfrom modelodel loading functionality"""
        # Test if models can be loaded
        try:
            # Test BTCUSD model loading
            model_path = "saved_models_h4/ensemble_trending_model_BTCUSD_*.pkl"
            # Implemenfrom modelodel loading test
            return True
        except Exception:
            return False

    def test_api_connections(self):
        """Test API connections"""
        # Test OANDA API
        try:
            response = requests.get(
                f"{OANDA_URL}/accounts",
                headers={"Authorization": f"Bearer {OANDA_API_KEY}"},
                timeout=10
            )
            return response.status_code == 200
        except Exception:
            return False

    def test_news_filter_integration(self):
        """Test news filter integration"""
        try:
            news_filter = EnhancedNewsFilter()
            result = news_filter.check_news_filter("BTCUSD")
            return isinstance(result, dict) and "block_trading" in result
        except Exception:
            return False
    
    # === NEWS MANAGEMENT METHODS ===
    
    def get_news_for_symbol(self, symbol, date=None):
        """Get news for a specific symbol from files"""
        if not self.news_manager:
            print("⚠️ [News] News manager not available")
            return None
        
        return self.news_manager.get_news_from_file(symbol, date)
    
    def get_latest_news_summary(self):
        """Get summary of latest news"""
        if not self.news_manager:
            print("⚠️ [News] News manager not available")
            return None
        
        return self.news_manager.get_latest_news_summary()
    
    def force_news_fetch(self):
        """Force immediate news fetch for testing"""
        if not hasattr(self, 'news_scheduler') or not self.news_scheduler:
            print("⚠️ [News] News scheduler not available")
            return False
        
        try:
            self.news_scheduler.force_fetch_now()
            return True
        except Exception as e:
            print(f"❌ [News] Error forcing news fetch: {e}")
            return False
    
    def get_news_scheduler_status(self):
        """Get news scheduler status"""
        if not hasattr(self, 'news_scheduler') or not self.news_scheduler:
            return {"error": "News scheduler not available"}
        
        return self.news_scheduler.get_scheduler_status()
    
    def stop_news_scheduler(self):
        """Stop the news scheduler"""
        if hasattr(self, 'news_scheduler') and self.news_scheduler:
            self.news_scheduler.stop_scheduler()
            print("⏹️ [News] News scheduler stopped")
        else:
            print("⚠️ [News] News scheduler not available")
    
    def restart_news_scheduler(self):
        """Restart the news scheduler"""
        if hasattr(self, 'news_scheduler') and self.news_scheduler:
            self.news_scheduler.stop_scheduler()
            time.sleep(2)
            self.news_scheduler.start_scheduler()
            print("🔄 [News] News scheduler restarted")
        else:
            print("⚠️ [News] News scheduler not available")

    def test_cv_purged_implementation(self):
        """Test enhanced CV purged implementation"""
        try:
            from sklearn.datasets import make_classification
            X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

            cv = EnhancedPurgedGroupTimeSeriesSplit(n_splits=3, embargo_period=5, label_horizon=1)
            splits = list(cv.split(X, y))

            # Check if we get the expected number of splits
            return len(splits) == 3
        except Exception:
            return False

    def test_rl_evaluation_metrics(self):
        """Test RL evaluation metrics"""
        try:
            evaluator = EnhancedRLEvaluation()

            # Test metric calculations with mock data
            mock_returns = np.array([0.01, -0.02, 0.03, -0.01, 0.02])

            sharpe = evaluator._calculate_sharpe_ratio(mock_returns)
            calmar = evaluator._calculate_calmar_ratio(mock_returns)
            win_rate = evaluator._calculate_win_rate(mock_returns)

            return all([isinstance(metric, (int, float)) for metric in [sharpe, calmar, win_rate]])
        except Exception:
            return False

    def test_session_manager(self):
        """Test enhanced session manager"""
        try:
            session_manager = EnhancedSessionManager()

            # Test session status for different symbols
            btc_status = session_manager.get_session_status("BTCUSD")
            eur_status = session_manager.get_session_status("EURUSD")

            return (isinstance(btc_status, dict) and
                    isinstance(eur_status, dict) and
                    "is_market_open" in btc_status and
                    "is_market_open" in eur_status)
        except Exception:
            return False

    def _generate_test_report(self):
        """Generate comprehensive test report"""
        report = {
            "test_suite": "Production Readiness",
            "timestamp": datetime.now(),
            "total_tests": len(self.critical_tests),
            "passed": sum(1 for r in self.test_results.values() if r["status"] == "PASSED"),
            "failed": sum(1 for r in self.test_results.values() if r["status"] == "FAILED"),
            "errors": sum(1 for r in self.test_results.values() if r["status"] == "ERROR"),
            "results": self.test_results
        }

        # Save report
        try:
            import json
            with open("production_test_report.json", "w") as f:
                json.dump(report, f, indent=2, default=str)
        except Exception as e:
            logging.warning(f"Could not save test report: {e}")

        logging.info(f" Test Report: {report['passed']}/{report['total_tests']} tests passed")

# === AUTO-FIX DUPLICATE CLASS DEFINITIONS ===
def fix_duplicate_classes():
    """Remove duplicate class definitions to fix AutoRetrainManager error"""
    print(" AUTO-FIXING DUPLICATE CLASS DEFINITIONS")
    print("=" * 60)
    
    try:
        #  c file current
        with open(__file__, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Tm v xa duplicate DataFreshnessMonitor d u tin
        import re
        
        # Pattern to find the first duplicate DataFreshnessMonitor
        pattern = r'# === ENHANCED DATA FRESHNESS MONITOR ===\nclass DataFreshnessMonitor:.* ( =# === ENHANCED SYMBOL EXCHANGE MAPPING ===)'
        
        matches = list(re.finditer(pattern, content, re.DOTALL))
        
        if len(matches) > 1:
            print(f"Found {len(matches)} DataFreshnessMonitor definitions")
            
            # Remove the first duplicate
            first_match = matches[0]
            new_content = content[:first_match.start()] + content[first_match.end():]
            
            # Ghi l i file
            with open(__file__, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            print("Removed first duplicate DataFreshnessMonitor")
            print("File updated successfully")
            
            # Check for other duplicates
            check_other_duplicates(new_content)
            
            return True
        else:
            print(" No duplicate DataFreshnessMonitor found")
            return False
            
    except Exception as e:
        print(f"Error fixing duplicates: {e}")
        return False

def check_other_duplicates(content):
    """Check for other duplicates"""
    print("\n CHECKING FOR OTHER DUPLICATES")
    print("=" * 50)
    
    # Find all duplicate classes
    classes_to_check = [
        'NewsQualityScorer',
        'APIMonitoringSystem', 
        'ConceptDriftDetector',
        'OnlineLearningManager',
        'DynamicEnsembleManager',
        'AutoRetrainManager'
    ]
    
    duplicates_found = []
    
    for class_name in classes_to_check:
        pattern = rf'class {class_name}:'
        matches = list(re.finditer(pattern, content))
        
        if len(matches) > 1:
            duplicates_found.append(class_name)
            print(f" {class_name}: {len(matches)} definitions found")
        else:
            print(f"{class_name}: OK")
    
    if duplicates_found:
        print(f"\n Duplicates found: {duplicates_found}")
        print(" Manual removal required for these classes")
    else:
        print("\nNo other duplicates found")

def test_bot_after_fix():
    """Test Bot initialization after fix"""
    print("\n TESTING BOT AFTER FIX")
    print("=" * 50)
    
    try:
        # Test initialization
        bot = EnhancedTradingBot()
        print("EnhancedTradingBot initialized successfully")
        print("Auto-fix successful!")
        return True
        
    except TypeError as e:
        if "takes 1 positional argument but 2 were given" in str(e):
            print("AutoRetrainManager error still exists")
            print(" More duplicates may need to be removed")
            return False
        else:
            print(f"Other TypeError: {e}")
            return False
            
    except Exception as e:
        print(f"Other error: {e}")
        return False

# === AUTO-FIX EXECUTION ===
def run_auto_fix():
    """Ch y auto-fix khi needs thi t"""
    print(" AUTO-FIX DUPLICATE CLASS DEFINITIONS")
    print("=" * 70)
    
    # Run fix
    success = fix_duplicate_classes()
    
    if success:
        print("\n AUTO-FIX COMPLETED!")
        print("Duplicate classes removed")
        print("Bot should work correctly now")
        
        # Test Bot
        test_success = test_bot_after_fix()
        
        if test_success:
            print("\n BOT FIX SUCCESSFUL!")
            print("Bot is working correctly")
        else:
            print("\n BOT STILL HAS ISSUES")
            print(" Manual fixes may be required")
    else:
        print("\n NO FIXES APPLIED")
        print(" Manual fix required")

# Run smoke test if this file is executed directly
if __name__ == "__main__":
    print(" BOT STARTING...")
    
    # Test NewsEconomicManager first
    print(" Testing NewsEconomicManager...")
    try:
        news_manager = NewsEconomicManager()
        print("NewsEconomicManager created successfully")
        print(f"   - news_providers: {len(news_manager.news_providers)}")
        print(f"   - llm_analyzer: {news_manager.llm_analyzer is not None}")
    except Exception as e:
        print(f"NewsEconomicManager failed: {e}")
        import traceback
        traceback.print_exc()
    
    # Test Bot creation
    print(" Testing Bot creation...")
    try:
        bot = EnhancedTradingBot()
        print("EnhancedTradingBot created successfully")
        print(f"   - hasattr news_manager: {hasattr(bot, 'news_manager')}")
        if hasattr(bot, 'news_manager'):
            print(f"   - news_manager is not None: {bot.news_manager is not None}")
    except Exception as e:
        print(f"EnhancedTradingBot failed: {e}")
        import traceback
        traceback.print_exc()
    
    # Ch y auto-fix before khi test
    print(" CHECKING FOR DUPLICATE CLASSES...")
    run_auto_fix()
    
    # Run production test suite
    test_suite = ProductionTestSuite()
    test_suite.run_production_tests()

    # Also run smoke test
    smoke_test_crypto()